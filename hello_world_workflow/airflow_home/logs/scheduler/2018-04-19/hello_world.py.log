[2018-04-19 21:03:01,507] {jobs.py:343} DagFileProcessor0 INFO - Started process (PID=3445) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:01,512] {jobs.py:534} DagFileProcessor0 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:01,514] {jobs.py:1521} DagFileProcessor0 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:01,514] {models.py:167} DagFileProcessor0 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:01,622] {jobs.py:1535} DagFileProcessor0 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:01,629] {models.py:3414} DagFileProcessor0 INFO - Creating ORM DAG for hello_world
[2018-04-19 21:03:01,644] {models.py:322} DagFileProcessor0 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:01,645] {models.py:328} DagFileProcessor0 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:01.644832
[2018-04-19 21:03:01,649] {jobs.py:351} DagFileProcessor0 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.142 seconds
[2018-04-19 21:03:02,730] {jobs.py:343} DagFileProcessor1 INFO - Started process (PID=3447) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:02,735] {jobs.py:534} DagFileProcessor1 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:02,736] {jobs.py:1521} DagFileProcessor1 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:02,737] {models.py:167} DagFileProcessor1 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:02,838] {jobs.py:1535} DagFileProcessor1 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:02,859] {models.py:322} DagFileProcessor1 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:02,859] {models.py:328} DagFileProcessor1 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:02.859786
[2018-04-19 21:03:02,864] {jobs.py:351} DagFileProcessor1 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:03:03,954] {jobs.py:343} DagFileProcessor2 INFO - Started process (PID=3448) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:03,959] {jobs.py:534} DagFileProcessor2 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:03,960] {jobs.py:1521} DagFileProcessor2 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:03,960] {models.py:167} DagFileProcessor2 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:04,075] {jobs.py:1535} DagFileProcessor2 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:04,096] {models.py:322} DagFileProcessor2 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:04,097] {models.py:328} DagFileProcessor2 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:04.097220
[2018-04-19 21:03:04,101] {jobs.py:351} DagFileProcessor2 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:03:05,178] {jobs.py:343} DagFileProcessor3 INFO - Started process (PID=3449) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:05,184] {jobs.py:534} DagFileProcessor3 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:05,185] {jobs.py:1521} DagFileProcessor3 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:05,185] {models.py:167} DagFileProcessor3 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:05,296] {jobs.py:1535} DagFileProcessor3 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:05,322] {models.py:322} DagFileProcessor3 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:05,322] {models.py:328} DagFileProcessor3 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:05.322441
[2018-04-19 21:03:05,326] {jobs.py:351} DagFileProcessor3 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:03:06,388] {jobs.py:343} DagFileProcessor4 INFO - Started process (PID=3450) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:06,393] {jobs.py:534} DagFileProcessor4 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:06,395] {jobs.py:1521} DagFileProcessor4 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:06,395] {models.py:167} DagFileProcessor4 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:06,503] {jobs.py:1535} DagFileProcessor4 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:06,526] {models.py:322} DagFileProcessor4 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:06,527] {models.py:328} DagFileProcessor4 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:06.527121
[2018-04-19 21:03:06,531] {jobs.py:351} DagFileProcessor4 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:03:07,610] {jobs.py:343} DagFileProcessor5 INFO - Started process (PID=3451) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:07,619] {jobs.py:534} DagFileProcessor5 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:07,621] {jobs.py:1521} DagFileProcessor5 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:07,621] {models.py:167} DagFileProcessor5 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:07,769] {jobs.py:1535} DagFileProcessor5 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:07,795] {models.py:322} DagFileProcessor5 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:07,796] {models.py:328} DagFileProcessor5 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:07.796343
[2018-04-19 21:03:07,806] {jobs.py:351} DagFileProcessor5 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.196 seconds
[2018-04-19 21:03:08,927] {jobs.py:343} DagFileProcessor6 INFO - Started process (PID=3453) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:08,932] {jobs.py:534} DagFileProcessor6 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:08,933] {jobs.py:1521} DagFileProcessor6 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:08,934] {models.py:167} DagFileProcessor6 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:09,036] {jobs.py:1535} DagFileProcessor6 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:09,057] {models.py:322} DagFileProcessor6 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:09,057] {models.py:328} DagFileProcessor6 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:09.057446
[2018-04-19 21:03:09,063] {jobs.py:351} DagFileProcessor6 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:03:10,144] {jobs.py:343} DagFileProcessor7 INFO - Started process (PID=3454) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:10,149] {jobs.py:534} DagFileProcessor7 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:10,150] {jobs.py:1521} DagFileProcessor7 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:10,150] {models.py:167} DagFileProcessor7 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:10,259] {jobs.py:1535} DagFileProcessor7 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:10,282] {models.py:322} DagFileProcessor7 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:10,283] {models.py:328} DagFileProcessor7 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:10.283140
[2018-04-19 21:03:10,289] {jobs.py:351} DagFileProcessor7 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:03:11,359] {jobs.py:343} DagFileProcessor8 INFO - Started process (PID=3455) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:11,364] {jobs.py:534} DagFileProcessor8 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:11,365] {jobs.py:1521} DagFileProcessor8 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:11,365] {models.py:167} DagFileProcessor8 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:11,470] {jobs.py:1535} DagFileProcessor8 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:11,490] {models.py:322} DagFileProcessor8 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:11,491] {models.py:328} DagFileProcessor8 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:11.491274
[2018-04-19 21:03:11,497] {jobs.py:351} DagFileProcessor8 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.138 seconds
[2018-04-19 21:03:12,572] {jobs.py:343} DagFileProcessor9 INFO - Started process (PID=3457) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:12,577] {jobs.py:534} DagFileProcessor9 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:12,579] {jobs.py:1521} DagFileProcessor9 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:12,579] {models.py:167} DagFileProcessor9 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:12,685] {jobs.py:1535} DagFileProcessor9 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:12,706] {models.py:322} DagFileProcessor9 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:12,706] {models.py:328} DagFileProcessor9 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:12.706800
[2018-04-19 21:03:12,712] {jobs.py:351} DagFileProcessor9 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.140 seconds
[2018-04-19 21:03:13,802] {jobs.py:343} DagFileProcessor10 INFO - Started process (PID=3458) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:13,806] {jobs.py:534} DagFileProcessor10 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:13,807] {jobs.py:1521} DagFileProcessor10 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:13,808] {models.py:167} DagFileProcessor10 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:13,911] {jobs.py:1535} DagFileProcessor10 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:13,931] {models.py:322} DagFileProcessor10 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:13,932] {models.py:328} DagFileProcessor10 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:13.931860
[2018-04-19 21:03:13,937] {jobs.py:351} DagFileProcessor10 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.136 seconds
[2018-04-19 21:03:15,020] {jobs.py:343} DagFileProcessor11 INFO - Started process (PID=3459) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:15,025] {jobs.py:534} DagFileProcessor11 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:15,026] {jobs.py:1521} DagFileProcessor11 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:15,027] {models.py:167} DagFileProcessor11 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:15,154] {jobs.py:1535} DagFileProcessor11 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:15,185] {models.py:322} DagFileProcessor11 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:15,186] {models.py:328} DagFileProcessor11 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:15.185850
[2018-04-19 21:03:15,197] {jobs.py:351} DagFileProcessor11 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.177 seconds
[2018-04-19 21:03:16,228] {jobs.py:343} DagFileProcessor12 INFO - Started process (PID=3460) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:16,233] {jobs.py:534} DagFileProcessor12 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:16,235] {jobs.py:1521} DagFileProcessor12 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:16,235] {models.py:167} DagFileProcessor12 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:16,355] {jobs.py:1535} DagFileProcessor12 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:16,391] {models.py:322} DagFileProcessor12 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:16,392] {models.py:328} DagFileProcessor12 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:16.392393
[2018-04-19 21:03:16,403] {jobs.py:351} DagFileProcessor12 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.175 seconds
[2018-04-19 21:03:17,437] {jobs.py:343} DagFileProcessor13 INFO - Started process (PID=3461) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:17,443] {jobs.py:534} DagFileProcessor13 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:17,445] {jobs.py:1521} DagFileProcessor13 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:17,445] {models.py:167} DagFileProcessor13 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:17,575] {jobs.py:1535} DagFileProcessor13 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:17,597] {models.py:322} DagFileProcessor13 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:17,598] {models.py:328} DagFileProcessor13 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:17.597922
[2018-04-19 21:03:17,604] {jobs.py:351} DagFileProcessor13 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.167 seconds
[2018-04-19 21:03:18,657] {jobs.py:343} DagFileProcessor14 INFO - Started process (PID=3462) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:18,665] {jobs.py:534} DagFileProcessor14 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:18,667] {jobs.py:1521} DagFileProcessor14 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:18,668] {models.py:167} DagFileProcessor14 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:18,803] {jobs.py:1535} DagFileProcessor14 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:18,825] {models.py:322} DagFileProcessor14 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:18,825] {models.py:328} DagFileProcessor14 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:18.825571
[2018-04-19 21:03:18,831] {jobs.py:351} DagFileProcessor14 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.175 seconds
[2018-04-19 21:03:19,884] {jobs.py:343} DagFileProcessor15 INFO - Started process (PID=3463) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:19,889] {jobs.py:534} DagFileProcessor15 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:19,890] {jobs.py:1521} DagFileProcessor15 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:19,891] {models.py:167} DagFileProcessor15 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:20,000] {jobs.py:1535} DagFileProcessor15 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:20,023] {models.py:322} DagFileProcessor15 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:20,023] {models.py:328} DagFileProcessor15 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:20.023628
[2018-04-19 21:03:20,030] {jobs.py:351} DagFileProcessor15 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:03:21,101] {jobs.py:343} DagFileProcessor16 INFO - Started process (PID=3464) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:21,106] {jobs.py:534} DagFileProcessor16 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:21,107] {jobs.py:1521} DagFileProcessor16 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:21,108] {models.py:167} DagFileProcessor16 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:21,214] {jobs.py:1535} DagFileProcessor16 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:21,236] {models.py:322} DagFileProcessor16 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:21,237] {models.py:328} DagFileProcessor16 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:21.237069
[2018-04-19 21:03:21,242] {jobs.py:351} DagFileProcessor16 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:03:22,317] {jobs.py:343} DagFileProcessor17 INFO - Started process (PID=3466) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:22,322] {jobs.py:534} DagFileProcessor17 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:22,323] {jobs.py:1521} DagFileProcessor17 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:22,323] {models.py:167} DagFileProcessor17 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:22,424] {jobs.py:1535} DagFileProcessor17 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:22,444] {models.py:322} DagFileProcessor17 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:22,444] {models.py:328} DagFileProcessor17 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:22.444777
[2018-04-19 21:03:22,450] {jobs.py:351} DagFileProcessor17 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:03:23,535] {jobs.py:343} DagFileProcessor18 INFO - Started process (PID=3467) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:23,540] {jobs.py:534} DagFileProcessor18 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:23,542] {jobs.py:1521} DagFileProcessor18 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:23,542] {models.py:167} DagFileProcessor18 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:23,648] {jobs.py:1535} DagFileProcessor18 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:23,674] {models.py:322} DagFileProcessor18 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:23,675] {models.py:328} DagFileProcessor18 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:23.674881
[2018-04-19 21:03:23,681] {jobs.py:351} DagFileProcessor18 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:03:24,746] {jobs.py:343} DagFileProcessor19 INFO - Started process (PID=3468) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:24,750] {jobs.py:534} DagFileProcessor19 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:24,751] {jobs.py:1521} DagFileProcessor19 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:24,752] {models.py:167} DagFileProcessor19 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:24,859] {jobs.py:1535} DagFileProcessor19 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:24,881] {models.py:322} DagFileProcessor19 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:24,882] {models.py:328} DagFileProcessor19 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:24.882313
[2018-04-19 21:03:24,888] {jobs.py:351} DagFileProcessor19 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.142 seconds
[2018-04-19 21:03:25,968] {jobs.py:343} DagFileProcessor20 INFO - Started process (PID=3469) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:25,973] {jobs.py:534} DagFileProcessor20 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:25,974] {jobs.py:1521} DagFileProcessor20 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:25,974] {models.py:167} DagFileProcessor20 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:26,078] {jobs.py:1535} DagFileProcessor20 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:26,107] {models.py:322} DagFileProcessor20 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:26,107] {models.py:328} DagFileProcessor20 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:26.107681
[2018-04-19 21:03:26,114] {jobs.py:351} DagFileProcessor20 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:03:27,187] {jobs.py:343} DagFileProcessor21 INFO - Started process (PID=3470) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:27,197] {jobs.py:534} DagFileProcessor21 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:27,198] {jobs.py:1521} DagFileProcessor21 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:27,199] {models.py:167} DagFileProcessor21 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:27,305] {jobs.py:1535} DagFileProcessor21 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:27,328] {models.py:322} DagFileProcessor21 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:27,328] {models.py:328} DagFileProcessor21 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:27.328420
[2018-04-19 21:03:27,334] {jobs.py:351} DagFileProcessor21 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:03:28,406] {jobs.py:343} DagFileProcessor22 INFO - Started process (PID=3471) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:28,411] {jobs.py:534} DagFileProcessor22 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:28,412] {jobs.py:1521} DagFileProcessor22 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:28,412] {models.py:167} DagFileProcessor22 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:28,518] {jobs.py:1535} DagFileProcessor22 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:28,539] {models.py:322} DagFileProcessor22 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:28,540] {models.py:328} DagFileProcessor22 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:28.540350
[2018-04-19 21:03:28,546] {jobs.py:351} DagFileProcessor22 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:03:29,621] {jobs.py:343} DagFileProcessor23 INFO - Started process (PID=3472) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:29,626] {jobs.py:534} DagFileProcessor23 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:29,627] {jobs.py:1521} DagFileProcessor23 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:29,627] {models.py:167} DagFileProcessor23 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:29,740] {jobs.py:1535} DagFileProcessor23 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:29,762] {models.py:322} DagFileProcessor23 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:29,763] {models.py:328} DagFileProcessor23 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:29.762845
[2018-04-19 21:03:29,768] {jobs.py:351} DagFileProcessor23 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:03:30,836] {jobs.py:343} DagFileProcessor24 INFO - Started process (PID=3473) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:30,841] {jobs.py:534} DagFileProcessor24 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:30,842] {jobs.py:1521} DagFileProcessor24 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:30,842] {models.py:167} DagFileProcessor24 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:30,945] {jobs.py:1535} DagFileProcessor24 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:30,967] {models.py:322} DagFileProcessor24 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:30,967] {models.py:328} DagFileProcessor24 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:30.967552
[2018-04-19 21:03:30,973] {jobs.py:351} DagFileProcessor24 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.137 seconds
[2018-04-19 21:03:32,057] {jobs.py:343} DagFileProcessor25 INFO - Started process (PID=3475) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:32,062] {jobs.py:534} DagFileProcessor25 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:32,064] {jobs.py:1521} DagFileProcessor25 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:32,064] {models.py:167} DagFileProcessor25 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:32,173] {jobs.py:1535} DagFileProcessor25 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:32,196] {models.py:322} DagFileProcessor25 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:32,197] {models.py:328} DagFileProcessor25 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:32.196835
[2018-04-19 21:03:32,200] {jobs.py:351} DagFileProcessor25 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:03:33,274] {jobs.py:343} DagFileProcessor26 INFO - Started process (PID=3476) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:33,278] {jobs.py:534} DagFileProcessor26 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:33,279] {jobs.py:1521} DagFileProcessor26 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:33,280] {models.py:167} DagFileProcessor26 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:33,395] {jobs.py:1535} DagFileProcessor26 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:33,428] {models.py:322} DagFileProcessor26 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:33,429] {models.py:328} DagFileProcessor26 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:33.429423
[2018-04-19 21:03:33,434] {jobs.py:351} DagFileProcessor26 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.161 seconds
[2018-04-19 21:03:34,489] {jobs.py:343} DagFileProcessor27 INFO - Started process (PID=3477) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:34,494] {jobs.py:534} DagFileProcessor27 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:34,495] {jobs.py:1521} DagFileProcessor27 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:34,495] {models.py:167} DagFileProcessor27 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:34,603] {jobs.py:1535} DagFileProcessor27 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:34,627] {models.py:322} DagFileProcessor27 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:34,628] {models.py:328} DagFileProcessor27 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:34.628320
[2018-04-19 21:03:34,632] {jobs.py:351} DagFileProcessor27 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:03:35,708] {jobs.py:343} DagFileProcessor28 INFO - Started process (PID=3478) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:35,713] {jobs.py:534} DagFileProcessor28 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:35,714] {jobs.py:1521} DagFileProcessor28 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:35,714] {models.py:167} DagFileProcessor28 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:35,822] {jobs.py:1535} DagFileProcessor28 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:35,847] {models.py:322} DagFileProcessor28 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:35,848] {models.py:328} DagFileProcessor28 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:35.847953
[2018-04-19 21:03:35,852] {jobs.py:351} DagFileProcessor28 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:03:36,928] {jobs.py:343} DagFileProcessor29 INFO - Started process (PID=3479) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:36,933] {jobs.py:534} DagFileProcessor29 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:36,934] {jobs.py:1521} DagFileProcessor29 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:36,934] {models.py:167} DagFileProcessor29 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:37,044] {jobs.py:1535} DagFileProcessor29 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:37,071] {models.py:322} DagFileProcessor29 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:37,071] {models.py:328} DagFileProcessor29 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:37.071660
[2018-04-19 21:03:37,076] {jobs.py:351} DagFileProcessor29 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:03:38,155] {jobs.py:343} DagFileProcessor30 INFO - Started process (PID=3480) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:38,160] {jobs.py:534} DagFileProcessor30 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:38,161] {jobs.py:1521} DagFileProcessor30 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:38,161] {models.py:167} DagFileProcessor30 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:38,270] {jobs.py:1535} DagFileProcessor30 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:38,295] {models.py:322} DagFileProcessor30 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:38,295] {models.py:328} DagFileProcessor30 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:38.295577
[2018-04-19 21:03:38,300] {jobs.py:351} DagFileProcessor30 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:03:39,374] {jobs.py:343} DagFileProcessor31 INFO - Started process (PID=3481) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:39,378] {jobs.py:534} DagFileProcessor31 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:39,379] {jobs.py:1521} DagFileProcessor31 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:39,380] {models.py:167} DagFileProcessor31 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:39,480] {jobs.py:1535} DagFileProcessor31 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:39,502] {models.py:322} DagFileProcessor31 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:39,503] {models.py:328} DagFileProcessor31 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:39.503270
[2018-04-19 21:03:39,507] {jobs.py:351} DagFileProcessor31 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.133 seconds
[2018-04-19 21:03:40,592] {jobs.py:343} DagFileProcessor32 INFO - Started process (PID=3482) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:40,597] {jobs.py:534} DagFileProcessor32 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:40,598] {jobs.py:1521} DagFileProcessor32 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:40,598] {models.py:167} DagFileProcessor32 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:40,701] {jobs.py:1535} DagFileProcessor32 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:40,724] {models.py:322} DagFileProcessor32 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:40,724] {models.py:328} DagFileProcessor32 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:40.724610
[2018-04-19 21:03:40,728] {jobs.py:351} DagFileProcessor32 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.137 seconds
[2018-04-19 21:03:41,805] {jobs.py:343} DagFileProcessor33 INFO - Started process (PID=3483) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:41,810] {jobs.py:534} DagFileProcessor33 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:41,811] {jobs.py:1521} DagFileProcessor33 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:41,812] {models.py:167} DagFileProcessor33 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:41,914] {jobs.py:1535} DagFileProcessor33 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:41,940] {models.py:322} DagFileProcessor33 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:41,941] {models.py:328} DagFileProcessor33 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:41.940971
[2018-04-19 21:03:41,945] {jobs.py:351} DagFileProcessor33 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.140 seconds
[2018-04-19 21:03:43,023] {jobs.py:343} DagFileProcessor34 INFO - Started process (PID=3485) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:43,027] {jobs.py:534} DagFileProcessor34 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:43,028] {jobs.py:1521} DagFileProcessor34 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:43,028] {models.py:167} DagFileProcessor34 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:43,135] {jobs.py:1535} DagFileProcessor34 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:43,159] {models.py:322} DagFileProcessor34 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:43,159] {models.py:328} DagFileProcessor34 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:43.159553
[2018-04-19 21:03:43,164] {jobs.py:351} DagFileProcessor34 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:03:44,247] {jobs.py:343} DagFileProcessor35 INFO - Started process (PID=3486) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:44,252] {jobs.py:534} DagFileProcessor35 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:44,253] {jobs.py:1521} DagFileProcessor35 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:44,254] {models.py:167} DagFileProcessor35 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:44,358] {jobs.py:1535} DagFileProcessor35 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:44,384] {models.py:322} DagFileProcessor35 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:44,384] {models.py:328} DagFileProcessor35 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:44.384685
[2018-04-19 21:03:44,389] {jobs.py:351} DagFileProcessor35 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.142 seconds
[2018-04-19 21:03:45,471] {jobs.py:343} DagFileProcessor36 INFO - Started process (PID=3487) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:45,476] {jobs.py:534} DagFileProcessor36 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:45,477] {jobs.py:1521} DagFileProcessor36 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:45,478] {models.py:167} DagFileProcessor36 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:45,579] {jobs.py:1535} DagFileProcessor36 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:45,602] {models.py:322} DagFileProcessor36 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:45,602] {models.py:328} DagFileProcessor36 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:45.602575
[2018-04-19 21:03:45,606] {jobs.py:351} DagFileProcessor36 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:03:46,689] {jobs.py:343} DagFileProcessor37 INFO - Started process (PID=3488) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:46,694] {jobs.py:534} DagFileProcessor37 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:46,695] {jobs.py:1521} DagFileProcessor37 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:46,695] {models.py:167} DagFileProcessor37 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:46,799] {jobs.py:1535} DagFileProcessor37 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:46,824] {models.py:322} DagFileProcessor37 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:46,824] {models.py:328} DagFileProcessor37 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:46.824392
[2018-04-19 21:03:46,828] {jobs.py:351} DagFileProcessor37 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:03:47,906] {jobs.py:343} DagFileProcessor38 INFO - Started process (PID=3489) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:47,911] {jobs.py:534} DagFileProcessor38 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:47,912] {jobs.py:1521} DagFileProcessor38 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:47,913] {models.py:167} DagFileProcessor38 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:48,014] {jobs.py:1535} DagFileProcessor38 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:48,036] {models.py:322} DagFileProcessor38 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:48,036] {models.py:328} DagFileProcessor38 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:48.036752
[2018-04-19 21:03:48,041] {jobs.py:351} DagFileProcessor38 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:03:49,118] {jobs.py:343} DagFileProcessor39 INFO - Started process (PID=3490) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:49,123] {jobs.py:534} DagFileProcessor39 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:49,124] {jobs.py:1521} DagFileProcessor39 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:49,124] {models.py:167} DagFileProcessor39 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:49,228] {jobs.py:1535} DagFileProcessor39 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:49,248] {models.py:322} DagFileProcessor39 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:49,249] {models.py:328} DagFileProcessor39 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:49.248912
[2018-04-19 21:03:49,253] {jobs.py:351} DagFileProcessor39 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:03:50,349] {jobs.py:343} DagFileProcessor40 INFO - Started process (PID=3491) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:50,354] {jobs.py:534} DagFileProcessor40 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:50,355] {jobs.py:1521} DagFileProcessor40 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:50,355] {models.py:167} DagFileProcessor40 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:50,457] {jobs.py:1535} DagFileProcessor40 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:50,478] {models.py:322} DagFileProcessor40 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:50,479] {models.py:328} DagFileProcessor40 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:50.478952
[2018-04-19 21:03:50,483] {jobs.py:351} DagFileProcessor40 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:03:51,565] {jobs.py:343} DagFileProcessor41 INFO - Started process (PID=3492) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:51,570] {jobs.py:534} DagFileProcessor41 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:51,571] {jobs.py:1521} DagFileProcessor41 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:51,572] {models.py:167} DagFileProcessor41 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:51,672] {jobs.py:1535} DagFileProcessor41 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:51,694] {models.py:322} DagFileProcessor41 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:51,695] {models.py:328} DagFileProcessor41 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:51.694925
[2018-04-19 21:03:51,699] {jobs.py:351} DagFileProcessor41 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:03:52,782] {jobs.py:343} DagFileProcessor42 INFO - Started process (PID=3494) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:52,787] {jobs.py:534} DagFileProcessor42 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:52,788] {jobs.py:1521} DagFileProcessor42 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:52,788] {models.py:167} DagFileProcessor42 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:52,909] {jobs.py:1535} DagFileProcessor42 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:52,935] {models.py:322} DagFileProcessor42 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:52,936] {models.py:328} DagFileProcessor42 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:52.936147
[2018-04-19 21:03:52,941] {jobs.py:351} DagFileProcessor42 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:03:53,999] {jobs.py:343} DagFileProcessor43 INFO - Started process (PID=3495) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:54,004] {jobs.py:534} DagFileProcessor43 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:54,005] {jobs.py:1521} DagFileProcessor43 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:54,005] {models.py:167} DagFileProcessor43 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:54,107] {jobs.py:1535} DagFileProcessor43 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:54,130] {models.py:322} DagFileProcessor43 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:54,131] {models.py:328} DagFileProcessor43 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:54.130813
[2018-04-19 21:03:54,134] {jobs.py:351} DagFileProcessor43 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.136 seconds
[2018-04-19 21:03:55,210] {jobs.py:343} DagFileProcessor44 INFO - Started process (PID=3497) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:55,215] {jobs.py:534} DagFileProcessor44 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:55,216] {jobs.py:1521} DagFileProcessor44 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:55,216] {models.py:167} DagFileProcessor44 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:55,328] {jobs.py:1535} DagFileProcessor44 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:55,360] {models.py:322} DagFileProcessor44 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:55,361] {models.py:328} DagFileProcessor44 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:55.361405
[2018-04-19 21:03:55,368] {jobs.py:351} DagFileProcessor44 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:03:56,436] {jobs.py:343} DagFileProcessor45 INFO - Started process (PID=3498) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:56,446] {jobs.py:534} DagFileProcessor45 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:56,447] {jobs.py:1521} DagFileProcessor45 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:56,448] {models.py:167} DagFileProcessor45 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:56,550] {jobs.py:1535} DagFileProcessor45 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:56,575] {models.py:322} DagFileProcessor45 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:56,575] {models.py:328} DagFileProcessor45 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:56.575751
[2018-04-19 21:03:56,580] {jobs.py:351} DagFileProcessor45 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:03:57,652] {jobs.py:343} DagFileProcessor46 INFO - Started process (PID=3499) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:57,657] {jobs.py:534} DagFileProcessor46 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:57,658] {jobs.py:1521} DagFileProcessor46 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:57,658] {models.py:167} DagFileProcessor46 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:57,758] {jobs.py:1535} DagFileProcessor46 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:57,781] {models.py:322} DagFileProcessor46 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:57,782] {models.py:328} DagFileProcessor46 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:57.781826
[2018-04-19 21:03:57,786] {jobs.py:351} DagFileProcessor46 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:03:58,865] {jobs.py:343} DagFileProcessor47 INFO - Started process (PID=3500) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:58,870] {jobs.py:534} DagFileProcessor47 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:03:58,871] {jobs.py:1521} DagFileProcessor47 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:03:58,872] {models.py:167} DagFileProcessor47 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:58,970] {jobs.py:1535} DagFileProcessor47 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:03:58,994] {models.py:322} DagFileProcessor47 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:03:58,994] {models.py:328} DagFileProcessor47 INFO - Failing jobs without heartbeat after 2018-04-19 20:58:58.994672
[2018-04-19 21:03:58,998] {jobs.py:351} DagFileProcessor47 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.133 seconds
[2018-04-19 21:04:00,090] {jobs.py:343} DagFileProcessor48 INFO - Started process (PID=3501) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:00,095] {jobs.py:534} DagFileProcessor48 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:00,096] {jobs.py:1521} DagFileProcessor48 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:00,096] {models.py:167} DagFileProcessor48 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:00,195] {jobs.py:1535} DagFileProcessor48 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:00,220] {models.py:322} DagFileProcessor48 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:00,220] {models.py:328} DagFileProcessor48 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:00.220656
[2018-04-19 21:04:00,224] {jobs.py:351} DagFileProcessor48 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:04:01,298] {jobs.py:343} DagFileProcessor49 INFO - Started process (PID=3502) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:01,303] {jobs.py:534} DagFileProcessor49 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:01,304] {jobs.py:1521} DagFileProcessor49 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:01,305] {models.py:167} DagFileProcessor49 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:01,413] {jobs.py:1535} DagFileProcessor49 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:01,438] {models.py:322} DagFileProcessor49 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:01,438] {models.py:328} DagFileProcessor49 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:01.438649
[2018-04-19 21:04:01,448] {jobs.py:351} DagFileProcessor49 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:04:02,525] {jobs.py:343} DagFileProcessor50 INFO - Started process (PID=3504) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:02,530] {jobs.py:534} DagFileProcessor50 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:02,531] {jobs.py:1521} DagFileProcessor50 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:02,532] {models.py:167} DagFileProcessor50 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:02,630] {jobs.py:1535} DagFileProcessor50 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:02,654] {models.py:322} DagFileProcessor50 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:02,655] {models.py:328} DagFileProcessor50 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:02.655208
[2018-04-19 21:04:02,659] {jobs.py:351} DagFileProcessor50 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:04:03,740] {jobs.py:343} DagFileProcessor51 INFO - Started process (PID=3505) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:03,744] {jobs.py:534} DagFileProcessor51 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:03,745] {jobs.py:1521} DagFileProcessor51 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:03,746] {models.py:167} DagFileProcessor51 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:03,855] {jobs.py:1535} DagFileProcessor51 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:03,880] {models.py:322} DagFileProcessor51 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:03,880] {models.py:328} DagFileProcessor51 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:03.880400
[2018-04-19 21:04:03,884] {jobs.py:351} DagFileProcessor51 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:04:04,951] {jobs.py:343} DagFileProcessor52 INFO - Started process (PID=3506) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:04,956] {jobs.py:534} DagFileProcessor52 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:04,957] {jobs.py:1521} DagFileProcessor52 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:04,958] {models.py:167} DagFileProcessor52 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:05,055] {jobs.py:1535} DagFileProcessor52 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:05,081] {models.py:322} DagFileProcessor52 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:05,082] {models.py:328} DagFileProcessor52 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:05.082195
[2018-04-19 21:04:05,086] {jobs.py:351} DagFileProcessor52 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:04:06,168] {jobs.py:343} DagFileProcessor53 INFO - Started process (PID=3507) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:06,173] {jobs.py:534} DagFileProcessor53 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:06,174] {jobs.py:1521} DagFileProcessor53 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:06,174] {models.py:167} DagFileProcessor53 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:06,279] {jobs.py:1535} DagFileProcessor53 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:06,303] {models.py:322} DagFileProcessor53 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:06,304] {models.py:328} DagFileProcessor53 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:06.304167
[2018-04-19 21:04:06,308] {jobs.py:351} DagFileProcessor53 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.140 seconds
[2018-04-19 21:04:07,385] {jobs.py:343} DagFileProcessor54 INFO - Started process (PID=3508) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:07,390] {jobs.py:534} DagFileProcessor54 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:07,391] {jobs.py:1521} DagFileProcessor54 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:07,391] {models.py:167} DagFileProcessor54 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:07,492] {jobs.py:1535} DagFileProcessor54 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:07,516] {models.py:322} DagFileProcessor54 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:07,516] {models.py:328} DagFileProcessor54 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:07.516603
[2018-04-19 21:04:07,521] {jobs.py:351} DagFileProcessor54 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.136 seconds
[2018-04-19 21:04:08,614] {jobs.py:343} DagFileProcessor55 INFO - Started process (PID=3509) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:08,619] {jobs.py:534} DagFileProcessor55 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:08,620] {jobs.py:1521} DagFileProcessor55 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:08,620] {models.py:167} DagFileProcessor55 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:08,720] {jobs.py:1535} DagFileProcessor55 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:08,745] {models.py:322} DagFileProcessor55 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:08,746] {models.py:328} DagFileProcessor55 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:08.746378
[2018-04-19 21:04:08,750] {jobs.py:351} DagFileProcessor55 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.136 seconds
[2018-04-19 21:04:09,831] {jobs.py:343} DagFileProcessor56 INFO - Started process (PID=3510) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:09,836] {jobs.py:534} DagFileProcessor56 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:09,837] {jobs.py:1521} DagFileProcessor56 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:09,837] {models.py:167} DagFileProcessor56 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:09,938] {jobs.py:1535} DagFileProcessor56 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:09,961] {models.py:322} DagFileProcessor56 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:09,961] {models.py:328} DagFileProcessor56 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:09.961541
[2018-04-19 21:04:09,965] {jobs.py:351} DagFileProcessor56 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:04:11,050] {jobs.py:343} DagFileProcessor57 INFO - Started process (PID=3511) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:11,055] {jobs.py:534} DagFileProcessor57 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:11,056] {jobs.py:1521} DagFileProcessor57 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:11,056] {models.py:167} DagFileProcessor57 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:11,157] {jobs.py:1535} DagFileProcessor57 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:11,178] {models.py:322} DagFileProcessor57 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:11,179] {models.py:328} DagFileProcessor57 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:11.179078
[2018-04-19 21:04:11,183] {jobs.py:351} DagFileProcessor57 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.133 seconds
[2018-04-19 21:04:12,271] {jobs.py:343} DagFileProcessor58 INFO - Started process (PID=3513) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:12,276] {jobs.py:534} DagFileProcessor58 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:12,277] {jobs.py:1521} DagFileProcessor58 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:12,278] {models.py:167} DagFileProcessor58 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:12,384] {jobs.py:1535} DagFileProcessor58 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:12,410] {models.py:322} DagFileProcessor58 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:12,410] {models.py:328} DagFileProcessor58 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:12.410446
[2018-04-19 21:04:12,415] {jobs.py:351} DagFileProcessor58 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:04:13,486] {jobs.py:343} DagFileProcessor59 INFO - Started process (PID=3514) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:13,491] {jobs.py:534} DagFileProcessor59 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:13,492] {jobs.py:1521} DagFileProcessor59 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:13,492] {models.py:167} DagFileProcessor59 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:13,617] {jobs.py:1535} DagFileProcessor59 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:13,644] {models.py:322} DagFileProcessor59 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:13,645] {models.py:328} DagFileProcessor59 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:13.645407
[2018-04-19 21:04:13,651] {jobs.py:351} DagFileProcessor59 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.165 seconds
[2018-04-19 21:04:14,715] {jobs.py:343} DagFileProcessor60 INFO - Started process (PID=3515) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:14,719] {jobs.py:534} DagFileProcessor60 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:14,720] {jobs.py:1521} DagFileProcessor60 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:14,721] {models.py:167} DagFileProcessor60 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:14,821] {jobs.py:1535} DagFileProcessor60 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:14,844] {models.py:322} DagFileProcessor60 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:14,844] {models.py:328} DagFileProcessor60 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:14.844680
[2018-04-19 21:04:14,848] {jobs.py:351} DagFileProcessor60 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:04:15,934] {jobs.py:343} DagFileProcessor61 INFO - Started process (PID=3516) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:15,939] {jobs.py:534} DagFileProcessor61 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:15,940] {jobs.py:1521} DagFileProcessor61 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:15,940] {models.py:167} DagFileProcessor61 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:16,054] {jobs.py:1535} DagFileProcessor61 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:16,080] {models.py:322} DagFileProcessor61 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:16,081] {models.py:328} DagFileProcessor61 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:16.081022
[2018-04-19 21:04:16,085] {jobs.py:351} DagFileProcessor61 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:04:17,148] {jobs.py:343} DagFileProcessor62 INFO - Started process (PID=3517) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:17,153] {jobs.py:534} DagFileProcessor62 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:17,154] {jobs.py:1521} DagFileProcessor62 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:17,154] {models.py:167} DagFileProcessor62 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:17,258] {jobs.py:1535} DagFileProcessor62 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:17,279] {models.py:322} DagFileProcessor62 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:17,280] {models.py:328} DagFileProcessor62 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:17.280219
[2018-04-19 21:04:17,284] {jobs.py:351} DagFileProcessor62 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.137 seconds
[2018-04-19 21:04:18,369] {jobs.py:343} DagFileProcessor63 INFO - Started process (PID=3518) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:18,374] {jobs.py:534} DagFileProcessor63 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:18,375] {jobs.py:1521} DagFileProcessor63 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:18,375] {models.py:167} DagFileProcessor63 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:18,482] {jobs.py:1535} DagFileProcessor63 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:18,506] {models.py:322} DagFileProcessor63 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:18,506] {models.py:328} DagFileProcessor63 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:18.506389
[2018-04-19 21:04:18,513] {jobs.py:351} DagFileProcessor63 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:04:19,587] {jobs.py:343} DagFileProcessor64 INFO - Started process (PID=3519) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:19,592] {jobs.py:534} DagFileProcessor64 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:19,593] {jobs.py:1521} DagFileProcessor64 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:19,593] {models.py:167} DagFileProcessor64 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:19,693] {jobs.py:1535} DagFileProcessor64 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:19,716] {models.py:322} DagFileProcessor64 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:19,716] {models.py:328} DagFileProcessor64 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:19.716795
[2018-04-19 21:04:19,721] {jobs.py:351} DagFileProcessor64 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:04:20,812] {jobs.py:343} DagFileProcessor65 INFO - Started process (PID=3527) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:20,817] {jobs.py:534} DagFileProcessor65 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:20,818] {jobs.py:1521} DagFileProcessor65 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:20,818] {models.py:167} DagFileProcessor65 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:20,921] {jobs.py:1535} DagFileProcessor65 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:20,942] {models.py:322} DagFileProcessor65 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:20,943] {models.py:328} DagFileProcessor65 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:20.942855
[2018-04-19 21:04:20,947] {jobs.py:351} DagFileProcessor65 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:04:22,031] {jobs.py:343} DagFileProcessor66 INFO - Started process (PID=3529) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:22,036] {jobs.py:534} DagFileProcessor66 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:22,037] {jobs.py:1521} DagFileProcessor66 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:22,038] {models.py:167} DagFileProcessor66 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:22,149] {jobs.py:1535} DagFileProcessor66 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:22,173] {models.py:322} DagFileProcessor66 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:22,173] {models.py:328} DagFileProcessor66 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:22.173584
[2018-04-19 21:04:22,178] {jobs.py:351} DagFileProcessor66 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:04:23,246] {jobs.py:343} DagFileProcessor67 INFO - Started process (PID=3530) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:23,251] {jobs.py:534} DagFileProcessor67 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:23,252] {jobs.py:1521} DagFileProcessor67 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:23,253] {models.py:167} DagFileProcessor67 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:23,355] {jobs.py:1535} DagFileProcessor67 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:23,376] {models.py:322} DagFileProcessor67 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:23,377] {models.py:328} DagFileProcessor67 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:23.377322
[2018-04-19 21:04:23,381] {jobs.py:351} DagFileProcessor67 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:04:24,459] {jobs.py:343} DagFileProcessor68 INFO - Started process (PID=3531) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:24,464] {jobs.py:534} DagFileProcessor68 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:24,465] {jobs.py:1521} DagFileProcessor68 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:24,466] {models.py:167} DagFileProcessor68 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:24,570] {jobs.py:1535} DagFileProcessor68 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:24,593] {models.py:322} DagFileProcessor68 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:24,594] {models.py:328} DagFileProcessor68 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:24.594206
[2018-04-19 21:04:24,598] {jobs.py:351} DagFileProcessor68 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:04:25,674] {jobs.py:343} DagFileProcessor69 INFO - Started process (PID=3532) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:25,679] {jobs.py:534} DagFileProcessor69 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:25,680] {jobs.py:1521} DagFileProcessor69 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:25,681] {models.py:167} DagFileProcessor69 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:25,793] {jobs.py:1535} DagFileProcessor69 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:25,816] {models.py:322} DagFileProcessor69 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:25,817] {models.py:328} DagFileProcessor69 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:25.816802
[2018-04-19 21:04:25,821] {jobs.py:351} DagFileProcessor69 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:04:26,902] {jobs.py:343} DagFileProcessor70 INFO - Started process (PID=3533) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:26,907] {jobs.py:534} DagFileProcessor70 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:26,908] {jobs.py:1521} DagFileProcessor70 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:26,909] {models.py:167} DagFileProcessor70 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:27,026] {jobs.py:1535} DagFileProcessor70 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:27,048] {models.py:322} DagFileProcessor70 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:27,048] {models.py:328} DagFileProcessor70 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:27.048470
[2018-04-19 21:04:27,052] {jobs.py:351} DagFileProcessor70 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:04:28,116] {jobs.py:343} DagFileProcessor71 INFO - Started process (PID=3534) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:28,121] {jobs.py:534} DagFileProcessor71 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:28,122] {jobs.py:1521} DagFileProcessor71 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:28,123] {models.py:167} DagFileProcessor71 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:28,236] {jobs.py:1535} DagFileProcessor71 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:28,260] {models.py:322} DagFileProcessor71 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:28,261] {models.py:328} DagFileProcessor71 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:28.260910
[2018-04-19 21:04:28,265] {jobs.py:351} DagFileProcessor71 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:04:29,329] {jobs.py:343} DagFileProcessor72 INFO - Started process (PID=3535) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:29,334] {jobs.py:534} DagFileProcessor72 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:29,335] {jobs.py:1521} DagFileProcessor72 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:29,335] {models.py:167} DagFileProcessor72 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:29,437] {jobs.py:1535} DagFileProcessor72 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:29,460] {models.py:322} DagFileProcessor72 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:29,460] {models.py:328} DagFileProcessor72 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:29.460368
[2018-04-19 21:04:29,464] {jobs.py:351} DagFileProcessor72 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:04:30,549] {jobs.py:343} DagFileProcessor73 INFO - Started process (PID=3536) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:30,554] {jobs.py:534} DagFileProcessor73 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:30,555] {jobs.py:1521} DagFileProcessor73 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:30,555] {models.py:167} DagFileProcessor73 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:30,664] {jobs.py:1535} DagFileProcessor73 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:30,687] {models.py:322} DagFileProcessor73 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:30,687] {models.py:328} DagFileProcessor73 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:30.687525
[2018-04-19 21:04:30,691] {jobs.py:351} DagFileProcessor73 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:04:31,762] {jobs.py:343} DagFileProcessor74 INFO - Started process (PID=3537) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:31,767] {jobs.py:534} DagFileProcessor74 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:31,768] {jobs.py:1521} DagFileProcessor74 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:31,768] {models.py:167} DagFileProcessor74 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:31,871] {jobs.py:1535} DagFileProcessor74 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:31,892] {models.py:322} DagFileProcessor74 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:31,892] {models.py:328} DagFileProcessor74 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:31.892679
[2018-04-19 21:04:31,896] {jobs.py:351} DagFileProcessor74 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:04:32,987] {jobs.py:343} DagFileProcessor75 INFO - Started process (PID=3539) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:32,992] {jobs.py:534} DagFileProcessor75 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:32,993] {jobs.py:1521} DagFileProcessor75 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:32,993] {models.py:167} DagFileProcessor75 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:33,098] {jobs.py:1535} DagFileProcessor75 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:33,121] {models.py:322} DagFileProcessor75 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:33,121] {models.py:328} DagFileProcessor75 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:33.121801
[2018-04-19 21:04:33,125] {jobs.py:351} DagFileProcessor75 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:04:34,206] {jobs.py:343} DagFileProcessor76 INFO - Started process (PID=3540) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:34,214] {jobs.py:534} DagFileProcessor76 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:34,216] {jobs.py:1521} DagFileProcessor76 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:34,216] {models.py:167} DagFileProcessor76 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:34,329] {jobs.py:1535} DagFileProcessor76 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:34,351] {models.py:322} DagFileProcessor76 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:34,351] {models.py:328} DagFileProcessor76 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:34.351472
[2018-04-19 21:04:34,355] {jobs.py:351} DagFileProcessor76 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:04:35,421] {jobs.py:343} DagFileProcessor77 INFO - Started process (PID=3541) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:35,426] {jobs.py:534} DagFileProcessor77 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:35,427] {jobs.py:1521} DagFileProcessor77 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:35,427] {models.py:167} DagFileProcessor77 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:35,532] {jobs.py:1535} DagFileProcessor77 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:35,555] {models.py:322} DagFileProcessor77 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:35,556] {models.py:328} DagFileProcessor77 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:35.555871
[2018-04-19 21:04:35,560] {jobs.py:351} DagFileProcessor77 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:04:36,639] {jobs.py:343} DagFileProcessor78 INFO - Started process (PID=3542) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:36,643] {jobs.py:534} DagFileProcessor78 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:36,644] {jobs.py:1521} DagFileProcessor78 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:36,645] {models.py:167} DagFileProcessor78 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:36,752] {jobs.py:1535} DagFileProcessor78 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:36,776] {models.py:322} DagFileProcessor78 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:36,777] {models.py:328} DagFileProcessor78 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:36.776954
[2018-04-19 21:04:36,781] {jobs.py:351} DagFileProcessor78 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.142 seconds
[2018-04-19 21:04:37,853] {jobs.py:343} DagFileProcessor79 INFO - Started process (PID=3543) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:37,858] {jobs.py:534} DagFileProcessor79 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:37,859] {jobs.py:1521} DagFileProcessor79 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:37,860] {models.py:167} DagFileProcessor79 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:37,983] {jobs.py:1535} DagFileProcessor79 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:38,004] {models.py:322} DagFileProcessor79 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:38,005] {models.py:328} DagFileProcessor79 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:38.004887
[2018-04-19 21:04:38,010] {jobs.py:351} DagFileProcessor79 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:04:39,076] {jobs.py:343} DagFileProcessor80 INFO - Started process (PID=3544) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:39,083] {jobs.py:534} DagFileProcessor80 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:39,090] {jobs.py:1521} DagFileProcessor80 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:39,092] {models.py:167} DagFileProcessor80 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:39,220] {jobs.py:1535} DagFileProcessor80 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:39,241] {models.py:322} DagFileProcessor80 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:39,241] {models.py:328} DagFileProcessor80 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:39.241525
[2018-04-19 21:04:39,246] {jobs.py:351} DagFileProcessor80 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.170 seconds
[2018-04-19 21:04:40,296] {jobs.py:343} DagFileProcessor81 INFO - Started process (PID=3545) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:40,303] {jobs.py:534} DagFileProcessor81 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:40,307] {jobs.py:1521} DagFileProcessor81 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:40,307] {models.py:167} DagFileProcessor81 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:40,424] {jobs.py:1535} DagFileProcessor81 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:40,445] {models.py:322} DagFileProcessor81 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:40,445] {models.py:328} DagFileProcessor81 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:40.445335
[2018-04-19 21:04:40,450] {jobs.py:351} DagFileProcessor81 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:04:41,504] {jobs.py:343} DagFileProcessor82 INFO - Started process (PID=3546) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:41,509] {jobs.py:534} DagFileProcessor82 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:41,513] {jobs.py:1521} DagFileProcessor82 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:41,513] {models.py:167} DagFileProcessor82 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:41,624] {jobs.py:1535} DagFileProcessor82 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:41,647] {models.py:322} DagFileProcessor82 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:41,647] {models.py:328} DagFileProcessor82 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:41.647558
[2018-04-19 21:04:41,652] {jobs.py:351} DagFileProcessor82 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:04:42,724] {jobs.py:343} DagFileProcessor83 INFO - Started process (PID=3548) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:42,729] {jobs.py:534} DagFileProcessor83 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:42,734] {jobs.py:1521} DagFileProcessor83 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:42,734] {models.py:167} DagFileProcessor83 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:42,840] {jobs.py:1535} DagFileProcessor83 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:42,864] {models.py:322} DagFileProcessor83 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:42,864] {models.py:328} DagFileProcessor83 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:42.864467
[2018-04-19 21:04:42,869] {jobs.py:351} DagFileProcessor83 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:04:43,944] {jobs.py:343} DagFileProcessor84 INFO - Started process (PID=3549) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:43,950] {jobs.py:534} DagFileProcessor84 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:43,954] {jobs.py:1521} DagFileProcessor84 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:43,954] {models.py:167} DagFileProcessor84 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:44,053] {jobs.py:1535} DagFileProcessor84 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:44,075] {models.py:322} DagFileProcessor84 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:44,075] {models.py:328} DagFileProcessor84 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:44.075549
[2018-04-19 21:04:44,079] {jobs.py:351} DagFileProcessor84 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:04:45,175] {jobs.py:343} DagFileProcessor85 INFO - Started process (PID=3550) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:45,180] {jobs.py:534} DagFileProcessor85 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:45,181] {jobs.py:1521} DagFileProcessor85 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:45,181] {models.py:167} DagFileProcessor85 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:45,280] {jobs.py:1535} DagFileProcessor85 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:45,301] {models.py:322} DagFileProcessor85 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:45,302] {models.py:328} DagFileProcessor85 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:45.302269
[2018-04-19 21:04:45,306] {jobs.py:351} DagFileProcessor85 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.131 seconds
[2018-04-19 21:04:46,397] {jobs.py:343} DagFileProcessor86 INFO - Started process (PID=3551) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:46,402] {jobs.py:534} DagFileProcessor86 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:46,403] {jobs.py:1521} DagFileProcessor86 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:46,403] {models.py:167} DagFileProcessor86 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:46,508] {jobs.py:1535} DagFileProcessor86 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:46,533] {models.py:322} DagFileProcessor86 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:46,533] {models.py:328} DagFileProcessor86 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:46.533717
[2018-04-19 21:04:46,538] {jobs.py:351} DagFileProcessor86 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:04:47,609] {jobs.py:343} DagFileProcessor87 INFO - Started process (PID=3552) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:47,614] {jobs.py:534} DagFileProcessor87 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:47,615] {jobs.py:1521} DagFileProcessor87 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:47,615] {models.py:167} DagFileProcessor87 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:47,714] {jobs.py:1535} DagFileProcessor87 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:47,736] {models.py:322} DagFileProcessor87 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:47,737] {models.py:328} DagFileProcessor87 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:47.736917
[2018-04-19 21:04:47,741] {jobs.py:351} DagFileProcessor87 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.132 seconds
[2018-04-19 21:04:48,827] {jobs.py:343} DagFileProcessor88 INFO - Started process (PID=3553) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:48,832] {jobs.py:534} DagFileProcessor88 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:48,833] {jobs.py:1521} DagFileProcessor88 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:48,833] {models.py:167} DagFileProcessor88 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:48,932] {jobs.py:1535} DagFileProcessor88 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:48,955] {models.py:322} DagFileProcessor88 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:48,956] {models.py:328} DagFileProcessor88 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:48.956218
[2018-04-19 21:04:48,960] {jobs.py:351} DagFileProcessor88 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.133 seconds
[2018-04-19 21:04:50,044] {jobs.py:343} DagFileProcessor89 INFO - Started process (PID=3554) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:50,049] {jobs.py:534} DagFileProcessor89 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:50,050] {jobs.py:1521} DagFileProcessor89 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:50,050] {models.py:167} DagFileProcessor89 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:50,150] {jobs.py:1535} DagFileProcessor89 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:50,171] {models.py:322} DagFileProcessor89 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:50,172] {models.py:328} DagFileProcessor89 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:50.172138
[2018-04-19 21:04:50,176] {jobs.py:351} DagFileProcessor89 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.132 seconds
[2018-04-19 21:04:51,277] {jobs.py:343} DagFileProcessor90 INFO - Started process (PID=3555) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:51,283] {jobs.py:534} DagFileProcessor90 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:51,284] {jobs.py:1521} DagFileProcessor90 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:51,285] {models.py:167} DagFileProcessor90 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:51,394] {jobs.py:1535} DagFileProcessor90 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:51,417] {models.py:322} DagFileProcessor90 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:51,418] {models.py:328} DagFileProcessor90 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:51.418342
[2018-04-19 21:04:51,422] {jobs.py:351} DagFileProcessor90 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:04:52,495] {jobs.py:343} DagFileProcessor91 INFO - Started process (PID=3557) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:52,501] {jobs.py:534} DagFileProcessor91 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:52,502] {jobs.py:1521} DagFileProcessor91 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:52,503] {models.py:167} DagFileProcessor91 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:52,611] {jobs.py:1535} DagFileProcessor91 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:52,635] {models.py:322} DagFileProcessor91 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:52,635] {models.py:328} DagFileProcessor91 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:52.635681
[2018-04-19 21:04:52,639] {jobs.py:351} DagFileProcessor91 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:04:53,712] {jobs.py:343} DagFileProcessor92 INFO - Started process (PID=3558) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:53,717] {jobs.py:534} DagFileProcessor92 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:53,718] {jobs.py:1521} DagFileProcessor92 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:53,719] {models.py:167} DagFileProcessor92 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:53,827] {jobs.py:1535} DagFileProcessor92 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:53,851] {models.py:322} DagFileProcessor92 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:53,851] {models.py:328} DagFileProcessor92 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:53.851599
[2018-04-19 21:04:53,855] {jobs.py:351} DagFileProcessor92 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:04:54,930] {jobs.py:343} DagFileProcessor93 INFO - Started process (PID=3559) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:54,935] {jobs.py:534} DagFileProcessor93 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:54,936] {jobs.py:1521} DagFileProcessor93 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:54,937] {models.py:167} DagFileProcessor93 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:55,040] {jobs.py:1535} DagFileProcessor93 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:55,063] {models.py:322} DagFileProcessor93 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:55,063] {models.py:328} DagFileProcessor93 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:55.063561
[2018-04-19 21:04:55,067] {jobs.py:351} DagFileProcessor93 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.138 seconds
[2018-04-19 21:04:56,147] {jobs.py:343} DagFileProcessor94 INFO - Started process (PID=3560) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:56,166] {jobs.py:534} DagFileProcessor94 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:56,168] {jobs.py:1521} DagFileProcessor94 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:56,168] {models.py:167} DagFileProcessor94 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:56,272] {jobs.py:1535} DagFileProcessor94 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:56,294] {models.py:322} DagFileProcessor94 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:56,295] {models.py:328} DagFileProcessor94 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:56.295109
[2018-04-19 21:04:56,300] {jobs.py:351} DagFileProcessor94 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:04:57,381] {jobs.py:343} DagFileProcessor95 INFO - Started process (PID=3561) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:57,386] {jobs.py:534} DagFileProcessor95 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:57,388] {jobs.py:1521} DagFileProcessor95 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:57,388] {models.py:167} DagFileProcessor95 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:57,486] {jobs.py:1535} DagFileProcessor95 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:57,506] {models.py:322} DagFileProcessor95 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:57,507] {models.py:328} DagFileProcessor95 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:57.507156
[2018-04-19 21:04:57,511] {jobs.py:351} DagFileProcessor95 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.130 seconds
[2018-04-19 21:04:58,596] {jobs.py:343} DagFileProcessor96 INFO - Started process (PID=3562) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:58,601] {jobs.py:534} DagFileProcessor96 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:58,602] {jobs.py:1521} DagFileProcessor96 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:58,603] {models.py:167} DagFileProcessor96 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:58,702] {jobs.py:1535} DagFileProcessor96 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:58,723] {models.py:322} DagFileProcessor96 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:58,724] {models.py:328} DagFileProcessor96 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:58.724049
[2018-04-19 21:04:58,728] {jobs.py:351} DagFileProcessor96 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.131 seconds
[2018-04-19 21:04:59,815] {jobs.py:343} DagFileProcessor97 INFO - Started process (PID=3563) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:59,820] {jobs.py:534} DagFileProcessor97 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:04:59,821] {jobs.py:1521} DagFileProcessor97 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:04:59,822] {models.py:167} DagFileProcessor97 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:59,934] {jobs.py:1535} DagFileProcessor97 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:04:59,958] {models.py:322} DagFileProcessor97 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:04:59,959] {models.py:328} DagFileProcessor97 INFO - Failing jobs without heartbeat after 2018-04-19 20:59:59.958856
[2018-04-19 21:04:59,963] {jobs.py:351} DagFileProcessor97 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:05:01,030] {jobs.py:343} DagFileProcessor98 INFO - Started process (PID=3564) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:01,036] {jobs.py:534} DagFileProcessor98 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:01,037] {jobs.py:1521} DagFileProcessor98 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:01,038] {models.py:167} DagFileProcessor98 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:01,141] {jobs.py:1535} DagFileProcessor98 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:01,163] {models.py:322} DagFileProcessor98 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:01,164] {models.py:328} DagFileProcessor98 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:01.163888
[2018-04-19 21:05:01,168] {jobs.py:351} DagFileProcessor98 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.138 seconds
[2018-04-19 21:05:02,251] {jobs.py:343} DagFileProcessor99 INFO - Started process (PID=3566) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:02,256] {jobs.py:534} DagFileProcessor99 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:02,257] {jobs.py:1521} DagFileProcessor99 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:02,257] {models.py:167} DagFileProcessor99 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:02,361] {jobs.py:1535} DagFileProcessor99 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:02,383] {models.py:322} DagFileProcessor99 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:02,384] {models.py:328} DagFileProcessor99 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:02.384223
[2018-04-19 21:05:02,388] {jobs.py:351} DagFileProcessor99 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.138 seconds
[2018-04-19 21:05:03,479] {jobs.py:343} DagFileProcessor100 INFO - Started process (PID=3567) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:03,484] {jobs.py:534} DagFileProcessor100 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:03,485] {jobs.py:1521} DagFileProcessor100 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:03,485] {models.py:167} DagFileProcessor100 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:03,585] {jobs.py:1535} DagFileProcessor100 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:03,607] {models.py:322} DagFileProcessor100 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:03,607] {models.py:328} DagFileProcessor100 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:03.607423
[2018-04-19 21:05:03,611] {jobs.py:351} DagFileProcessor100 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.132 seconds
[2018-04-19 21:05:04,697] {jobs.py:343} DagFileProcessor101 INFO - Started process (PID=3568) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:04,702] {jobs.py:534} DagFileProcessor101 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:04,703] {jobs.py:1521} DagFileProcessor101 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:04,703] {models.py:167} DagFileProcessor101 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:04,815] {jobs.py:1535} DagFileProcessor101 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:04,836] {models.py:322} DagFileProcessor101 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:04,837] {models.py:328} DagFileProcessor101 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:04.837285
[2018-04-19 21:05:04,841] {jobs.py:351} DagFileProcessor101 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:05:05,915] {jobs.py:343} DagFileProcessor102 INFO - Started process (PID=3569) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:05,920] {jobs.py:534} DagFileProcessor102 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:05,921] {jobs.py:1521} DagFileProcessor102 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:05,922] {models.py:167} DagFileProcessor102 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:06,024] {jobs.py:1535} DagFileProcessor102 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:06,045] {models.py:322} DagFileProcessor102 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:06,046] {models.py:328} DagFileProcessor102 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:06.046072
[2018-04-19 21:05:06,050] {jobs.py:351} DagFileProcessor102 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:05:07,133] {jobs.py:343} DagFileProcessor103 INFO - Started process (PID=3570) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:07,138] {jobs.py:534} DagFileProcessor103 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:07,139] {jobs.py:1521} DagFileProcessor103 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:07,140] {models.py:167} DagFileProcessor103 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:07,239] {jobs.py:1535} DagFileProcessor103 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:07,263] {models.py:322} DagFileProcessor103 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:07,264] {models.py:328} DagFileProcessor103 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:07.264053
[2018-04-19 21:05:07,268] {jobs.py:351} DagFileProcessor103 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:05:08,353] {jobs.py:343} DagFileProcessor104 INFO - Started process (PID=3571) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:08,358] {jobs.py:534} DagFileProcessor104 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:08,359] {jobs.py:1521} DagFileProcessor104 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:08,359] {models.py:167} DagFileProcessor104 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:08,461] {jobs.py:1535} DagFileProcessor104 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:08,483] {models.py:322} DagFileProcessor104 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:08,484] {models.py:328} DagFileProcessor104 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:08.484041
[2018-04-19 21:05:08,488] {jobs.py:351} DagFileProcessor104 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:05:09,588] {jobs.py:343} DagFileProcessor105 INFO - Started process (PID=3572) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:09,593] {jobs.py:534} DagFileProcessor105 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:09,595] {jobs.py:1521} DagFileProcessor105 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:09,595] {models.py:167} DagFileProcessor105 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:09,695] {jobs.py:1535} DagFileProcessor105 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:09,716] {models.py:322} DagFileProcessor105 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:09,717] {models.py:328} DagFileProcessor105 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:09.717358
[2018-04-19 21:05:09,721] {jobs.py:351} DagFileProcessor105 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.133 seconds
[2018-04-19 21:05:10,805] {jobs.py:343} DagFileProcessor106 INFO - Started process (PID=3573) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:10,810] {jobs.py:534} DagFileProcessor106 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:10,811] {jobs.py:1521} DagFileProcessor106 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:10,811] {models.py:167} DagFileProcessor106 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:10,910] {jobs.py:1535} DagFileProcessor106 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:10,933] {models.py:322} DagFileProcessor106 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:10,933] {models.py:328} DagFileProcessor106 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:10.933613
[2018-04-19 21:05:10,937] {jobs.py:351} DagFileProcessor106 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.133 seconds
[2018-04-19 21:05:12,023] {jobs.py:343} DagFileProcessor107 INFO - Started process (PID=3581) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:12,028] {jobs.py:534} DagFileProcessor107 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:12,029] {jobs.py:1521} DagFileProcessor107 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:12,029] {models.py:167} DagFileProcessor107 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:12,144] {jobs.py:1535} DagFileProcessor107 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:12,165] {models.py:322} DagFileProcessor107 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:12,166] {models.py:328} DagFileProcessor107 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:12.166388
[2018-04-19 21:05:12,170] {jobs.py:351} DagFileProcessor107 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:05:13,240] {jobs.py:343} DagFileProcessor108 INFO - Started process (PID=3583) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:13,245] {jobs.py:534} DagFileProcessor108 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:13,246] {jobs.py:1521} DagFileProcessor108 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:13,246] {models.py:167} DagFileProcessor108 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:13,345] {jobs.py:1535} DagFileProcessor108 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:13,365] {models.py:322} DagFileProcessor108 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:13,365] {models.py:328} DagFileProcessor108 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:13.365573
[2018-04-19 21:05:13,370] {jobs.py:351} DagFileProcessor108 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.130 seconds
[2018-04-19 21:05:14,459] {jobs.py:343} DagFileProcessor109 INFO - Started process (PID=3584) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:14,464] {jobs.py:534} DagFileProcessor109 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:14,465] {jobs.py:1521} DagFileProcessor109 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:14,466] {models.py:167} DagFileProcessor109 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:14,574] {jobs.py:1535} DagFileProcessor109 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:14,595] {models.py:322} DagFileProcessor109 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:14,596] {models.py:328} DagFileProcessor109 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:14.595808
[2018-04-19 21:05:14,600] {jobs.py:351} DagFileProcessor109 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.140 seconds
[2018-04-19 21:05:15,682] {jobs.py:343} DagFileProcessor110 INFO - Started process (PID=3585) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:15,687] {jobs.py:534} DagFileProcessor110 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:15,689] {jobs.py:1521} DagFileProcessor110 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:15,690] {models.py:167} DagFileProcessor110 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:15,788] {jobs.py:1535} DagFileProcessor110 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:15,809] {models.py:322} DagFileProcessor110 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:15,809] {models.py:328} DagFileProcessor110 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:15.809529
[2018-04-19 21:05:15,813] {jobs.py:351} DagFileProcessor110 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.131 seconds
[2018-04-19 21:05:16,898] {jobs.py:343} DagFileProcessor111 INFO - Started process (PID=3586) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:16,903] {jobs.py:534} DagFileProcessor111 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:16,906] {jobs.py:1521} DagFileProcessor111 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:16,907] {models.py:167} DagFileProcessor111 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:17,014] {jobs.py:1535} DagFileProcessor111 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:17,040] {models.py:322} DagFileProcessor111 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:17,041] {models.py:328} DagFileProcessor111 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:17.040791
[2018-04-19 21:05:17,045] {jobs.py:351} DagFileProcessor111 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:05:18,112] {jobs.py:343} DagFileProcessor112 INFO - Started process (PID=3587) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:18,117] {jobs.py:534} DagFileProcessor112 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:18,120] {jobs.py:1521} DagFileProcessor112 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:18,120] {models.py:167} DagFileProcessor112 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:18,221] {jobs.py:1535} DagFileProcessor112 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:18,242] {models.py:322} DagFileProcessor112 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:18,243] {models.py:328} DagFileProcessor112 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:18.242870
[2018-04-19 21:05:18,247] {jobs.py:351} DagFileProcessor112 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:05:19,330] {jobs.py:343} DagFileProcessor113 INFO - Started process (PID=3588) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:19,335] {jobs.py:534} DagFileProcessor113 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:19,337] {jobs.py:1521} DagFileProcessor113 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:19,337] {models.py:167} DagFileProcessor113 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:19,438] {jobs.py:1535} DagFileProcessor113 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:19,460] {models.py:322} DagFileProcessor113 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:19,461] {models.py:328} DagFileProcessor113 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:19.460884
[2018-04-19 21:05:19,465] {jobs.py:351} DagFileProcessor113 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:05:20,547] {jobs.py:343} DagFileProcessor114 INFO - Started process (PID=3589) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:20,552] {jobs.py:534} DagFileProcessor114 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:20,554] {jobs.py:1521} DagFileProcessor114 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:20,554] {models.py:167} DagFileProcessor114 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:20,654] {jobs.py:1535} DagFileProcessor114 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:20,676] {models.py:322} DagFileProcessor114 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:20,677] {models.py:328} DagFileProcessor114 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:20.677260
[2018-04-19 21:05:20,681] {jobs.py:351} DagFileProcessor114 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:05:21,772] {jobs.py:343} DagFileProcessor115 INFO - Started process (PID=3590) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:21,777] {jobs.py:534} DagFileProcessor115 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:21,778] {jobs.py:1521} DagFileProcessor115 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:21,778] {models.py:167} DagFileProcessor115 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:21,882] {jobs.py:1535} DagFileProcessor115 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:21,903] {models.py:322} DagFileProcessor115 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:21,904] {models.py:328} DagFileProcessor115 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:21.904048
[2018-04-19 21:05:21,908] {jobs.py:351} DagFileProcessor115 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.136 seconds
[2018-04-19 21:05:22,992] {jobs.py:343} DagFileProcessor116 INFO - Started process (PID=3592) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:22,997] {jobs.py:534} DagFileProcessor116 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:22,998] {jobs.py:1521} DagFileProcessor116 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:22,998] {models.py:167} DagFileProcessor116 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:23,104] {jobs.py:1535} DagFileProcessor116 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:23,126] {models.py:322} DagFileProcessor116 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:23,126] {models.py:328} DagFileProcessor116 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:23.126617
[2018-04-19 21:05:23,131] {jobs.py:351} DagFileProcessor116 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.138 seconds
[2018-04-19 21:05:24,211] {jobs.py:343} DagFileProcessor117 INFO - Started process (PID=3593) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:24,216] {jobs.py:534} DagFileProcessor117 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:24,217] {jobs.py:1521} DagFileProcessor117 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:24,217] {models.py:167} DagFileProcessor117 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:24,322] {jobs.py:1535} DagFileProcessor117 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:24,345] {models.py:322} DagFileProcessor117 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:24,345] {models.py:328} DagFileProcessor117 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:24.345577
[2018-04-19 21:05:24,349] {jobs.py:351} DagFileProcessor117 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:05:25,432] {jobs.py:343} DagFileProcessor118 INFO - Started process (PID=3594) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:25,438] {jobs.py:534} DagFileProcessor118 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:25,439] {jobs.py:1521} DagFileProcessor118 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:25,439] {models.py:167} DagFileProcessor118 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:25,539] {jobs.py:1535} DagFileProcessor118 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:25,562] {models.py:322} DagFileProcessor118 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:25,563] {models.py:328} DagFileProcessor118 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:25.563021
[2018-04-19 21:05:25,567] {jobs.py:351} DagFileProcessor118 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:05:26,651] {jobs.py:343} DagFileProcessor119 INFO - Started process (PID=3595) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:26,663] {jobs.py:534} DagFileProcessor119 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:26,664] {jobs.py:1521} DagFileProcessor119 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:26,665] {models.py:167} DagFileProcessor119 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:26,775] {jobs.py:1535} DagFileProcessor119 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:26,798] {models.py:322} DagFileProcessor119 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:26,799] {models.py:328} DagFileProcessor119 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:26.798909
[2018-04-19 21:05:26,804] {jobs.py:351} DagFileProcessor119 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:05:27,875] {jobs.py:343} DagFileProcessor120 INFO - Started process (PID=3596) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:27,880] {jobs.py:534} DagFileProcessor120 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:27,881] {jobs.py:1521} DagFileProcessor120 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:27,882] {models.py:167} DagFileProcessor120 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:27,984] {jobs.py:1535} DagFileProcessor120 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:28,005] {models.py:322} DagFileProcessor120 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:28,005] {models.py:328} DagFileProcessor120 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:28.005450
[2018-04-19 21:05:28,009] {jobs.py:351} DagFileProcessor120 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:05:29,095] {jobs.py:343} DagFileProcessor121 INFO - Started process (PID=3597) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:29,100] {jobs.py:534} DagFileProcessor121 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:29,101] {jobs.py:1521} DagFileProcessor121 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:29,101] {models.py:167} DagFileProcessor121 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:29,206] {jobs.py:1535} DagFileProcessor121 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:29,231] {models.py:322} DagFileProcessor121 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:29,232] {models.py:328} DagFileProcessor121 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:29.231899
[2018-04-19 21:05:29,236] {jobs.py:351} DagFileProcessor121 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:05:30,310] {jobs.py:343} DagFileProcessor122 INFO - Started process (PID=3598) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:30,316] {jobs.py:534} DagFileProcessor122 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:30,318] {jobs.py:1521} DagFileProcessor122 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:30,318] {models.py:167} DagFileProcessor122 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:30,439] {jobs.py:1535} DagFileProcessor122 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:30,466] {models.py:322} DagFileProcessor122 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:30,466] {models.py:328} DagFileProcessor122 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:30.466728
[2018-04-19 21:05:30,472] {jobs.py:351} DagFileProcessor122 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.162 seconds
[2018-04-19 21:05:31,523] {jobs.py:343} DagFileProcessor123 INFO - Started process (PID=3606) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:31,528] {jobs.py:534} DagFileProcessor123 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:31,529] {jobs.py:1521} DagFileProcessor123 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:31,529] {models.py:167} DagFileProcessor123 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:31,636] {jobs.py:1535} DagFileProcessor123 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:31,657] {models.py:322} DagFileProcessor123 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:31,658] {models.py:328} DagFileProcessor123 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:31.658026
[2018-04-19 21:05:31,662] {jobs.py:351} DagFileProcessor123 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:05:32,734] {jobs.py:343} DagFileProcessor124 INFO - Started process (PID=3609) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:32,739] {jobs.py:534} DagFileProcessor124 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:32,740] {jobs.py:1521} DagFileProcessor124 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:32,740] {models.py:167} DagFileProcessor124 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:32,847] {jobs.py:1535} DagFileProcessor124 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:32,869] {models.py:322} DagFileProcessor124 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:32,870] {models.py:328} DagFileProcessor124 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:32.870296
[2018-04-19 21:05:32,875] {jobs.py:351} DagFileProcessor124 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:05:33,960] {jobs.py:343} DagFileProcessor125 INFO - Started process (PID=3610) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:33,964] {jobs.py:534} DagFileProcessor125 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:33,965] {jobs.py:1521} DagFileProcessor125 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:33,966] {models.py:167} DagFileProcessor125 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:34,073] {jobs.py:1535} DagFileProcessor125 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:34,094] {models.py:322} DagFileProcessor125 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:34,095] {models.py:328} DagFileProcessor125 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:34.095202
[2018-04-19 21:05:34,099] {jobs.py:351} DagFileProcessor125 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.140 seconds
[2018-04-19 21:05:35,175] {jobs.py:343} DagFileProcessor126 INFO - Started process (PID=3611) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:35,180] {jobs.py:534} DagFileProcessor126 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:35,181] {jobs.py:1521} DagFileProcessor126 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:35,182] {models.py:167} DagFileProcessor126 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:35,295] {jobs.py:1535} DagFileProcessor126 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:35,316] {models.py:322} DagFileProcessor126 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:35,317] {models.py:328} DagFileProcessor126 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:35.316894
[2018-04-19 21:05:35,322] {jobs.py:351} DagFileProcessor126 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:05:36,389] {jobs.py:343} DagFileProcessor127 INFO - Started process (PID=3613) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:36,394] {jobs.py:534} DagFileProcessor127 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:36,395] {jobs.py:1521} DagFileProcessor127 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:36,395] {models.py:167} DagFileProcessor127 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:36,513] {jobs.py:1535} DagFileProcessor127 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:36,536] {models.py:322} DagFileProcessor127 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:36,537] {models.py:328} DagFileProcessor127 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:36.536847
[2018-04-19 21:05:36,543] {jobs.py:351} DagFileProcessor127 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:05:37,604] {jobs.py:343} DagFileProcessor128 INFO - Started process (PID=3614) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:37,609] {jobs.py:534} DagFileProcessor128 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:37,610] {jobs.py:1521} DagFileProcessor128 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:37,611] {models.py:167} DagFileProcessor128 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:37,726] {jobs.py:1535} DagFileProcessor128 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:37,747] {models.py:322} DagFileProcessor128 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:37,747] {models.py:328} DagFileProcessor128 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:37.747521
[2018-04-19 21:05:37,753] {jobs.py:351} DagFileProcessor128 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:05:38,824] {jobs.py:343} DagFileProcessor129 INFO - Started process (PID=3615) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:38,829] {jobs.py:534} DagFileProcessor129 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:38,830] {jobs.py:1521} DagFileProcessor129 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:38,831] {models.py:167} DagFileProcessor129 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:38,954] {jobs.py:1535} DagFileProcessor129 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:38,981] {models.py:322} DagFileProcessor129 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:38,982] {models.py:328} DagFileProcessor129 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:38.982205
[2018-04-19 21:05:38,987] {jobs.py:351} DagFileProcessor129 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.163 seconds
[2018-04-19 21:05:40,052] {jobs.py:343} DagFileProcessor130 INFO - Started process (PID=3616) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:40,057] {jobs.py:534} DagFileProcessor130 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:40,058] {jobs.py:1521} DagFileProcessor130 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:40,059] {models.py:167} DagFileProcessor130 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:40,176] {jobs.py:1535} DagFileProcessor130 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:40,201] {models.py:322} DagFileProcessor130 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:40,202] {models.py:328} DagFileProcessor130 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:40.201817
[2018-04-19 21:05:40,206] {jobs.py:351} DagFileProcessor130 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.155 seconds
[2018-04-19 21:05:41,263] {jobs.py:343} DagFileProcessor131 INFO - Started process (PID=3617) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:41,268] {jobs.py:534} DagFileProcessor131 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:41,269] {jobs.py:1521} DagFileProcessor131 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:41,269] {models.py:167} DagFileProcessor131 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:41,388] {jobs.py:1535} DagFileProcessor131 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:41,417] {models.py:322} DagFileProcessor131 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:41,417] {models.py:328} DagFileProcessor131 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:41.417706
[2018-04-19 21:05:41,422] {jobs.py:351} DagFileProcessor131 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:05:42,472] {jobs.py:343} DagFileProcessor132 INFO - Started process (PID=3619) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:42,477] {jobs.py:534} DagFileProcessor132 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:42,478] {jobs.py:1521} DagFileProcessor132 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:42,478] {models.py:167} DagFileProcessor132 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:42,594] {jobs.py:1535} DagFileProcessor132 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:42,619] {models.py:322} DagFileProcessor132 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:42,620] {models.py:328} DagFileProcessor132 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:42.620284
[2018-04-19 21:05:42,624] {jobs.py:351} DagFileProcessor132 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:05:43,688] {jobs.py:343} DagFileProcessor133 INFO - Started process (PID=3620) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:43,693] {jobs.py:534} DagFileProcessor133 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:43,695] {jobs.py:1521} DagFileProcessor133 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:43,696] {models.py:167} DagFileProcessor133 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:43,808] {jobs.py:1535} DagFileProcessor133 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:43,836] {models.py:322} DagFileProcessor133 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:43,836] {models.py:328} DagFileProcessor133 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:43.836792
[2018-04-19 21:05:43,841] {jobs.py:351} DagFileProcessor133 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:05:44,903] {jobs.py:343} DagFileProcessor134 INFO - Started process (PID=3621) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:44,908] {jobs.py:534} DagFileProcessor134 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:44,910] {jobs.py:1521} DagFileProcessor134 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:44,910] {models.py:167} DagFileProcessor134 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:45,021] {jobs.py:1535} DagFileProcessor134 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:45,045] {models.py:322} DagFileProcessor134 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:45,046] {models.py:328} DagFileProcessor134 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:45.046147
[2018-04-19 21:05:45,050] {jobs.py:351} DagFileProcessor134 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:05:46,121] {jobs.py:343} DagFileProcessor135 INFO - Started process (PID=3622) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:46,126] {jobs.py:534} DagFileProcessor135 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:46,127] {jobs.py:1521} DagFileProcessor135 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:46,128] {models.py:167} DagFileProcessor135 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:46,241] {jobs.py:1535} DagFileProcessor135 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:46,266] {models.py:322} DagFileProcessor135 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:46,267] {models.py:328} DagFileProcessor135 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:46.267239
[2018-04-19 21:05:46,271] {jobs.py:351} DagFileProcessor135 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:05:47,334] {jobs.py:343} DagFileProcessor136 INFO - Started process (PID=3623) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:47,339] {jobs.py:534} DagFileProcessor136 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:47,340] {jobs.py:1521} DagFileProcessor136 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:47,340] {models.py:167} DagFileProcessor136 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:47,452] {jobs.py:1535} DagFileProcessor136 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:47,477] {models.py:322} DagFileProcessor136 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:47,477] {models.py:328} DagFileProcessor136 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:47.477456
[2018-04-19 21:05:47,482] {jobs.py:351} DagFileProcessor136 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:05:48,544] {jobs.py:343} DagFileProcessor137 INFO - Started process (PID=3624) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:48,549] {jobs.py:534} DagFileProcessor137 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:48,550] {jobs.py:1521} DagFileProcessor137 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:48,550] {models.py:167} DagFileProcessor137 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:48,661] {jobs.py:1535} DagFileProcessor137 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:48,685] {models.py:322} DagFileProcessor137 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:48,685] {models.py:328} DagFileProcessor137 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:48.685774
[2018-04-19 21:05:48,690] {jobs.py:351} DagFileProcessor137 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:05:49,761] {jobs.py:343} DagFileProcessor138 INFO - Started process (PID=3625) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:49,770] {jobs.py:534} DagFileProcessor138 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:49,772] {jobs.py:1521} DagFileProcessor138 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:49,772] {models.py:167} DagFileProcessor138 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:49,879] {jobs.py:1535} DagFileProcessor138 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:49,901] {models.py:322} DagFileProcessor138 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:49,902] {models.py:328} DagFileProcessor138 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:49.902205
[2018-04-19 21:05:49,908] {jobs.py:351} DagFileProcessor138 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:05:50,978] {jobs.py:343} DagFileProcessor139 INFO - Started process (PID=3626) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:50,983] {jobs.py:534} DagFileProcessor139 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:50,985] {jobs.py:1521} DagFileProcessor139 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:50,985] {models.py:167} DagFileProcessor139 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:51,095] {jobs.py:1535} DagFileProcessor139 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:51,115] {models.py:322} DagFileProcessor139 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:51,116] {models.py:328} DagFileProcessor139 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:51.116139
[2018-04-19 21:05:51,121] {jobs.py:351} DagFileProcessor139 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:05:52,202] {jobs.py:343} DagFileProcessor140 INFO - Started process (PID=3628) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:52,207] {jobs.py:534} DagFileProcessor140 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:52,208] {jobs.py:1521} DagFileProcessor140 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:52,208] {models.py:167} DagFileProcessor140 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:52,310] {jobs.py:1535} DagFileProcessor140 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:52,331] {models.py:322} DagFileProcessor140 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:52,331] {models.py:328} DagFileProcessor140 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:52.331429
[2018-04-19 21:05:52,336] {jobs.py:351} DagFileProcessor140 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:05:53,423] {jobs.py:343} DagFileProcessor141 INFO - Started process (PID=3629) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:53,428] {jobs.py:534} DagFileProcessor141 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:53,429] {jobs.py:1521} DagFileProcessor141 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:53,430] {models.py:167} DagFileProcessor141 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:53,534] {jobs.py:1535} DagFileProcessor141 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:53,556] {models.py:322} DagFileProcessor141 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:53,557] {models.py:328} DagFileProcessor141 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:53.557322
[2018-04-19 21:05:53,562] {jobs.py:351} DagFileProcessor141 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:05:54,635] {jobs.py:343} DagFileProcessor142 INFO - Started process (PID=3630) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:54,640] {jobs.py:534} DagFileProcessor142 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:54,641] {jobs.py:1521} DagFileProcessor142 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:54,641] {models.py:167} DagFileProcessor142 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:54,744] {jobs.py:1535} DagFileProcessor142 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:54,764] {models.py:322} DagFileProcessor142 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:54,765] {models.py:328} DagFileProcessor142 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:54.765184
[2018-04-19 21:05:54,770] {jobs.py:351} DagFileProcessor142 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:05:55,856] {jobs.py:343} DagFileProcessor143 INFO - Started process (PID=3631) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:55,862] {jobs.py:534} DagFileProcessor143 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:55,863] {jobs.py:1521} DagFileProcessor143 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:55,863] {models.py:167} DagFileProcessor143 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:55,974] {jobs.py:1535} DagFileProcessor143 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:55,997] {models.py:322} DagFileProcessor143 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:55,998] {models.py:328} DagFileProcessor143 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:55.998010
[2018-04-19 21:05:56,002] {jobs.py:351} DagFileProcessor143 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:05:57,074] {jobs.py:343} DagFileProcessor144 INFO - Started process (PID=3632) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:57,079] {jobs.py:534} DagFileProcessor144 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:57,080] {jobs.py:1521} DagFileProcessor144 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:57,080] {models.py:167} DagFileProcessor144 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:57,182] {jobs.py:1535} DagFileProcessor144 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:57,203] {models.py:322} DagFileProcessor144 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:57,203] {models.py:328} DagFileProcessor144 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:57.203480
[2018-04-19 21:05:57,207] {jobs.py:351} DagFileProcessor144 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.133 seconds
[2018-04-19 21:05:58,305] {jobs.py:343} DagFileProcessor145 INFO - Started process (PID=3633) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:58,310] {jobs.py:534} DagFileProcessor145 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:58,312] {jobs.py:1521} DagFileProcessor145 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:58,312] {models.py:167} DagFileProcessor145 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:58,415] {jobs.py:1535} DagFileProcessor145 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:58,435] {models.py:322} DagFileProcessor145 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:58,436] {models.py:328} DagFileProcessor145 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:58.435969
[2018-04-19 21:05:58,440] {jobs.py:351} DagFileProcessor145 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:05:59,519] {jobs.py:343} DagFileProcessor146 INFO - Started process (PID=3634) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:59,524] {jobs.py:534} DagFileProcessor146 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:05:59,527] {jobs.py:1521} DagFileProcessor146 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:05:59,527] {models.py:167} DagFileProcessor146 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:59,633] {jobs.py:1535} DagFileProcessor146 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:05:59,655] {models.py:322} DagFileProcessor146 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:05:59,655] {models.py:328} DagFileProcessor146 INFO - Failing jobs without heartbeat after 2018-04-19 21:00:59.655776
[2018-04-19 21:05:59,660] {jobs.py:351} DagFileProcessor146 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.140 seconds
[2018-04-19 21:06:00,735] {jobs.py:343} DagFileProcessor147 INFO - Started process (PID=3635) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:00,740] {jobs.py:534} DagFileProcessor147 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:00,742] {jobs.py:1521} DagFileProcessor147 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:00,743] {models.py:167} DagFileProcessor147 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:00,845] {jobs.py:1535} DagFileProcessor147 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:00,866] {models.py:322} DagFileProcessor147 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:00,867] {models.py:328} DagFileProcessor147 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:00.866822
[2018-04-19 21:06:00,870] {jobs.py:351} DagFileProcessor147 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:06:01,956] {jobs.py:343} DagFileProcessor148 INFO - Started process (PID=3636) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:01,961] {jobs.py:534} DagFileProcessor148 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:01,963] {jobs.py:1521} DagFileProcessor148 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:01,964] {models.py:167} DagFileProcessor148 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:02,070] {jobs.py:1535} DagFileProcessor148 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:02,097] {models.py:322} DagFileProcessor148 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:02,097] {models.py:328} DagFileProcessor148 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:02.097779
[2018-04-19 21:06:02,101] {jobs.py:351} DagFileProcessor148 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:06:03,180] {jobs.py:343} DagFileProcessor149 INFO - Started process (PID=3638) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:03,185] {jobs.py:534} DagFileProcessor149 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:03,188] {jobs.py:1521} DagFileProcessor149 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:03,188] {models.py:167} DagFileProcessor149 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:03,295] {jobs.py:1535} DagFileProcessor149 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:03,323] {models.py:322} DagFileProcessor149 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:03,324] {models.py:328} DagFileProcessor149 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:03.323809
[2018-04-19 21:06:03,327] {jobs.py:351} DagFileProcessor149 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:06:04,406] {jobs.py:343} DagFileProcessor150 INFO - Started process (PID=3640) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:04,410] {jobs.py:534} DagFileProcessor150 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:04,412] {jobs.py:1521} DagFileProcessor150 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:04,412] {models.py:167} DagFileProcessor150 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:04,514] {jobs.py:1535} DagFileProcessor150 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:04,536] {models.py:322} DagFileProcessor150 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:04,536] {models.py:328} DagFileProcessor150 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:04.536576
[2018-04-19 21:06:04,540] {jobs.py:351} DagFileProcessor150 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:06:05,626] {jobs.py:343} DagFileProcessor151 INFO - Started process (PID=3641) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:05,631] {jobs.py:534} DagFileProcessor151 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:05,632] {jobs.py:1521} DagFileProcessor151 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:05,633] {models.py:167} DagFileProcessor151 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:05,739] {jobs.py:1535} DagFileProcessor151 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:05,761] {models.py:322} DagFileProcessor151 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:05,761] {models.py:328} DagFileProcessor151 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:05.761570
[2018-04-19 21:06:05,766] {jobs.py:351} DagFileProcessor151 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.140 seconds
[2018-04-19 21:06:06,846] {jobs.py:343} DagFileProcessor152 INFO - Started process (PID=3642) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:06,852] {jobs.py:534} DagFileProcessor152 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:06,853] {jobs.py:1521} DagFileProcessor152 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:06,853] {models.py:167} DagFileProcessor152 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:06,957] {jobs.py:1535} DagFileProcessor152 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:06,978] {models.py:322} DagFileProcessor152 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:06,979] {models.py:328} DagFileProcessor152 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:06.978955
[2018-04-19 21:06:06,983] {jobs.py:351} DagFileProcessor152 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.137 seconds
[2018-04-19 21:06:08,065] {jobs.py:343} DagFileProcessor153 INFO - Started process (PID=3643) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:08,070] {jobs.py:534} DagFileProcessor153 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:08,071] {jobs.py:1521} DagFileProcessor153 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:08,071] {models.py:167} DagFileProcessor153 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:08,174] {jobs.py:1535} DagFileProcessor153 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:08,195] {models.py:322} DagFileProcessor153 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:08,196] {models.py:328} DagFileProcessor153 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:08.195731
[2018-04-19 21:06:08,201] {jobs.py:351} DagFileProcessor153 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.136 seconds
[2018-04-19 21:06:09,280] {jobs.py:343} DagFileProcessor154 INFO - Started process (PID=3644) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:09,285] {jobs.py:534} DagFileProcessor154 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:09,286] {jobs.py:1521} DagFileProcessor154 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:09,287] {models.py:167} DagFileProcessor154 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:09,392] {jobs.py:1535} DagFileProcessor154 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:09,412] {models.py:322} DagFileProcessor154 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:09,413] {models.py:328} DagFileProcessor154 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:09.413078
[2018-04-19 21:06:09,417] {jobs.py:351} DagFileProcessor154 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.137 seconds
[2018-04-19 21:06:10,505] {jobs.py:343} DagFileProcessor155 INFO - Started process (PID=3645) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:10,510] {jobs.py:534} DagFileProcessor155 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:10,511] {jobs.py:1521} DagFileProcessor155 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:10,511] {models.py:167} DagFileProcessor155 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:10,614] {jobs.py:1535} DagFileProcessor155 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:10,635] {models.py:322} DagFileProcessor155 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:10,636] {models.py:328} DagFileProcessor155 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:10.636164
[2018-04-19 21:06:10,640] {jobs.py:351} DagFileProcessor155 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:06:11,722] {jobs.py:343} DagFileProcessor156 INFO - Started process (PID=3646) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:11,727] {jobs.py:534} DagFileProcessor156 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:11,728] {jobs.py:1521} DagFileProcessor156 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:11,729] {models.py:167} DagFileProcessor156 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:11,837] {jobs.py:1535} DagFileProcessor156 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:11,859] {models.py:322} DagFileProcessor156 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:11,860] {models.py:328} DagFileProcessor156 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:11.859859
[2018-04-19 21:06:11,863] {jobs.py:351} DagFileProcessor156 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:06:12,938] {jobs.py:343} DagFileProcessor157 INFO - Started process (PID=3655) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:12,943] {jobs.py:534} DagFileProcessor157 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:12,944] {jobs.py:1521} DagFileProcessor157 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:12,944] {models.py:167} DagFileProcessor157 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:13,048] {jobs.py:1535} DagFileProcessor157 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:13,068] {models.py:322} DagFileProcessor157 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:13,069] {models.py:328} DagFileProcessor157 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:13.069366
[2018-04-19 21:06:13,074] {jobs.py:351} DagFileProcessor157 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.136 seconds
[2018-04-19 21:06:14,151] {jobs.py:343} DagFileProcessor158 INFO - Started process (PID=3657) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:14,157] {jobs.py:534} DagFileProcessor158 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:14,158] {jobs.py:1521} DagFileProcessor158 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:14,158] {models.py:167} DagFileProcessor158 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:14,277] {jobs.py:1535} DagFileProcessor158 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:14,298] {models.py:322} DagFileProcessor158 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:14,299] {models.py:328} DagFileProcessor158 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:14.299129
[2018-04-19 21:06:14,305] {jobs.py:351} DagFileProcessor158 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:06:15,372] {jobs.py:343} DagFileProcessor159 INFO - Started process (PID=3659) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:15,377] {jobs.py:534} DagFileProcessor159 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:15,378] {jobs.py:1521} DagFileProcessor159 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:15,378] {models.py:167} DagFileProcessor159 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:15,483] {jobs.py:1535} DagFileProcessor159 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:15,507] {models.py:322} DagFileProcessor159 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:15,508] {models.py:328} DagFileProcessor159 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:15.508225
[2018-04-19 21:06:15,512] {jobs.py:351} DagFileProcessor159 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:06:16,602] {jobs.py:343} DagFileProcessor160 INFO - Started process (PID=3660) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:16,607] {jobs.py:534} DagFileProcessor160 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:16,608] {jobs.py:1521} DagFileProcessor160 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:16,609] {models.py:167} DagFileProcessor160 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:16,714] {jobs.py:1535} DagFileProcessor160 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:16,737] {models.py:322} DagFileProcessor160 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:16,738] {models.py:328} DagFileProcessor160 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:16.738269
[2018-04-19 21:06:16,742] {jobs.py:351} DagFileProcessor160 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:06:17,816] {jobs.py:343} DagFileProcessor161 INFO - Started process (PID=3661) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:17,821] {jobs.py:534} DagFileProcessor161 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:17,822] {jobs.py:1521} DagFileProcessor161 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:17,822] {models.py:167} DagFileProcessor161 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:17,926] {jobs.py:1535} DagFileProcessor161 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:17,947] {models.py:322} DagFileProcessor161 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:17,947] {models.py:328} DagFileProcessor161 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:17.947549
[2018-04-19 21:06:17,951] {jobs.py:351} DagFileProcessor161 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:06:19,034] {jobs.py:343} DagFileProcessor162 INFO - Started process (PID=3662) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:19,039] {jobs.py:534} DagFileProcessor162 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:19,040] {jobs.py:1521} DagFileProcessor162 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:19,041] {models.py:167} DagFileProcessor162 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:19,146] {jobs.py:1535} DagFileProcessor162 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:19,168] {models.py:322} DagFileProcessor162 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:19,168] {models.py:328} DagFileProcessor162 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:19.168733
[2018-04-19 21:06:19,173] {jobs.py:351} DagFileProcessor162 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:06:20,253] {jobs.py:343} DagFileProcessor163 INFO - Started process (PID=3663) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:20,258] {jobs.py:534} DagFileProcessor163 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:20,259] {jobs.py:1521} DagFileProcessor163 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:20,260] {models.py:167} DagFileProcessor163 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:20,365] {jobs.py:1535} DagFileProcessor163 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:20,386] {models.py:322} DagFileProcessor163 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:20,386] {models.py:328} DagFileProcessor163 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:20.386576
[2018-04-19 21:06:20,390] {jobs.py:351} DagFileProcessor163 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.137 seconds
[2018-04-19 21:06:21,471] {jobs.py:343} DagFileProcessor164 INFO - Started process (PID=3664) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:21,476] {jobs.py:534} DagFileProcessor164 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:21,477] {jobs.py:1521} DagFileProcessor164 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:21,478] {models.py:167} DagFileProcessor164 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:21,582] {jobs.py:1535} DagFileProcessor164 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:21,604] {models.py:322} DagFileProcessor164 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:21,604] {models.py:328} DagFileProcessor164 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:21.604763
[2018-04-19 21:06:21,609] {jobs.py:351} DagFileProcessor164 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.138 seconds
[2018-04-19 21:06:22,706] {jobs.py:343} DagFileProcessor165 INFO - Started process (PID=3666) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:22,711] {jobs.py:534} DagFileProcessor165 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:22,712] {jobs.py:1521} DagFileProcessor165 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:22,712] {models.py:167} DagFileProcessor165 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:22,821] {jobs.py:1535} DagFileProcessor165 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:22,845] {models.py:322} DagFileProcessor165 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:22,845] {models.py:328} DagFileProcessor165 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:22.845402
[2018-04-19 21:06:22,849] {jobs.py:351} DagFileProcessor165 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:06:23,917] {jobs.py:343} DagFileProcessor166 INFO - Started process (PID=3667) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:23,922] {jobs.py:534} DagFileProcessor166 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:23,923] {jobs.py:1521} DagFileProcessor166 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:23,923] {models.py:167} DagFileProcessor166 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:24,023] {jobs.py:1535} DagFileProcessor166 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:24,044] {models.py:322} DagFileProcessor166 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:24,045] {models.py:328} DagFileProcessor166 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:24.045345
[2018-04-19 21:06:24,049] {jobs.py:351} DagFileProcessor166 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.132 seconds
[2018-04-19 21:06:25,142] {jobs.py:343} DagFileProcessor167 INFO - Started process (PID=3668) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:25,147] {jobs.py:534} DagFileProcessor167 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:25,148] {jobs.py:1521} DagFileProcessor167 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:25,149] {models.py:167} DagFileProcessor167 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:25,251] {jobs.py:1535} DagFileProcessor167 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:25,273] {models.py:322} DagFileProcessor167 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:25,274] {models.py:328} DagFileProcessor167 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:25.273813
[2018-04-19 21:06:25,277] {jobs.py:351} DagFileProcessor167 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:06:26,365] {jobs.py:343} DagFileProcessor168 INFO - Started process (PID=3669) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:26,374] {jobs.py:534} DagFileProcessor168 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:26,375] {jobs.py:1521} DagFileProcessor168 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:26,376] {models.py:167} DagFileProcessor168 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:26,477] {jobs.py:1535} DagFileProcessor168 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:26,499] {models.py:322} DagFileProcessor168 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:26,500] {models.py:328} DagFileProcessor168 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:26.499897
[2018-04-19 21:06:26,504] {jobs.py:351} DagFileProcessor168 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:06:27,584] {jobs.py:343} DagFileProcessor169 INFO - Started process (PID=3670) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:27,589] {jobs.py:534} DagFileProcessor169 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:27,590] {jobs.py:1521} DagFileProcessor169 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:27,590] {models.py:167} DagFileProcessor169 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:27,691] {jobs.py:1535} DagFileProcessor169 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:27,714] {models.py:322} DagFileProcessor169 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:27,714] {models.py:328} DagFileProcessor169 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:27.714662
[2018-04-19 21:06:27,719] {jobs.py:351} DagFileProcessor169 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:06:28,816] {jobs.py:343} DagFileProcessor170 INFO - Started process (PID=3671) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:28,821] {jobs.py:534} DagFileProcessor170 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:28,822] {jobs.py:1521} DagFileProcessor170 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:28,823] {models.py:167} DagFileProcessor170 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:28,928] {jobs.py:1535} DagFileProcessor170 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:28,951] {models.py:322} DagFileProcessor170 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:28,951] {models.py:328} DagFileProcessor170 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:28.951497
[2018-04-19 21:06:28,956] {jobs.py:351} DagFileProcessor170 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.140 seconds
[2018-04-19 21:06:30,029] {jobs.py:343} DagFileProcessor171 INFO - Started process (PID=3672) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:30,034] {jobs.py:534} DagFileProcessor171 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:30,035] {jobs.py:1521} DagFileProcessor171 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:30,036] {models.py:167} DagFileProcessor171 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:30,135] {jobs.py:1535} DagFileProcessor171 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:30,157] {models.py:322} DagFileProcessor171 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:30,158] {models.py:328} DagFileProcessor171 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:30.158193
[2018-04-19 21:06:30,162] {jobs.py:351} DagFileProcessor171 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.133 seconds
[2018-04-19 21:06:31,245] {jobs.py:343} DagFileProcessor172 INFO - Started process (PID=3673) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:31,250] {jobs.py:534} DagFileProcessor172 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:31,251] {jobs.py:1521} DagFileProcessor172 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:31,252] {models.py:167} DagFileProcessor172 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:31,360] {jobs.py:1535} DagFileProcessor172 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:31,383] {models.py:322} DagFileProcessor172 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:31,384] {models.py:328} DagFileProcessor172 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:31.384209
[2018-04-19 21:06:31,389] {jobs.py:351} DagFileProcessor172 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:06:32,464] {jobs.py:343} DagFileProcessor173 INFO - Started process (PID=3675) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:32,469] {jobs.py:534} DagFileProcessor173 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:32,470] {jobs.py:1521} DagFileProcessor173 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:32,470] {models.py:167} DagFileProcessor173 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:32,579] {jobs.py:1535} DagFileProcessor173 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:32,607] {models.py:322} DagFileProcessor173 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:32,608] {models.py:328} DagFileProcessor173 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:32.608105
[2018-04-19 21:06:32,612] {jobs.py:351} DagFileProcessor173 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:06:33,685] {jobs.py:343} DagFileProcessor174 INFO - Started process (PID=3676) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:33,690] {jobs.py:534} DagFileProcessor174 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:33,691] {jobs.py:1521} DagFileProcessor174 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:33,691] {models.py:167} DagFileProcessor174 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:33,792] {jobs.py:1535} DagFileProcessor174 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:33,815] {models.py:322} DagFileProcessor174 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:33,816] {models.py:328} DagFileProcessor174 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:33.816190
[2018-04-19 21:06:33,820] {jobs.py:351} DagFileProcessor174 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:06:34,919] {jobs.py:343} DagFileProcessor175 INFO - Started process (PID=3677) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:34,924] {jobs.py:534} DagFileProcessor175 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:34,926] {jobs.py:1521} DagFileProcessor175 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:34,926] {models.py:167} DagFileProcessor175 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:35,027] {jobs.py:1535} DagFileProcessor175 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:35,049] {models.py:322} DagFileProcessor175 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:35,049] {models.py:328} DagFileProcessor175 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:35.049540
[2018-04-19 21:06:35,053] {jobs.py:351} DagFileProcessor175 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:06:36,138] {jobs.py:343} DagFileProcessor176 INFO - Started process (PID=3678) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:36,143] {jobs.py:534} DagFileProcessor176 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:36,145] {jobs.py:1521} DagFileProcessor176 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:36,146] {models.py:167} DagFileProcessor176 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:36,246] {jobs.py:1535} DagFileProcessor176 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:36,267] {models.py:322} DagFileProcessor176 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:36,268] {models.py:328} DagFileProcessor176 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:36.267962
[2018-04-19 21:06:36,272] {jobs.py:351} DagFileProcessor176 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:06:37,356] {jobs.py:343} DagFileProcessor177 INFO - Started process (PID=3679) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:37,361] {jobs.py:534} DagFileProcessor177 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:37,363] {jobs.py:1521} DagFileProcessor177 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:37,363] {models.py:167} DagFileProcessor177 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:37,467] {jobs.py:1535} DagFileProcessor177 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:37,489] {models.py:322} DagFileProcessor177 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:37,490] {models.py:328} DagFileProcessor177 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:37.489794
[2018-04-19 21:06:37,494] {jobs.py:351} DagFileProcessor177 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.138 seconds
[2018-04-19 21:06:38,573] {jobs.py:343} DagFileProcessor178 INFO - Started process (PID=3680) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:38,578] {jobs.py:534} DagFileProcessor178 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:38,580] {jobs.py:1521} DagFileProcessor178 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:38,580] {models.py:167} DagFileProcessor178 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:38,681] {jobs.py:1535} DagFileProcessor178 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:38,705] {models.py:322} DagFileProcessor178 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:38,706] {models.py:328} DagFileProcessor178 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:38.705928
[2018-04-19 21:06:38,712] {jobs.py:351} DagFileProcessor178 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:06:39,787] {jobs.py:343} DagFileProcessor179 INFO - Started process (PID=3681) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:39,792] {jobs.py:534} DagFileProcessor179 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:39,794] {jobs.py:1521} DagFileProcessor179 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:39,794] {models.py:167} DagFileProcessor179 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:39,895] {jobs.py:1535} DagFileProcessor179 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:39,918] {models.py:322} DagFileProcessor179 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:39,919] {models.py:328} DagFileProcessor179 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:39.918870
[2018-04-19 21:06:39,923] {jobs.py:351} DagFileProcessor179 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.136 seconds
[2018-04-19 21:06:41,008] {jobs.py:343} DagFileProcessor180 INFO - Started process (PID=3682) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:41,013] {jobs.py:534} DagFileProcessor180 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:41,014] {jobs.py:1521} DagFileProcessor180 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:41,015] {models.py:167} DagFileProcessor180 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:41,126] {jobs.py:1535} DagFileProcessor180 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:41,147] {models.py:322} DagFileProcessor180 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:41,148] {models.py:328} DagFileProcessor180 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:41.148058
[2018-04-19 21:06:41,152] {jobs.py:351} DagFileProcessor180 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:06:42,222] {jobs.py:343} DagFileProcessor181 INFO - Started process (PID=3683) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:42,227] {jobs.py:534} DagFileProcessor181 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:42,228] {jobs.py:1521} DagFileProcessor181 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:42,228] {models.py:167} DagFileProcessor181 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:42,333] {jobs.py:1535} DagFileProcessor181 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:42,357] {models.py:322} DagFileProcessor181 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:42,357] {models.py:328} DagFileProcessor181 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:42.357446
[2018-04-19 21:06:42,361] {jobs.py:351} DagFileProcessor181 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.140 seconds
[2018-04-19 21:06:43,444] {jobs.py:343} DagFileProcessor182 INFO - Started process (PID=3685) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:43,449] {jobs.py:534} DagFileProcessor182 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:43,450] {jobs.py:1521} DagFileProcessor182 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:43,450] {models.py:167} DagFileProcessor182 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:43,549] {jobs.py:1535} DagFileProcessor182 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:43,570] {models.py:322} DagFileProcessor182 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:43,570] {models.py:328} DagFileProcessor182 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:43.570665
[2018-04-19 21:06:43,575] {jobs.py:351} DagFileProcessor182 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.132 seconds
[2018-04-19 21:06:44,662] {jobs.py:343} DagFileProcessor183 INFO - Started process (PID=3686) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:44,667] {jobs.py:534} DagFileProcessor183 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:44,669] {jobs.py:1521} DagFileProcessor183 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:44,669] {models.py:167} DagFileProcessor183 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:44,766] {jobs.py:1535} DagFileProcessor183 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:44,788] {models.py:322} DagFileProcessor183 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:44,789] {models.py:328} DagFileProcessor183 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:44.788841
[2018-04-19 21:06:44,792] {jobs.py:351} DagFileProcessor183 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.130 seconds
[2018-04-19 21:06:45,881] {jobs.py:343} DagFileProcessor184 INFO - Started process (PID=3687) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:45,886] {jobs.py:534} DagFileProcessor184 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:45,888] {jobs.py:1521} DagFileProcessor184 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:45,888] {models.py:167} DagFileProcessor184 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:45,985] {jobs.py:1535} DagFileProcessor184 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:46,009] {models.py:322} DagFileProcessor184 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:46,010] {models.py:328} DagFileProcessor184 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:46.009822
[2018-04-19 21:06:46,013] {jobs.py:351} DagFileProcessor184 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.132 seconds
[2018-04-19 21:06:47,105] {jobs.py:343} DagFileProcessor185 INFO - Started process (PID=3688) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:47,110] {jobs.py:534} DagFileProcessor185 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:47,111] {jobs.py:1521} DagFileProcessor185 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:47,112] {models.py:167} DagFileProcessor185 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:47,210] {jobs.py:1535} DagFileProcessor185 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:47,232] {models.py:322} DagFileProcessor185 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:47,233] {models.py:328} DagFileProcessor185 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:47.232969
[2018-04-19 21:06:47,237] {jobs.py:351} DagFileProcessor185 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.132 seconds
[2018-04-19 21:06:48,319] {jobs.py:343} DagFileProcessor186 INFO - Started process (PID=3689) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:48,324] {jobs.py:534} DagFileProcessor186 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:48,325] {jobs.py:1521} DagFileProcessor186 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:48,325] {models.py:167} DagFileProcessor186 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:48,425] {jobs.py:1535} DagFileProcessor186 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:48,447] {models.py:322} DagFileProcessor186 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:48,447] {models.py:328} DagFileProcessor186 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:48.447473
[2018-04-19 21:06:48,451] {jobs.py:351} DagFileProcessor186 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.132 seconds
[2018-04-19 21:06:49,538] {jobs.py:343} DagFileProcessor187 INFO - Started process (PID=3690) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:49,543] {jobs.py:534} DagFileProcessor187 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:49,544] {jobs.py:1521} DagFileProcessor187 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:49,545] {models.py:167} DagFileProcessor187 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:49,645] {jobs.py:1535} DagFileProcessor187 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:49,665] {models.py:322} DagFileProcessor187 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:49,666] {models.py:328} DagFileProcessor187 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:49.665946
[2018-04-19 21:06:49,671] {jobs.py:351} DagFileProcessor187 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.133 seconds
[2018-04-19 21:06:50,745] {jobs.py:343} DagFileProcessor188 INFO - Started process (PID=3691) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:50,750] {jobs.py:534} DagFileProcessor188 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:50,751] {jobs.py:1521} DagFileProcessor188 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:50,751] {models.py:167} DagFileProcessor188 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:50,863] {jobs.py:1535} DagFileProcessor188 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:50,885] {models.py:322} DagFileProcessor188 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:50,885] {models.py:328} DagFileProcessor188 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:50.885527
[2018-04-19 21:06:50,890] {jobs.py:351} DagFileProcessor188 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:06:51,963] {jobs.py:343} DagFileProcessor189 INFO - Started process (PID=3692) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:51,968] {jobs.py:534} DagFileProcessor189 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:51,969] {jobs.py:1521} DagFileProcessor189 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:51,969] {models.py:167} DagFileProcessor189 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:52,076] {jobs.py:1535} DagFileProcessor189 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:52,100] {models.py:322} DagFileProcessor189 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:52,100] {models.py:328} DagFileProcessor189 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:52.100391
[2018-04-19 21:06:52,104] {jobs.py:351} DagFileProcessor189 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.142 seconds
[2018-04-19 21:06:53,189] {jobs.py:343} DagFileProcessor190 INFO - Started process (PID=3694) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:53,194] {jobs.py:534} DagFileProcessor190 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:53,195] {jobs.py:1521} DagFileProcessor190 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:53,196] {models.py:167} DagFileProcessor190 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:53,300] {jobs.py:1535} DagFileProcessor190 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:53,323] {models.py:322} DagFileProcessor190 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:53,324] {models.py:328} DagFileProcessor190 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:53.323802
[2018-04-19 21:06:53,328] {jobs.py:351} DagFileProcessor190 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:06:54,404] {jobs.py:343} DagFileProcessor191 INFO - Started process (PID=3695) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:54,409] {jobs.py:534} DagFileProcessor191 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:54,410] {jobs.py:1521} DagFileProcessor191 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:54,411] {models.py:167} DagFileProcessor191 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:54,519] {jobs.py:1535} DagFileProcessor191 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:54,542] {models.py:322} DagFileProcessor191 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:54,543] {models.py:328} DagFileProcessor191 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:54.543424
[2018-04-19 21:06:54,547] {jobs.py:351} DagFileProcessor191 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:06:55,623] {jobs.py:343} DagFileProcessor192 INFO - Started process (PID=3696) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:55,628] {jobs.py:534} DagFileProcessor192 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:55,629] {jobs.py:1521} DagFileProcessor192 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:55,630] {models.py:167} DagFileProcessor192 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:55,735] {jobs.py:1535} DagFileProcessor192 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:55,757] {models.py:322} DagFileProcessor192 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:55,757] {models.py:328} DagFileProcessor192 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:55.757718
[2018-04-19 21:06:55,762] {jobs.py:351} DagFileProcessor192 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:06:56,841] {jobs.py:343} DagFileProcessor193 INFO - Started process (PID=3697) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:56,855] {jobs.py:534} DagFileProcessor193 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:56,857] {jobs.py:1521} DagFileProcessor193 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:56,857] {models.py:167} DagFileProcessor193 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:56,964] {jobs.py:1535} DagFileProcessor193 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:56,986] {models.py:322} DagFileProcessor193 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:56,987] {models.py:328} DagFileProcessor193 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:56.987303
[2018-04-19 21:06:56,991] {jobs.py:351} DagFileProcessor193 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:06:58,054] {jobs.py:343} DagFileProcessor194 INFO - Started process (PID=3698) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:58,059] {jobs.py:534} DagFileProcessor194 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:58,061] {jobs.py:1521} DagFileProcessor194 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:58,061] {models.py:167} DagFileProcessor194 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:58,166] {jobs.py:1535} DagFileProcessor194 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:58,188] {models.py:322} DagFileProcessor194 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:58,188] {models.py:328} DagFileProcessor194 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:58.188402
[2018-04-19 21:06:58,192] {jobs.py:351} DagFileProcessor194 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.138 seconds
[2018-04-19 21:06:59,283] {jobs.py:343} DagFileProcessor195 INFO - Started process (PID=3699) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:59,288] {jobs.py:534} DagFileProcessor195 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:06:59,290] {jobs.py:1521} DagFileProcessor195 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:06:59,290] {models.py:167} DagFileProcessor195 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:59,396] {jobs.py:1535} DagFileProcessor195 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:06:59,418] {models.py:322} DagFileProcessor195 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:06:59,418] {models.py:328} DagFileProcessor195 INFO - Failing jobs without heartbeat after 2018-04-19 21:01:59.418582
[2018-04-19 21:06:59,422] {jobs.py:351} DagFileProcessor195 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:07:00,499] {jobs.py:343} DagFileProcessor196 INFO - Started process (PID=3700) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:00,504] {jobs.py:534} DagFileProcessor196 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:00,505] {jobs.py:1521} DagFileProcessor196 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:00,505] {models.py:167} DagFileProcessor196 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:00,624] {jobs.py:1535} DagFileProcessor196 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:00,646] {models.py:322} DagFileProcessor196 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:00,646] {models.py:328} DagFileProcessor196 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:00.646731
[2018-04-19 21:07:00,651] {jobs.py:351} DagFileProcessor196 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:07:01,716] {jobs.py:343} DagFileProcessor197 INFO - Started process (PID=3701) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:01,721] {jobs.py:534} DagFileProcessor197 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:01,722] {jobs.py:1521} DagFileProcessor197 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:01,722] {models.py:167} DagFileProcessor197 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:01,830] {jobs.py:1535} DagFileProcessor197 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:01,851] {models.py:322} DagFileProcessor197 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:01,851] {models.py:328} DagFileProcessor197 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:01.851736
[2018-04-19 21:07:01,855] {jobs.py:351} DagFileProcessor197 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.140 seconds
[2018-04-19 21:07:02,935] {jobs.py:343} DagFileProcessor198 INFO - Started process (PID=3703) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:02,940] {jobs.py:534} DagFileProcessor198 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:02,941] {jobs.py:1521} DagFileProcessor198 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:02,941] {models.py:167} DagFileProcessor198 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:03,047] {jobs.py:1535} DagFileProcessor198 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:03,069] {models.py:322} DagFileProcessor198 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:03,069] {models.py:328} DagFileProcessor198 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:03.069677
[2018-04-19 21:07:03,074] {jobs.py:351} DagFileProcessor198 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:07:04,151] {jobs.py:343} DagFileProcessor199 INFO - Started process (PID=3704) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:04,156] {jobs.py:534} DagFileProcessor199 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:04,157] {jobs.py:1521} DagFileProcessor199 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:04,158] {models.py:167} DagFileProcessor199 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:04,261] {jobs.py:1535} DagFileProcessor199 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:04,284] {models.py:322} DagFileProcessor199 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:04,284] {models.py:328} DagFileProcessor199 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:04.284626
[2018-04-19 21:07:04,288] {jobs.py:351} DagFileProcessor199 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.137 seconds
[2018-04-19 21:07:05,381] {jobs.py:343} DagFileProcessor200 INFO - Started process (PID=3705) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:05,387] {jobs.py:534} DagFileProcessor200 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:05,388] {jobs.py:1521} DagFileProcessor200 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:05,388] {models.py:167} DagFileProcessor200 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:05,503] {jobs.py:1535} DagFileProcessor200 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:05,526] {models.py:322} DagFileProcessor200 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:05,527] {models.py:328} DagFileProcessor200 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:05.527305
[2018-04-19 21:07:05,532] {jobs.py:351} DagFileProcessor200 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:07:06,597] {jobs.py:343} DagFileProcessor201 INFO - Started process (PID=3706) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:06,602] {jobs.py:534} DagFileProcessor201 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:06,603] {jobs.py:1521} DagFileProcessor201 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:06,603] {models.py:167} DagFileProcessor201 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:06,711] {jobs.py:1535} DagFileProcessor201 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:06,736] {models.py:322} DagFileProcessor201 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:06,737] {models.py:328} DagFileProcessor201 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:06.736986
[2018-04-19 21:07:06,741] {jobs.py:351} DagFileProcessor201 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:07:07,814] {jobs.py:343} DagFileProcessor202 INFO - Started process (PID=3707) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:07,819] {jobs.py:534} DagFileProcessor202 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:07,821] {jobs.py:1521} DagFileProcessor202 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:07,821] {models.py:167} DagFileProcessor202 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:07,924] {jobs.py:1535} DagFileProcessor202 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:07,944] {models.py:322} DagFileProcessor202 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:07,945] {models.py:328} DagFileProcessor202 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:07.945219
[2018-04-19 21:07:07,949] {jobs.py:351} DagFileProcessor202 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:07:09,031] {jobs.py:343} DagFileProcessor203 INFO - Started process (PID=3708) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:09,036] {jobs.py:534} DagFileProcessor203 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:09,037] {jobs.py:1521} DagFileProcessor203 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:09,037] {models.py:167} DagFileProcessor203 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:09,141] {jobs.py:1535} DagFileProcessor203 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:09,162] {models.py:322} DagFileProcessor203 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:09,162] {models.py:328} DagFileProcessor203 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:09.162535
[2018-04-19 21:07:09,166] {jobs.py:351} DagFileProcessor203 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:07:10,250] {jobs.py:343} DagFileProcessor204 INFO - Started process (PID=3709) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:10,255] {jobs.py:534} DagFileProcessor204 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:10,256] {jobs.py:1521} DagFileProcessor204 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:10,257] {models.py:167} DagFileProcessor204 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:10,365] {jobs.py:1535} DagFileProcessor204 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:10,387] {models.py:322} DagFileProcessor204 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:10,388] {models.py:328} DagFileProcessor204 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:10.388018
[2018-04-19 21:07:10,392] {jobs.py:351} DagFileProcessor204 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:07:11,481] {jobs.py:343} DagFileProcessor205 INFO - Started process (PID=3710) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:11,486] {jobs.py:534} DagFileProcessor205 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:11,487] {jobs.py:1521} DagFileProcessor205 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:11,488] {models.py:167} DagFileProcessor205 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:11,590] {jobs.py:1535} DagFileProcessor205 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:11,610] {models.py:322} DagFileProcessor205 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:11,611] {models.py:328} DagFileProcessor205 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:11.610904
[2018-04-19 21:07:11,615] {jobs.py:351} DagFileProcessor205 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.133 seconds
[2018-04-19 21:07:12,699] {jobs.py:343} DagFileProcessor206 INFO - Started process (PID=3719) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:12,704] {jobs.py:534} DagFileProcessor206 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:12,705] {jobs.py:1521} DagFileProcessor206 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:12,706] {models.py:167} DagFileProcessor206 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:12,809] {jobs.py:1535} DagFileProcessor206 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:12,828] {models.py:322} DagFileProcessor206 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:12,829] {models.py:328} DagFileProcessor206 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:12.829322
[2018-04-19 21:07:12,834] {jobs.py:351} DagFileProcessor206 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:07:13,912] {jobs.py:343} DagFileProcessor207 INFO - Started process (PID=3720) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:13,917] {jobs.py:534} DagFileProcessor207 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:13,919] {jobs.py:1521} DagFileProcessor207 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:13,919] {models.py:167} DagFileProcessor207 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:14,027] {jobs.py:1535} DagFileProcessor207 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:14,050] {models.py:322} DagFileProcessor207 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:14,051] {models.py:328} DagFileProcessor207 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:14.051128
[2018-04-19 21:07:14,055] {jobs.py:351} DagFileProcessor207 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:07:15,131] {jobs.py:343} DagFileProcessor208 INFO - Started process (PID=3721) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:15,136] {jobs.py:534} DagFileProcessor208 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:15,137] {jobs.py:1521} DagFileProcessor208 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:15,138] {models.py:167} DagFileProcessor208 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:15,244] {jobs.py:1535} DagFileProcessor208 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:15,267] {models.py:322} DagFileProcessor208 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:15,268] {models.py:328} DagFileProcessor208 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:15.268380
[2018-04-19 21:07:15,274] {jobs.py:351} DagFileProcessor208 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:07:16,347] {jobs.py:343} DagFileProcessor209 INFO - Started process (PID=3722) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:16,353] {jobs.py:534} DagFileProcessor209 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:16,354] {jobs.py:1521} DagFileProcessor209 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:16,354] {models.py:167} DagFileProcessor209 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:16,458] {jobs.py:1535} DagFileProcessor209 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:16,479] {models.py:322} DagFileProcessor209 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:16,479] {models.py:328} DagFileProcessor209 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:16.479494
[2018-04-19 21:07:16,483] {jobs.py:351} DagFileProcessor209 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.136 seconds
[2018-04-19 21:07:17,580] {jobs.py:343} DagFileProcessor210 INFO - Started process (PID=3723) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:17,585] {jobs.py:534} DagFileProcessor210 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:17,587] {jobs.py:1521} DagFileProcessor210 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:17,588] {models.py:167} DagFileProcessor210 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:17,698] {jobs.py:1535} DagFileProcessor210 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:17,720] {models.py:322} DagFileProcessor210 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:17,720] {models.py:328} DagFileProcessor210 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:17.720780
[2018-04-19 21:07:17,725] {jobs.py:351} DagFileProcessor210 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:07:18,797] {jobs.py:343} DagFileProcessor211 INFO - Started process (PID=3724) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:18,802] {jobs.py:534} DagFileProcessor211 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:18,804] {jobs.py:1521} DagFileProcessor211 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:18,805] {models.py:167} DagFileProcessor211 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:18,913] {jobs.py:1535} DagFileProcessor211 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:18,937] {models.py:322} DagFileProcessor211 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:18,937] {models.py:328} DagFileProcessor211 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:18.937574
[2018-04-19 21:07:18,942] {jobs.py:351} DagFileProcessor211 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:07:20,012] {jobs.py:343} DagFileProcessor212 INFO - Started process (PID=3725) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:20,017] {jobs.py:534} DagFileProcessor212 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:20,019] {jobs.py:1521} DagFileProcessor212 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:20,019] {models.py:167} DagFileProcessor212 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:20,124] {jobs.py:1535} DagFileProcessor212 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:20,145] {models.py:322} DagFileProcessor212 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:20,146] {models.py:328} DagFileProcessor212 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:20.145907
[2018-04-19 21:07:20,149] {jobs.py:351} DagFileProcessor212 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.138 seconds
[2018-04-19 21:07:21,234] {jobs.py:343} DagFileProcessor213 INFO - Started process (PID=3726) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:21,239] {jobs.py:534} DagFileProcessor213 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:21,241] {jobs.py:1521} DagFileProcessor213 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:21,241] {models.py:167} DagFileProcessor213 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:21,349] {jobs.py:1535} DagFileProcessor213 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:21,371] {models.py:322} DagFileProcessor213 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:21,371] {models.py:328} DagFileProcessor213 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:21.371734
[2018-04-19 21:07:21,375] {jobs.py:351} DagFileProcessor213 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:07:22,451] {jobs.py:343} DagFileProcessor214 INFO - Started process (PID=3728) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:22,456] {jobs.py:534} DagFileProcessor214 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:22,458] {jobs.py:1521} DagFileProcessor214 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:22,458] {models.py:167} DagFileProcessor214 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:22,559] {jobs.py:1535} DagFileProcessor214 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:22,581] {models.py:322} DagFileProcessor214 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:22,582] {models.py:328} DagFileProcessor214 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:22.582138
[2018-04-19 21:07:22,586] {jobs.py:351} DagFileProcessor214 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:07:23,681] {jobs.py:343} DagFileProcessor215 INFO - Started process (PID=3729) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:23,686] {jobs.py:534} DagFileProcessor215 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:23,687] {jobs.py:1521} DagFileProcessor215 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:23,687] {models.py:167} DagFileProcessor215 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:23,796] {jobs.py:1535} DagFileProcessor215 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:23,819] {models.py:322} DagFileProcessor215 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:23,819] {models.py:328} DagFileProcessor215 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:23.819404
[2018-04-19 21:07:23,823] {jobs.py:351} DagFileProcessor215 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.142 seconds
[2018-04-19 21:07:24,899] {jobs.py:343} DagFileProcessor216 INFO - Started process (PID=3730) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:24,906] {jobs.py:534} DagFileProcessor216 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:24,909] {jobs.py:1521} DagFileProcessor216 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:24,910] {models.py:167} DagFileProcessor216 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:25,020] {jobs.py:1535} DagFileProcessor216 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:25,043] {models.py:322} DagFileProcessor216 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:25,043] {models.py:328} DagFileProcessor216 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:25.043500
[2018-04-19 21:07:25,047] {jobs.py:351} DagFileProcessor216 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:07:26,109] {jobs.py:343} DagFileProcessor217 INFO - Started process (PID=3731) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:26,118] {jobs.py:534} DagFileProcessor217 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:26,120] {jobs.py:1521} DagFileProcessor217 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:26,121] {models.py:167} DagFileProcessor217 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:26,241] {jobs.py:1535} DagFileProcessor217 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:26,266] {models.py:322} DagFileProcessor217 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:26,267] {models.py:328} DagFileProcessor217 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:26.266937
[2018-04-19 21:07:26,271] {jobs.py:351} DagFileProcessor217 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.163 seconds
[2018-04-19 21:07:27,328] {jobs.py:343} DagFileProcessor218 INFO - Started process (PID=3732) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:27,333] {jobs.py:534} DagFileProcessor218 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:27,334] {jobs.py:1521} DagFileProcessor218 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:27,334] {models.py:167} DagFileProcessor218 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:27,440] {jobs.py:1535} DagFileProcessor218 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:27,464] {models.py:322} DagFileProcessor218 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:27,465] {models.py:328} DagFileProcessor218 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:27.465002
[2018-04-19 21:07:27,469] {jobs.py:351} DagFileProcessor218 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:07:28,550] {jobs.py:343} DagFileProcessor219 INFO - Started process (PID=3733) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:28,558] {jobs.py:534} DagFileProcessor219 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:28,560] {jobs.py:1521} DagFileProcessor219 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:28,560] {models.py:167} DagFileProcessor219 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:28,669] {jobs.py:1535} DagFileProcessor219 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:28,689] {models.py:322} DagFileProcessor219 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:28,690] {models.py:328} DagFileProcessor219 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:28.690318
[2018-04-19 21:07:28,694] {jobs.py:351} DagFileProcessor219 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:07:29,779] {jobs.py:343} DagFileProcessor220 INFO - Started process (PID=3734) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:29,785] {jobs.py:534} DagFileProcessor220 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:29,787] {jobs.py:1521} DagFileProcessor220 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:29,787] {models.py:167} DagFileProcessor220 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:29,903] {jobs.py:1535} DagFileProcessor220 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:29,929] {models.py:322} DagFileProcessor220 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:29,930] {models.py:328} DagFileProcessor220 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:29.930098
[2018-04-19 21:07:29,934] {jobs.py:351} DagFileProcessor220 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.155 seconds
[2018-04-19 21:07:30,990] {jobs.py:343} DagFileProcessor221 INFO - Started process (PID=3735) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:30,995] {jobs.py:534} DagFileProcessor221 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:30,996] {jobs.py:1521} DagFileProcessor221 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:30,996] {models.py:167} DagFileProcessor221 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:31,126] {jobs.py:1535} DagFileProcessor221 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:31,154] {models.py:322} DagFileProcessor221 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:31,154] {models.py:328} DagFileProcessor221 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:31.154479
[2018-04-19 21:07:31,158] {jobs.py:351} DagFileProcessor221 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.169 seconds
[2018-04-19 21:07:32,207] {jobs.py:343} DagFileProcessor222 INFO - Started process (PID=3736) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:32,212] {jobs.py:534} DagFileProcessor222 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:32,214] {jobs.py:1521} DagFileProcessor222 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:32,214] {models.py:167} DagFileProcessor222 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:32,325] {jobs.py:1535} DagFileProcessor222 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:32,347] {models.py:322} DagFileProcessor222 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:32,347] {models.py:328} DagFileProcessor222 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:32.347374
[2018-04-19 21:07:32,351] {jobs.py:351} DagFileProcessor222 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:07:33,420] {jobs.py:343} DagFileProcessor223 INFO - Started process (PID=3738) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:33,425] {jobs.py:534} DagFileProcessor223 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:33,426] {jobs.py:1521} DagFileProcessor223 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:33,426] {models.py:167} DagFileProcessor223 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:33,531] {jobs.py:1535} DagFileProcessor223 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:33,554] {models.py:322} DagFileProcessor223 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:33,554] {models.py:328} DagFileProcessor223 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:33.554553
[2018-04-19 21:07:33,558] {jobs.py:351} DagFileProcessor223 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:07:34,638] {jobs.py:343} DagFileProcessor224 INFO - Started process (PID=3739) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:34,642] {jobs.py:534} DagFileProcessor224 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:34,644] {jobs.py:1521} DagFileProcessor224 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:34,644] {models.py:167} DagFileProcessor224 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:34,746] {jobs.py:1535} DagFileProcessor224 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:34,767] {models.py:322} DagFileProcessor224 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:34,767] {models.py:328} DagFileProcessor224 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:34.767387
[2018-04-19 21:07:34,771] {jobs.py:351} DagFileProcessor224 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:07:35,868] {jobs.py:343} DagFileProcessor225 INFO - Started process (PID=3740) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:35,873] {jobs.py:534} DagFileProcessor225 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:35,874] {jobs.py:1521} DagFileProcessor225 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:35,875] {models.py:167} DagFileProcessor225 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:35,983] {jobs.py:1535} DagFileProcessor225 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:36,005] {models.py:322} DagFileProcessor225 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:36,005] {models.py:328} DagFileProcessor225 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:36.005552
[2018-04-19 21:07:36,009] {jobs.py:351} DagFileProcessor225 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:07:37,091] {jobs.py:343} DagFileProcessor226 INFO - Started process (PID=3741) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:37,097] {jobs.py:534} DagFileProcessor226 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:37,098] {jobs.py:1521} DagFileProcessor226 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:37,098] {models.py:167} DagFileProcessor226 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:37,213] {jobs.py:1535} DagFileProcessor226 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:37,234] {models.py:322} DagFileProcessor226 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:37,234] {models.py:328} DagFileProcessor226 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:37.234662
[2018-04-19 21:07:37,238] {jobs.py:351} DagFileProcessor226 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:07:38,311] {jobs.py:343} DagFileProcessor227 INFO - Started process (PID=3742) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:38,316] {jobs.py:534} DagFileProcessor227 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:38,317] {jobs.py:1521} DagFileProcessor227 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:38,317] {models.py:167} DagFileProcessor227 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:38,432] {jobs.py:1535} DagFileProcessor227 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:38,454] {models.py:322} DagFileProcessor227 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:38,455] {models.py:328} DagFileProcessor227 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:38.454958
[2018-04-19 21:07:38,460] {jobs.py:351} DagFileProcessor227 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:07:39,531] {jobs.py:343} DagFileProcessor228 INFO - Started process (PID=3743) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:39,536] {jobs.py:534} DagFileProcessor228 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:39,537] {jobs.py:1521} DagFileProcessor228 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:39,538] {models.py:167} DagFileProcessor228 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:39,646] {jobs.py:1535} DagFileProcessor228 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:39,668] {models.py:322} DagFileProcessor228 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:39,669] {models.py:328} DagFileProcessor228 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:39.668991
[2018-04-19 21:07:39,673] {jobs.py:351} DagFileProcessor228 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:07:40,754] {jobs.py:343} DagFileProcessor229 INFO - Started process (PID=3744) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:40,759] {jobs.py:534} DagFileProcessor229 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:40,760] {jobs.py:1521} DagFileProcessor229 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:40,761] {models.py:167} DagFileProcessor229 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:40,868] {jobs.py:1535} DagFileProcessor229 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:40,894] {models.py:322} DagFileProcessor229 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:40,895] {models.py:328} DagFileProcessor229 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:40.894819
[2018-04-19 21:07:40,899] {jobs.py:351} DagFileProcessor229 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:07:41,980] {jobs.py:343} DagFileProcessor230 INFO - Started process (PID=3745) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:41,985] {jobs.py:534} DagFileProcessor230 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:41,986] {jobs.py:1521} DagFileProcessor230 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:41,986] {models.py:167} DagFileProcessor230 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:42,092] {jobs.py:1535} DagFileProcessor230 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:42,118] {models.py:322} DagFileProcessor230 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:42,119] {models.py:328} DagFileProcessor230 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:42.119032
[2018-04-19 21:07:42,123] {jobs.py:351} DagFileProcessor230 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:07:43,192] {jobs.py:343} DagFileProcessor231 INFO - Started process (PID=3747) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:43,197] {jobs.py:534} DagFileProcessor231 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:43,198] {jobs.py:1521} DagFileProcessor231 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:43,198] {models.py:167} DagFileProcessor231 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:43,310] {jobs.py:1535} DagFileProcessor231 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:43,331] {models.py:322} DagFileProcessor231 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:43,331] {models.py:328} DagFileProcessor231 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:43.331736
[2018-04-19 21:07:43,336] {jobs.py:351} DagFileProcessor231 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:07:44,403] {jobs.py:343} DagFileProcessor232 INFO - Started process (PID=3748) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:44,408] {jobs.py:534} DagFileProcessor232 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:44,409] {jobs.py:1521} DagFileProcessor232 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:44,410] {models.py:167} DagFileProcessor232 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:44,512] {jobs.py:1535} DagFileProcessor232 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:44,532] {models.py:322} DagFileProcessor232 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:44,533] {models.py:328} DagFileProcessor232 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:44.533305
[2018-04-19 21:07:44,537] {jobs.py:351} DagFileProcessor232 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:07:45,619] {jobs.py:343} DagFileProcessor233 INFO - Started process (PID=3749) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:45,625] {jobs.py:534} DagFileProcessor233 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:45,626] {jobs.py:1521} DagFileProcessor233 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:45,626] {models.py:167} DagFileProcessor233 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:45,728] {jobs.py:1535} DagFileProcessor233 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:45,750] {models.py:322} DagFileProcessor233 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:45,751] {models.py:328} DagFileProcessor233 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:45.751107
[2018-04-19 21:07:45,755] {jobs.py:351} DagFileProcessor233 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.136 seconds
[2018-04-19 21:07:46,836] {jobs.py:343} DagFileProcessor234 INFO - Started process (PID=3750) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:46,841] {jobs.py:534} DagFileProcessor234 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:46,843] {jobs.py:1521} DagFileProcessor234 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:46,843] {models.py:167} DagFileProcessor234 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:46,945] {jobs.py:1535} DagFileProcessor234 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:46,967] {models.py:322} DagFileProcessor234 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:46,967] {models.py:328} DagFileProcessor234 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:46.967593
[2018-04-19 21:07:46,972] {jobs.py:351} DagFileProcessor234 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.136 seconds
[2018-04-19 21:07:48,067] {jobs.py:343} DagFileProcessor235 INFO - Started process (PID=3751) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:48,072] {jobs.py:534} DagFileProcessor235 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:48,073] {jobs.py:1521} DagFileProcessor235 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:48,073] {models.py:167} DagFileProcessor235 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:48,175] {jobs.py:1535} DagFileProcessor235 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:48,197] {models.py:322} DagFileProcessor235 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:48,198] {models.py:328} DagFileProcessor235 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:48.198353
[2018-04-19 21:07:48,202] {jobs.py:351} DagFileProcessor235 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:07:49,282] {jobs.py:343} DagFileProcessor236 INFO - Started process (PID=3752) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:49,287] {jobs.py:534} DagFileProcessor236 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:49,288] {jobs.py:1521} DagFileProcessor236 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:49,289] {models.py:167} DagFileProcessor236 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:49,393] {jobs.py:1535} DagFileProcessor236 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:49,415] {models.py:322} DagFileProcessor236 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:49,415] {models.py:328} DagFileProcessor236 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:49.415492
[2018-04-19 21:07:49,419] {jobs.py:351} DagFileProcessor236 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.137 seconds
[2018-04-19 21:07:50,504] {jobs.py:343} DagFileProcessor237 INFO - Started process (PID=3753) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:50,509] {jobs.py:534} DagFileProcessor237 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:50,510] {jobs.py:1521} DagFileProcessor237 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:50,510] {models.py:167} DagFileProcessor237 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:50,614] {jobs.py:1535} DagFileProcessor237 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:50,636] {models.py:322} DagFileProcessor237 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:50,636] {models.py:328} DagFileProcessor237 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:50.636400
[2018-04-19 21:07:50,640] {jobs.py:351} DagFileProcessor237 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.136 seconds
[2018-04-19 21:07:51,718] {jobs.py:343} DagFileProcessor238 INFO - Started process (PID=3754) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:51,724] {jobs.py:534} DagFileProcessor238 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:51,725] {jobs.py:1521} DagFileProcessor238 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:51,725] {models.py:167} DagFileProcessor238 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:51,829] {jobs.py:1535} DagFileProcessor238 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:51,853] {models.py:322} DagFileProcessor238 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:51,853] {models.py:328} DagFileProcessor238 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:51.853763
[2018-04-19 21:07:51,858] {jobs.py:351} DagFileProcessor238 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.140 seconds
[2018-04-19 21:07:52,933] {jobs.py:343} DagFileProcessor239 INFO - Started process (PID=3756) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:52,938] {jobs.py:534} DagFileProcessor239 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:52,939] {jobs.py:1521} DagFileProcessor239 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:52,939] {models.py:167} DagFileProcessor239 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:53,044] {jobs.py:1535} DagFileProcessor239 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:53,067] {models.py:322} DagFileProcessor239 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:53,068] {models.py:328} DagFileProcessor239 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:53.068017
[2018-04-19 21:07:53,072] {jobs.py:351} DagFileProcessor239 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:07:54,167] {jobs.py:343} DagFileProcessor240 INFO - Started process (PID=3757) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:54,172] {jobs.py:534} DagFileProcessor240 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:54,174] {jobs.py:1521} DagFileProcessor240 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:54,174] {models.py:167} DagFileProcessor240 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:54,272] {jobs.py:1535} DagFileProcessor240 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:54,294] {models.py:322} DagFileProcessor240 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:54,295] {models.py:328} DagFileProcessor240 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:54.295082
[2018-04-19 21:07:54,299] {jobs.py:351} DagFileProcessor240 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.132 seconds
[2018-04-19 21:07:55,387] {jobs.py:343} DagFileProcessor241 INFO - Started process (PID=3758) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:55,392] {jobs.py:534} DagFileProcessor241 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:55,395] {jobs.py:1521} DagFileProcessor241 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:55,395] {models.py:167} DagFileProcessor241 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:55,493] {jobs.py:1535} DagFileProcessor241 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:55,514] {models.py:322} DagFileProcessor241 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:55,514] {models.py:328} DagFileProcessor241 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:55.514746
[2018-04-19 21:07:55,518] {jobs.py:351} DagFileProcessor241 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.131 seconds
[2018-04-19 21:07:56,603] {jobs.py:343} DagFileProcessor242 INFO - Started process (PID=3759) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:56,617] {jobs.py:534} DagFileProcessor242 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:56,619] {jobs.py:1521} DagFileProcessor242 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:56,620] {models.py:167} DagFileProcessor242 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:56,723] {jobs.py:1535} DagFileProcessor242 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:56,747] {models.py:322} DagFileProcessor242 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:56,748] {models.py:328} DagFileProcessor242 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:56.748079
[2018-04-19 21:07:56,752] {jobs.py:351} DagFileProcessor242 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:07:57,822] {jobs.py:343} DagFileProcessor243 INFO - Started process (PID=3760) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:57,827] {jobs.py:534} DagFileProcessor243 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:57,829] {jobs.py:1521} DagFileProcessor243 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:57,829] {models.py:167} DagFileProcessor243 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:57,932] {jobs.py:1535} DagFileProcessor243 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:57,953] {models.py:322} DagFileProcessor243 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:57,954] {models.py:328} DagFileProcessor243 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:57.954146
[2018-04-19 21:07:57,959] {jobs.py:351} DagFileProcessor243 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.137 seconds
[2018-04-19 21:07:59,036] {jobs.py:343} DagFileProcessor244 INFO - Started process (PID=3761) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:59,041] {jobs.py:534} DagFileProcessor244 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:07:59,043] {jobs.py:1521} DagFileProcessor244 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:07:59,044] {models.py:167} DagFileProcessor244 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:59,144] {jobs.py:1535} DagFileProcessor244 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:07:59,169] {models.py:322} DagFileProcessor244 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:07:59,169] {models.py:328} DagFileProcessor244 INFO - Failing jobs without heartbeat after 2018-04-19 21:02:59.169689
[2018-04-19 21:07:59,173] {jobs.py:351} DagFileProcessor244 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.137 seconds
[2018-04-19 21:08:00,270] {jobs.py:343} DagFileProcessor245 INFO - Started process (PID=3762) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:00,275] {jobs.py:534} DagFileProcessor245 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:00,276] {jobs.py:1521} DagFileProcessor245 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:00,276] {models.py:167} DagFileProcessor245 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:00,381] {jobs.py:1535} DagFileProcessor245 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:00,406] {models.py:322} DagFileProcessor245 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:00,406] {models.py:328} DagFileProcessor245 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:00.406403
[2018-04-19 21:08:00,410] {jobs.py:351} DagFileProcessor245 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:08:01,491] {jobs.py:343} DagFileProcessor246 INFO - Started process (PID=3763) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:01,496] {jobs.py:534} DagFileProcessor246 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:01,497] {jobs.py:1521} DagFileProcessor246 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:01,497] {models.py:167} DagFileProcessor246 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:01,603] {jobs.py:1535} DagFileProcessor246 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:01,626] {models.py:322} DagFileProcessor246 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:01,627] {models.py:328} DagFileProcessor246 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:01.626866
[2018-04-19 21:08:01,631] {jobs.py:351} DagFileProcessor246 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.140 seconds
[2018-04-19 21:08:02,718] {jobs.py:343} DagFileProcessor247 INFO - Started process (PID=3765) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:02,722] {jobs.py:534} DagFileProcessor247 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:02,724] {jobs.py:1521} DagFileProcessor247 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:02,724] {models.py:167} DagFileProcessor247 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:02,827] {jobs.py:1535} DagFileProcessor247 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:02,850] {models.py:322} DagFileProcessor247 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:02,851] {models.py:328} DagFileProcessor247 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:02.851257
[2018-04-19 21:08:02,855] {jobs.py:351} DagFileProcessor247 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.137 seconds
[2018-04-19 21:08:03,937] {jobs.py:343} DagFileProcessor248 INFO - Started process (PID=3766) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:03,941] {jobs.py:534} DagFileProcessor248 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:03,942] {jobs.py:1521} DagFileProcessor248 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:03,943] {models.py:167} DagFileProcessor248 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:04,043] {jobs.py:1535} DagFileProcessor248 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:04,063] {models.py:322} DagFileProcessor248 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:04,064] {models.py:328} DagFileProcessor248 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:04.064187
[2018-04-19 21:08:04,068] {jobs.py:351} DagFileProcessor248 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.132 seconds
[2018-04-19 21:08:05,154] {jobs.py:343} DagFileProcessor249 INFO - Started process (PID=3767) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:05,160] {jobs.py:534} DagFileProcessor249 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:05,161] {jobs.py:1521} DagFileProcessor249 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:05,161] {models.py:167} DagFileProcessor249 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:05,261] {jobs.py:1535} DagFileProcessor249 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:05,282] {models.py:322} DagFileProcessor249 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:05,282] {models.py:328} DagFileProcessor249 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:05.282787
[2018-04-19 21:08:05,287] {jobs.py:351} DagFileProcessor249 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.132 seconds
[2018-04-19 21:08:06,384] {jobs.py:343} DagFileProcessor250 INFO - Started process (PID=3768) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:06,388] {jobs.py:534} DagFileProcessor250 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:06,389] {jobs.py:1521} DagFileProcessor250 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:06,390] {models.py:167} DagFileProcessor250 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:06,507] {jobs.py:1535} DagFileProcessor250 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:06,532] {models.py:322} DagFileProcessor250 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:06,533] {models.py:328} DagFileProcessor250 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:06.532844
[2018-04-19 21:08:06,537] {jobs.py:351} DagFileProcessor250 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:08:07,600] {jobs.py:343} DagFileProcessor251 INFO - Started process (PID=3769) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:07,604] {jobs.py:534} DagFileProcessor251 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:07,605] {jobs.py:1521} DagFileProcessor251 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:07,606] {models.py:167} DagFileProcessor251 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:07,707] {jobs.py:1535} DagFileProcessor251 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:07,730] {models.py:322} DagFileProcessor251 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:07,730] {models.py:328} DagFileProcessor251 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:07.730615
[2018-04-19 21:08:07,735] {jobs.py:351} DagFileProcessor251 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:08:08,814] {jobs.py:343} DagFileProcessor252 INFO - Started process (PID=3770) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:08,819] {jobs.py:534} DagFileProcessor252 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:08,820] {jobs.py:1521} DagFileProcessor252 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:08,820] {models.py:167} DagFileProcessor252 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:08,925] {jobs.py:1535} DagFileProcessor252 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:08,947] {models.py:322} DagFileProcessor252 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:08,947] {models.py:328} DagFileProcessor252 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:08.947455
[2018-04-19 21:08:08,951] {jobs.py:351} DagFileProcessor252 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.137 seconds
[2018-04-19 21:08:10,030] {jobs.py:343} DagFileProcessor253 INFO - Started process (PID=3771) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:10,035] {jobs.py:534} DagFileProcessor253 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:10,036] {jobs.py:1521} DagFileProcessor253 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:10,037] {models.py:167} DagFileProcessor253 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:10,135] {jobs.py:1535} DagFileProcessor253 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:10,156] {models.py:322} DagFileProcessor253 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:10,157] {models.py:328} DagFileProcessor253 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:10.157021
[2018-04-19 21:08:10,161] {jobs.py:351} DagFileProcessor253 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.130 seconds
[2018-04-19 21:08:11,248] {jobs.py:343} DagFileProcessor254 INFO - Started process (PID=3772) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:11,253] {jobs.py:534} DagFileProcessor254 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:11,254] {jobs.py:1521} DagFileProcessor254 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:11,255] {models.py:167} DagFileProcessor254 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:11,354] {jobs.py:1535} DagFileProcessor254 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:11,375] {models.py:322} DagFileProcessor254 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:11,376] {models.py:328} DagFileProcessor254 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:11.375979
[2018-04-19 21:08:11,380] {jobs.py:351} DagFileProcessor254 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.132 seconds
[2018-04-19 21:08:12,476] {jobs.py:343} DagFileProcessor255 INFO - Started process (PID=3780) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:12,481] {jobs.py:534} DagFileProcessor255 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:12,482] {jobs.py:1521} DagFileProcessor255 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:12,482] {models.py:167} DagFileProcessor255 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:12,591] {jobs.py:1535} DagFileProcessor255 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:12,612] {models.py:322} DagFileProcessor255 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:12,613] {models.py:328} DagFileProcessor255 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:12.612839
[2018-04-19 21:08:12,617] {jobs.py:351} DagFileProcessor255 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:08:13,697] {jobs.py:343} DagFileProcessor256 INFO - Started process (PID=3782) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:13,702] {jobs.py:534} DagFileProcessor256 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:13,703] {jobs.py:1521} DagFileProcessor256 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:13,703] {models.py:167} DagFileProcessor256 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:13,806] {jobs.py:1535} DagFileProcessor256 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:13,827] {models.py:322} DagFileProcessor256 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:13,828] {models.py:328} DagFileProcessor256 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:13.828024
[2018-04-19 21:08:13,832] {jobs.py:351} DagFileProcessor256 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:08:14,914] {jobs.py:343} DagFileProcessor257 INFO - Started process (PID=3783) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:14,919] {jobs.py:534} DagFileProcessor257 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:14,920] {jobs.py:1521} DagFileProcessor257 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:14,921] {models.py:167} DagFileProcessor257 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:15,023] {jobs.py:1535} DagFileProcessor257 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:15,044] {models.py:322} DagFileProcessor257 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:15,045] {models.py:328} DagFileProcessor257 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:15.045065
[2018-04-19 21:08:15,049] {jobs.py:351} DagFileProcessor257 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:08:16,134] {jobs.py:343} DagFileProcessor258 INFO - Started process (PID=3784) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:16,140] {jobs.py:534} DagFileProcessor258 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:16,142] {jobs.py:1521} DagFileProcessor258 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:16,142] {models.py:167} DagFileProcessor258 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:16,250] {jobs.py:1535} DagFileProcessor258 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:16,273] {models.py:322} DagFileProcessor258 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:16,274] {models.py:328} DagFileProcessor258 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:16.273826
[2018-04-19 21:08:16,277] {jobs.py:351} DagFileProcessor258 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:08:17,348] {jobs.py:343} DagFileProcessor259 INFO - Started process (PID=3785) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:17,353] {jobs.py:534} DagFileProcessor259 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:17,354] {jobs.py:1521} DagFileProcessor259 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:17,354] {models.py:167} DagFileProcessor259 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:17,458] {jobs.py:1535} DagFileProcessor259 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:17,477] {models.py:322} DagFileProcessor259 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:17,478] {models.py:328} DagFileProcessor259 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:17.477844
[2018-04-19 21:08:17,482] {jobs.py:351} DagFileProcessor259 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:08:18,577] {jobs.py:343} DagFileProcessor260 INFO - Started process (PID=3786) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:18,583] {jobs.py:534} DagFileProcessor260 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:18,584] {jobs.py:1521} DagFileProcessor260 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:18,584] {models.py:167} DagFileProcessor260 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:18,685] {jobs.py:1535} DagFileProcessor260 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:18,709] {models.py:322} DagFileProcessor260 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:18,710] {models.py:328} DagFileProcessor260 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:18.710295
[2018-04-19 21:08:18,714] {jobs.py:351} DagFileProcessor260 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.137 seconds
[2018-04-19 21:08:19,795] {jobs.py:343} DagFileProcessor261 INFO - Started process (PID=3787) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:19,800] {jobs.py:534} DagFileProcessor261 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:19,802] {jobs.py:1521} DagFileProcessor261 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:19,802] {models.py:167} DagFileProcessor261 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:19,911] {jobs.py:1535} DagFileProcessor261 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:19,932] {models.py:322} DagFileProcessor261 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:19,932] {models.py:328} DagFileProcessor261 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:19.932700
[2018-04-19 21:08:19,936] {jobs.py:351} DagFileProcessor261 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:08:21,012] {jobs.py:343} DagFileProcessor262 INFO - Started process (PID=3788) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:21,017] {jobs.py:534} DagFileProcessor262 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:21,018] {jobs.py:1521} DagFileProcessor262 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:21,019] {models.py:167} DagFileProcessor262 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:21,131] {jobs.py:1535} DagFileProcessor262 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:21,153] {models.py:322} DagFileProcessor262 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:21,154] {models.py:328} DagFileProcessor262 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:21.154121
[2018-04-19 21:08:21,158] {jobs.py:351} DagFileProcessor262 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:08:22,226] {jobs.py:343} DagFileProcessor263 INFO - Started process (PID=3789) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:22,231] {jobs.py:534} DagFileProcessor263 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:22,232] {jobs.py:1521} DagFileProcessor263 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:22,232] {models.py:167} DagFileProcessor263 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:22,333] {jobs.py:1535} DagFileProcessor263 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:22,355] {models.py:322} DagFileProcessor263 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:22,355] {models.py:328} DagFileProcessor263 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:22.355615
[2018-04-19 21:08:22,360] {jobs.py:351} DagFileProcessor263 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:08:23,445] {jobs.py:343} DagFileProcessor264 INFO - Started process (PID=3791) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:23,450] {jobs.py:534} DagFileProcessor264 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:23,451] {jobs.py:1521} DagFileProcessor264 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:23,452] {models.py:167} DagFileProcessor264 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:23,554] {jobs.py:1535} DagFileProcessor264 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:23,574] {models.py:322} DagFileProcessor264 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:23,575] {models.py:328} DagFileProcessor264 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:23.575014
[2018-04-19 21:08:23,579] {jobs.py:351} DagFileProcessor264 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:08:24,673] {jobs.py:343} DagFileProcessor265 INFO - Started process (PID=3792) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:24,678] {jobs.py:534} DagFileProcessor265 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:24,680] {jobs.py:1521} DagFileProcessor265 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:24,681] {models.py:167} DagFileProcessor265 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:24,792] {jobs.py:1535} DagFileProcessor265 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:24,813] {models.py:322} DagFileProcessor265 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:24,814] {models.py:328} DagFileProcessor265 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:24.814320
[2018-04-19 21:08:24,819] {jobs.py:351} DagFileProcessor265 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:08:25,894] {jobs.py:343} DagFileProcessor266 INFO - Started process (PID=3793) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:25,899] {jobs.py:534} DagFileProcessor266 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:25,901] {jobs.py:1521} DagFileProcessor266 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:25,902] {models.py:167} DagFileProcessor266 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:26,009] {jobs.py:1535} DagFileProcessor266 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:26,033] {models.py:322} DagFileProcessor266 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:26,033] {models.py:328} DagFileProcessor266 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:26.033707
[2018-04-19 21:08:26,039] {jobs.py:351} DagFileProcessor266 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:08:27,106] {jobs.py:343} DagFileProcessor267 INFO - Started process (PID=3794) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:27,111] {jobs.py:534} DagFileProcessor267 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:27,113] {jobs.py:1521} DagFileProcessor267 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:27,114] {models.py:167} DagFileProcessor267 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:27,220] {jobs.py:1535} DagFileProcessor267 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:27,243] {models.py:322} DagFileProcessor267 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:27,243] {models.py:328} DagFileProcessor267 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:27.243782
[2018-04-19 21:08:27,248] {jobs.py:351} DagFileProcessor267 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.142 seconds
[2018-04-19 21:08:28,320] {jobs.py:343} DagFileProcessor268 INFO - Started process (PID=3795) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:28,325] {jobs.py:534} DagFileProcessor268 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:28,327] {jobs.py:1521} DagFileProcessor268 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:28,328] {models.py:167} DagFileProcessor268 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:28,430] {jobs.py:1535} DagFileProcessor268 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:28,452] {models.py:322} DagFileProcessor268 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:28,452] {models.py:328} DagFileProcessor268 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:28.452419
[2018-04-19 21:08:28,456] {jobs.py:351} DagFileProcessor268 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.136 seconds
[2018-04-19 21:08:29,535] {jobs.py:343} DagFileProcessor269 INFO - Started process (PID=3796) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:29,540] {jobs.py:534} DagFileProcessor269 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:29,543] {jobs.py:1521} DagFileProcessor269 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:29,543] {models.py:167} DagFileProcessor269 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:29,645] {jobs.py:1535} DagFileProcessor269 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:29,666] {models.py:322} DagFileProcessor269 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:29,666] {models.py:328} DagFileProcessor269 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:29.666664
[2018-04-19 21:08:29,670] {jobs.py:351} DagFileProcessor269 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:08:30,765] {jobs.py:343} DagFileProcessor270 INFO - Started process (PID=3797) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:30,770] {jobs.py:534} DagFileProcessor270 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:30,771] {jobs.py:1521} DagFileProcessor270 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:30,772] {models.py:167} DagFileProcessor270 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:30,875] {jobs.py:1535} DagFileProcessor270 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:30,897] {models.py:322} DagFileProcessor270 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:30,898] {models.py:328} DagFileProcessor270 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:30.898218
[2018-04-19 21:08:30,902] {jobs.py:351} DagFileProcessor270 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.138 seconds
[2018-04-19 21:08:31,982] {jobs.py:343} DagFileProcessor271 INFO - Started process (PID=3798) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:31,987] {jobs.py:534} DagFileProcessor271 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:31,989] {jobs.py:1521} DagFileProcessor271 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:31,989] {models.py:167} DagFileProcessor271 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:32,090] {jobs.py:1535} DagFileProcessor271 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:32,111] {models.py:322} DagFileProcessor271 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:32,112] {models.py:328} DagFileProcessor271 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:32.111832
[2018-04-19 21:08:32,116] {jobs.py:351} DagFileProcessor271 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.133 seconds
[2018-04-19 21:08:33,200] {jobs.py:343} DagFileProcessor272 INFO - Started process (PID=3800) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:33,205] {jobs.py:534} DagFileProcessor272 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:33,207] {jobs.py:1521} DagFileProcessor272 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:33,207] {models.py:167} DagFileProcessor272 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:33,311] {jobs.py:1535} DagFileProcessor272 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:33,334] {models.py:322} DagFileProcessor272 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:33,335] {models.py:328} DagFileProcessor272 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:33.334879
[2018-04-19 21:08:33,339] {jobs.py:351} DagFileProcessor272 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:08:34,412] {jobs.py:343} DagFileProcessor273 INFO - Started process (PID=3801) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:34,417] {jobs.py:534} DagFileProcessor273 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:34,418] {jobs.py:1521} DagFileProcessor273 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:34,419] {models.py:167} DagFileProcessor273 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:34,518] {jobs.py:1535} DagFileProcessor273 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:34,540] {models.py:322} DagFileProcessor273 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:34,541] {models.py:328} DagFileProcessor273 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:34.541281
[2018-04-19 21:08:34,545] {jobs.py:351} DagFileProcessor273 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.133 seconds
[2018-04-19 21:08:35,630] {jobs.py:343} DagFileProcessor274 INFO - Started process (PID=3802) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:35,635] {jobs.py:534} DagFileProcessor274 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:35,636] {jobs.py:1521} DagFileProcessor274 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:35,637] {models.py:167} DagFileProcessor274 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:35,739] {jobs.py:1535} DagFileProcessor274 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:35,761] {models.py:322} DagFileProcessor274 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:35,762] {models.py:328} DagFileProcessor274 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:35.762332
[2018-04-19 21:08:35,768] {jobs.py:351} DagFileProcessor274 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.138 seconds
[2018-04-19 21:08:36,861] {jobs.py:343} DagFileProcessor275 INFO - Started process (PID=3803) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:36,866] {jobs.py:534} DagFileProcessor275 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:36,867] {jobs.py:1521} DagFileProcessor275 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:36,867] {models.py:167} DagFileProcessor275 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:36,973] {jobs.py:1535} DagFileProcessor275 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:36,999] {models.py:322} DagFileProcessor275 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:36,999] {models.py:328} DagFileProcessor275 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:36.999366
[2018-04-19 21:08:37,003] {jobs.py:351} DagFileProcessor275 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.142 seconds
[2018-04-19 21:08:38,082] {jobs.py:343} DagFileProcessor276 INFO - Started process (PID=3804) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:38,087] {jobs.py:534} DagFileProcessor276 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:38,088] {jobs.py:1521} DagFileProcessor276 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:38,088] {models.py:167} DagFileProcessor276 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:38,195] {jobs.py:1535} DagFileProcessor276 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:38,219] {models.py:322} DagFileProcessor276 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:38,220] {models.py:328} DagFileProcessor276 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:38.219864
[2018-04-19 21:08:38,224] {jobs.py:351} DagFileProcessor276 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:08:39,301] {jobs.py:343} DagFileProcessor277 INFO - Started process (PID=3805) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:39,306] {jobs.py:534} DagFileProcessor277 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:39,307] {jobs.py:1521} DagFileProcessor277 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:39,307] {models.py:167} DagFileProcessor277 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:39,414] {jobs.py:1535} DagFileProcessor277 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:39,436] {models.py:322} DagFileProcessor277 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:39,437] {models.py:328} DagFileProcessor277 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:39.437311
[2018-04-19 21:08:39,441] {jobs.py:351} DagFileProcessor277 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.140 seconds
[2018-04-19 21:08:40,514] {jobs.py:343} DagFileProcessor278 INFO - Started process (PID=3806) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:40,519] {jobs.py:534} DagFileProcessor278 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:40,521] {jobs.py:1521} DagFileProcessor278 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:40,522] {models.py:167} DagFileProcessor278 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:40,627] {jobs.py:1535} DagFileProcessor278 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:40,648] {models.py:322} DagFileProcessor278 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:40,649] {models.py:328} DagFileProcessor278 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:40.648876
[2018-04-19 21:08:40,653] {jobs.py:351} DagFileProcessor278 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:08:41,733] {jobs.py:343} DagFileProcessor279 INFO - Started process (PID=3807) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:41,738] {jobs.py:534} DagFileProcessor279 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:41,740] {jobs.py:1521} DagFileProcessor279 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:41,740] {models.py:167} DagFileProcessor279 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:41,875] {jobs.py:1535} DagFileProcessor279 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:41,896] {models.py:322} DagFileProcessor279 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:41,897] {models.py:328} DagFileProcessor279 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:41.897159
[2018-04-19 21:08:41,902] {jobs.py:351} DagFileProcessor279 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.169 seconds
[2018-04-19 21:08:42,954] {jobs.py:343} DagFileProcessor280 INFO - Started process (PID=3809) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:42,959] {jobs.py:534} DagFileProcessor280 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:42,960] {jobs.py:1521} DagFileProcessor280 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:42,960] {models.py:167} DagFileProcessor280 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:43,060] {jobs.py:1535} DagFileProcessor280 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:43,083] {models.py:322} DagFileProcessor280 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:43,084] {models.py:328} DagFileProcessor280 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:43.084281
[2018-04-19 21:08:43,089] {jobs.py:351} DagFileProcessor280 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:08:44,171] {jobs.py:343} DagFileProcessor281 INFO - Started process (PID=3810) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:44,175] {jobs.py:534} DagFileProcessor281 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:44,177] {jobs.py:1521} DagFileProcessor281 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:44,177] {models.py:167} DagFileProcessor281 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:44,279] {jobs.py:1535} DagFileProcessor281 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:44,306] {models.py:322} DagFileProcessor281 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:44,306] {models.py:328} DagFileProcessor281 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:44.306564
[2018-04-19 21:08:44,311] {jobs.py:351} DagFileProcessor281 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.140 seconds
[2018-04-19 21:08:45,383] {jobs.py:343} DagFileProcessor282 INFO - Started process (PID=3811) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:45,388] {jobs.py:534} DagFileProcessor282 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:45,389] {jobs.py:1521} DagFileProcessor282 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:45,390] {models.py:167} DagFileProcessor282 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:45,491] {jobs.py:1535} DagFileProcessor282 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:45,513] {models.py:322} DagFileProcessor282 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:45,513] {models.py:328} DagFileProcessor282 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:45.513653
[2018-04-19 21:08:45,517] {jobs.py:351} DagFileProcessor282 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:08:46,605] {jobs.py:343} DagFileProcessor283 INFO - Started process (PID=3812) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:46,610] {jobs.py:534} DagFileProcessor283 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:46,611] {jobs.py:1521} DagFileProcessor283 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:46,611] {models.py:167} DagFileProcessor283 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:46,710] {jobs.py:1535} DagFileProcessor283 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:46,734] {models.py:322} DagFileProcessor283 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:46,735] {models.py:328} DagFileProcessor283 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:46.735458
[2018-04-19 21:08:46,740] {jobs.py:351} DagFileProcessor283 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:08:47,822] {jobs.py:343} DagFileProcessor284 INFO - Started process (PID=3813) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:47,827] {jobs.py:534} DagFileProcessor284 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:47,828] {jobs.py:1521} DagFileProcessor284 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:47,828] {models.py:167} DagFileProcessor284 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:47,927] {jobs.py:1535} DagFileProcessor284 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:47,949] {models.py:322} DagFileProcessor284 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:47,950] {models.py:328} DagFileProcessor284 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:47.949972
[2018-04-19 21:08:47,954] {jobs.py:351} DagFileProcessor284 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.133 seconds
[2018-04-19 21:08:49,047] {jobs.py:343} DagFileProcessor285 INFO - Started process (PID=3814) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:49,053] {jobs.py:534} DagFileProcessor285 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:49,054] {jobs.py:1521} DagFileProcessor285 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:49,054] {models.py:167} DagFileProcessor285 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:49,155] {jobs.py:1535} DagFileProcessor285 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:49,177] {models.py:322} DagFileProcessor285 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:49,178] {models.py:328} DagFileProcessor285 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:49.178253
[2018-04-19 21:08:49,182] {jobs.py:351} DagFileProcessor285 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:08:50,261] {jobs.py:343} DagFileProcessor286 INFO - Started process (PID=3815) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:50,266] {jobs.py:534} DagFileProcessor286 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:50,267] {jobs.py:1521} DagFileProcessor286 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:50,267] {models.py:167} DagFileProcessor286 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:50,369] {jobs.py:1535} DagFileProcessor286 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:50,390] {models.py:322} DagFileProcessor286 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:50,391] {models.py:328} DagFileProcessor286 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:50.390942
[2018-04-19 21:08:50,396] {jobs.py:351} DagFileProcessor286 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:08:51,485] {jobs.py:343} DagFileProcessor287 INFO - Started process (PID=3816) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:51,490] {jobs.py:534} DagFileProcessor287 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:51,491] {jobs.py:1521} DagFileProcessor287 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:51,491] {models.py:167} DagFileProcessor287 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:51,600] {jobs.py:1535} DagFileProcessor287 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:51,623] {models.py:322} DagFileProcessor287 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:51,624] {models.py:328} DagFileProcessor287 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:51.624294
[2018-04-19 21:08:51,628] {jobs.py:351} DagFileProcessor287 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:08:52,695] {jobs.py:343} DagFileProcessor288 INFO - Started process (PID=3818) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:52,701] {jobs.py:534} DagFileProcessor288 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:52,702] {jobs.py:1521} DagFileProcessor288 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:52,702] {models.py:167} DagFileProcessor288 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:52,804] {jobs.py:1535} DagFileProcessor288 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:52,827] {models.py:322} DagFileProcessor288 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:52,827] {models.py:328} DagFileProcessor288 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:52.827486
[2018-04-19 21:08:52,831] {jobs.py:351} DagFileProcessor288 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.136 seconds
[2018-04-19 21:08:53,909] {jobs.py:343} DagFileProcessor289 INFO - Started process (PID=3819) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:53,915] {jobs.py:534} DagFileProcessor289 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:53,916] {jobs.py:1521} DagFileProcessor289 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:53,917] {models.py:167} DagFileProcessor289 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:54,020] {jobs.py:1535} DagFileProcessor289 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:54,044] {models.py:322} DagFileProcessor289 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:54,044] {models.py:328} DagFileProcessor289 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:54.044501
[2018-04-19 21:08:54,048] {jobs.py:351} DagFileProcessor289 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:08:55,142] {jobs.py:343} DagFileProcessor290 INFO - Started process (PID=3820) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:55,147] {jobs.py:534} DagFileProcessor290 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:55,148] {jobs.py:1521} DagFileProcessor290 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:55,149] {models.py:167} DagFileProcessor290 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:55,247] {jobs.py:1535} DagFileProcessor290 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:55,268] {models.py:322} DagFileProcessor290 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:55,269] {models.py:328} DagFileProcessor290 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:55.268978
[2018-04-19 21:08:55,273] {jobs.py:351} DagFileProcessor290 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.130 seconds
[2018-04-19 21:08:56,356] {jobs.py:343} DagFileProcessor291 INFO - Started process (PID=3821) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:56,361] {jobs.py:534} DagFileProcessor291 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:56,362] {jobs.py:1521} DagFileProcessor291 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:56,362] {models.py:167} DagFileProcessor291 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:56,469] {jobs.py:1535} DagFileProcessor291 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:56,491] {models.py:322} DagFileProcessor291 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:56,492] {models.py:328} DagFileProcessor291 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:56.491841
[2018-04-19 21:08:56,496] {jobs.py:351} DagFileProcessor291 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:08:57,571] {jobs.py:343} DagFileProcessor292 INFO - Started process (PID=3822) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:57,576] {jobs.py:534} DagFileProcessor292 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:57,577] {jobs.py:1521} DagFileProcessor292 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:57,578] {models.py:167} DagFileProcessor292 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:57,684] {jobs.py:1535} DagFileProcessor292 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:57,707] {models.py:322} DagFileProcessor292 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:57,708] {models.py:328} DagFileProcessor292 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:57.708022
[2018-04-19 21:08:57,712] {jobs.py:351} DagFileProcessor292 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:08:58,785] {jobs.py:343} DagFileProcessor293 INFO - Started process (PID=3823) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:58,790] {jobs.py:534} DagFileProcessor293 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:08:58,791] {jobs.py:1521} DagFileProcessor293 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:08:58,792] {models.py:167} DagFileProcessor293 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:58,900] {jobs.py:1535} DagFileProcessor293 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:08:58,924] {models.py:322} DagFileProcessor293 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:08:58,924] {models.py:328} DagFileProcessor293 INFO - Failing jobs without heartbeat after 2018-04-19 21:03:58.924672
[2018-04-19 21:08:58,929] {jobs.py:351} DagFileProcessor293 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:09:00,001] {jobs.py:343} DagFileProcessor294 INFO - Started process (PID=3824) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:00,006] {jobs.py:534} DagFileProcessor294 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:00,007] {jobs.py:1521} DagFileProcessor294 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:00,008] {models.py:167} DagFileProcessor294 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:00,112] {jobs.py:1535} DagFileProcessor294 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:00,134] {models.py:322} DagFileProcessor294 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:00,135] {models.py:328} DagFileProcessor294 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:00.134905
[2018-04-19 21:09:00,139] {jobs.py:351} DagFileProcessor294 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.138 seconds
[2018-04-19 21:09:01,228] {jobs.py:343} DagFileProcessor295 INFO - Started process (PID=3825) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:01,233] {jobs.py:534} DagFileProcessor295 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:01,234] {jobs.py:1521} DagFileProcessor295 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:01,234] {models.py:167} DagFileProcessor295 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:01,343] {jobs.py:1535} DagFileProcessor295 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:01,364] {models.py:322} DagFileProcessor295 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:01,365] {models.py:328} DagFileProcessor295 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:01.365379
[2018-04-19 21:09:01,369] {jobs.py:351} DagFileProcessor295 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:09:02,443] {jobs.py:343} DagFileProcessor296 INFO - Started process (PID=3826) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:02,448] {jobs.py:534} DagFileProcessor296 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:02,449] {jobs.py:1521} DagFileProcessor296 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:02,449] {models.py:167} DagFileProcessor296 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:02,547] {jobs.py:1535} DagFileProcessor296 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:02,569] {models.py:322} DagFileProcessor296 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:02,570] {models.py:328} DagFileProcessor296 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:02.569820
[2018-04-19 21:09:02,573] {jobs.py:351} DagFileProcessor296 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.131 seconds
[2018-04-19 21:09:03,664] {jobs.py:343} DagFileProcessor297 INFO - Started process (PID=3828) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:03,670] {jobs.py:534} DagFileProcessor297 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:03,671] {jobs.py:1521} DagFileProcessor297 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:03,671] {models.py:167} DagFileProcessor297 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:03,770] {jobs.py:1535} DagFileProcessor297 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:03,791] {models.py:322} DagFileProcessor297 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:03,792] {models.py:328} DagFileProcessor297 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:03.792180
[2018-04-19 21:09:03,796] {jobs.py:351} DagFileProcessor297 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.131 seconds
[2018-04-19 21:09:04,882] {jobs.py:343} DagFileProcessor298 INFO - Started process (PID=3829) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:04,887] {jobs.py:534} DagFileProcessor298 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:04,889] {jobs.py:1521} DagFileProcessor298 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:04,889] {models.py:167} DagFileProcessor298 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:04,992] {jobs.py:1535} DagFileProcessor298 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:05,016] {models.py:322} DagFileProcessor298 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:05,016] {models.py:328} DagFileProcessor298 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:05.016687
[2018-04-19 21:09:05,021] {jobs.py:351} DagFileProcessor298 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.138 seconds
[2018-04-19 21:09:06,095] {jobs.py:343} DagFileProcessor299 INFO - Started process (PID=3830) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:06,100] {jobs.py:534} DagFileProcessor299 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:06,101] {jobs.py:1521} DagFileProcessor299 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:06,101] {models.py:167} DagFileProcessor299 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:06,204] {jobs.py:1535} DagFileProcessor299 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:06,226] {models.py:322} DagFileProcessor299 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:06,227] {models.py:328} DagFileProcessor299 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:06.227274
[2018-04-19 21:09:06,231] {jobs.py:351} DagFileProcessor299 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.136 seconds
[2018-04-19 21:09:07,325] {jobs.py:343} DagFileProcessor300 INFO - Started process (PID=3831) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:07,330] {jobs.py:534} DagFileProcessor300 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:07,331] {jobs.py:1521} DagFileProcessor300 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:07,331] {models.py:167} DagFileProcessor300 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:07,432] {jobs.py:1535} DagFileProcessor300 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:07,453] {models.py:322} DagFileProcessor300 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:07,453] {models.py:328} DagFileProcessor300 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:07.453557
[2018-04-19 21:09:07,457] {jobs.py:351} DagFileProcessor300 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.133 seconds
[2018-04-19 21:09:08,542] {jobs.py:343} DagFileProcessor301 INFO - Started process (PID=3832) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:08,547] {jobs.py:534} DagFileProcessor301 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:08,548] {jobs.py:1521} DagFileProcessor301 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:08,548] {models.py:167} DagFileProcessor301 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:08,648] {jobs.py:1535} DagFileProcessor301 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:08,670] {models.py:322} DagFileProcessor301 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:08,670] {models.py:328} DagFileProcessor301 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:08.670413
[2018-04-19 21:09:08,674] {jobs.py:351} DagFileProcessor301 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.132 seconds
[2018-04-19 21:09:09,758] {jobs.py:343} DagFileProcessor302 INFO - Started process (PID=3833) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:09,763] {jobs.py:534} DagFileProcessor302 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:09,764] {jobs.py:1521} DagFileProcessor302 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:09,764] {models.py:167} DagFileProcessor302 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:09,863] {jobs.py:1535} DagFileProcessor302 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:09,884] {models.py:322} DagFileProcessor302 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:09,885] {models.py:328} DagFileProcessor302 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:09.885281
[2018-04-19 21:09:09,889] {jobs.py:351} DagFileProcessor302 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.131 seconds
[2018-04-19 21:09:10,975] {jobs.py:343} DagFileProcessor303 INFO - Started process (PID=3834) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:10,980] {jobs.py:534} DagFileProcessor303 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:10,981] {jobs.py:1521} DagFileProcessor303 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:10,981] {models.py:167} DagFileProcessor303 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:11,082] {jobs.py:1535} DagFileProcessor303 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:11,108] {models.py:322} DagFileProcessor303 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:11,108] {models.py:328} DagFileProcessor303 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:11.108530
[2018-04-19 21:09:11,112] {jobs.py:351} DagFileProcessor303 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.137 seconds
[2018-04-19 21:09:12,184] {jobs.py:343} DagFileProcessor304 INFO - Started process (PID=3836) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:12,189] {jobs.py:534} DagFileProcessor304 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:12,190] {jobs.py:1521} DagFileProcessor304 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:12,191] {models.py:167} DagFileProcessor304 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:12,299] {jobs.py:1535} DagFileProcessor304 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:12,319] {models.py:322} DagFileProcessor304 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:12,320] {models.py:328} DagFileProcessor304 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:12.320005
[2018-04-19 21:09:12,324] {jobs.py:351} DagFileProcessor304 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.140 seconds
[2018-04-19 21:09:13,408] {jobs.py:343} DagFileProcessor305 INFO - Started process (PID=3844) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:13,412] {jobs.py:534} DagFileProcessor305 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:13,414] {jobs.py:1521} DagFileProcessor305 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:13,414] {models.py:167} DagFileProcessor305 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:13,517] {jobs.py:1535} DagFileProcessor305 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:13,537] {models.py:322} DagFileProcessor305 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:13,537] {models.py:328} DagFileProcessor305 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:13.537704
[2018-04-19 21:09:13,542] {jobs.py:351} DagFileProcessor305 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.134 seconds
[2018-04-19 21:09:14,622] {jobs.py:343} DagFileProcessor306 INFO - Started process (PID=3845) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:14,627] {jobs.py:534} DagFileProcessor306 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:14,628] {jobs.py:1521} DagFileProcessor306 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:14,628] {models.py:167} DagFileProcessor306 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:14,729] {jobs.py:1535} DagFileProcessor306 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:14,750] {models.py:322} DagFileProcessor306 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:14,751] {models.py:328} DagFileProcessor306 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:14.751273
[2018-04-19 21:09:14,755] {jobs.py:351} DagFileProcessor306 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.133 seconds
[2018-04-19 21:09:15,838] {jobs.py:343} DagFileProcessor307 INFO - Started process (PID=3846) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:15,843] {jobs.py:534} DagFileProcessor307 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:15,844] {jobs.py:1521} DagFileProcessor307 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:15,844] {models.py:167} DagFileProcessor307 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:15,944] {jobs.py:1535} DagFileProcessor307 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:15,965] {models.py:322} DagFileProcessor307 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:15,965] {models.py:328} DagFileProcessor307 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:15.965601
[2018-04-19 21:09:15,969] {jobs.py:351} DagFileProcessor307 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.132 seconds
[2018-04-19 21:09:17,055] {jobs.py:343} DagFileProcessor308 INFO - Started process (PID=3847) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:17,060] {jobs.py:534} DagFileProcessor308 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:17,061] {jobs.py:1521} DagFileProcessor308 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:17,061] {models.py:167} DagFileProcessor308 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:17,164] {jobs.py:1535} DagFileProcessor308 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:17,188] {models.py:322} DagFileProcessor308 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:17,189] {models.py:328} DagFileProcessor308 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:17.189306
[2018-04-19 21:09:17,193] {jobs.py:351} DagFileProcessor308 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:09:18,275] {jobs.py:343} DagFileProcessor309 INFO - Started process (PID=3848) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:18,280] {jobs.py:534} DagFileProcessor309 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:18,281] {jobs.py:1521} DagFileProcessor309 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:18,281] {models.py:167} DagFileProcessor309 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:18,385] {jobs.py:1535} DagFileProcessor309 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:18,407] {models.py:322} DagFileProcessor309 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:18,408] {models.py:328} DagFileProcessor309 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:18.407956
[2018-04-19 21:09:18,412] {jobs.py:351} DagFileProcessor309 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.137 seconds
[2018-04-19 21:09:19,503] {jobs.py:343} DagFileProcessor310 INFO - Started process (PID=3849) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:19,508] {jobs.py:534} DagFileProcessor310 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:19,509] {jobs.py:1521} DagFileProcessor310 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:19,509] {models.py:167} DagFileProcessor310 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:19,611] {jobs.py:1535} DagFileProcessor310 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:19,633] {models.py:322} DagFileProcessor310 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:19,634] {models.py:328} DagFileProcessor310 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:19.633899
[2018-04-19 21:09:19,638] {jobs.py:351} DagFileProcessor310 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:09:20,722] {jobs.py:343} DagFileProcessor311 INFO - Started process (PID=3850) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:20,727] {jobs.py:534} DagFileProcessor311 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:20,728] {jobs.py:1521} DagFileProcessor311 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:20,729] {models.py:167} DagFileProcessor311 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:20,832] {jobs.py:1535} DagFileProcessor311 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:20,854] {models.py:322} DagFileProcessor311 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:20,855] {models.py:328} DagFileProcessor311 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:20.854823
[2018-04-19 21:09:20,859] {jobs.py:351} DagFileProcessor311 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.136 seconds
[2018-04-19 21:09:21,936] {jobs.py:343} DagFileProcessor312 INFO - Started process (PID=3851) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:21,941] {jobs.py:534} DagFileProcessor312 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:21,942] {jobs.py:1521} DagFileProcessor312 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:21,943] {models.py:167} DagFileProcessor312 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:22,045] {jobs.py:1535} DagFileProcessor312 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:22,067] {models.py:322} DagFileProcessor312 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:22,067] {models.py:328} DagFileProcessor312 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:22.067542
[2018-04-19 21:09:22,071] {jobs.py:351} DagFileProcessor312 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.135 seconds
[2018-04-19 21:09:23,157] {jobs.py:343} DagFileProcessor313 INFO - Started process (PID=3853) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:23,162] {jobs.py:534} DagFileProcessor313 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:23,163] {jobs.py:1521} DagFileProcessor313 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:23,164] {models.py:167} DagFileProcessor313 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:23,269] {jobs.py:1535} DagFileProcessor313 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:23,292] {models.py:322} DagFileProcessor313 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:23,292] {models.py:328} DagFileProcessor313 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:23.292614
[2018-04-19 21:09:23,297] {jobs.py:351} DagFileProcessor313 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.139 seconds
[2018-04-19 21:09:24,374] {jobs.py:343} DagFileProcessor314 INFO - Started process (PID=3854) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:24,381] {jobs.py:534} DagFileProcessor314 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:24,382] {jobs.py:1521} DagFileProcessor314 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:24,382] {models.py:167} DagFileProcessor314 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:24,494] {jobs.py:1535} DagFileProcessor314 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:24,519] {models.py:322} DagFileProcessor314 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:24,520] {models.py:328} DagFileProcessor314 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:24.520153
[2018-04-19 21:09:24,524] {jobs.py:351} DagFileProcessor314 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:09:25,606] {jobs.py:343} DagFileProcessor315 INFO - Started process (PID=3855) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:25,611] {jobs.py:534} DagFileProcessor315 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:25,613] {jobs.py:1521} DagFileProcessor315 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:25,613] {models.py:167} DagFileProcessor315 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:25,721] {jobs.py:1535} DagFileProcessor315 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:25,742] {models.py:322} DagFileProcessor315 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:25,743] {models.py:328} DagFileProcessor315 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:25.743041
[2018-04-19 21:09:25,747] {jobs.py:351} DagFileProcessor315 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:09:26,823] {jobs.py:343} DagFileProcessor316 INFO - Started process (PID=3856) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:26,828] {jobs.py:534} DagFileProcessor316 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:26,829] {jobs.py:1521} DagFileProcessor316 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:26,829] {models.py:167} DagFileProcessor316 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:26,937] {jobs.py:1535} DagFileProcessor316 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:26,957] {jobs.py:1169} DagFileProcessor316 INFO - Processing hello_world
[2018-04-19 21:09:26,985] {jobs.py:1173} DagFileProcessor316 INFO - Created <DagRun hello_world @ 2018-04-18 12:00:00: scheduled__2018-04-18T12:00:00, externally triggered: False>
[2018-04-19 21:09:26,987] {jobs.py:860} DagFileProcessor316 INFO - Examining DAG run <DagRun hello_world @ 2018-04-18 12:00:00: scheduled__2018-04-18T12:00:00, externally triggered: False>
[2018-04-19 21:09:26,991] {models.py:4024} DagFileProcessor316 INFO - Updating state for <DagRun hello_world @ 2018-04-18 12:00:00: scheduled__2018-04-18T12:00:00, externally triggered: False> considering 2 task(s)
/Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/venv_hello_world_workflow/lib/python3.6/site-packages/airflow/ti_deps/deps/base_ti_dep.py:94: DeprecationWarning: generator '_get_dep_statuses' raised StopIteration
  for dep_status in self._get_dep_statuses(ti, session, dep_context):
[2018-04-19 21:09:27,004] {jobs.py:566} DagFileProcessor316 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
/Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/venv_hello_world_workflow/lib/python3.6/site-packages/airflow/models.py:1140: DeprecationWarning: generator 'get_dep_statuses' raised StopIteration
  dep_context):
[2018-04-19 21:09:27,012] {models.py:1126} DagFileProcessor316 INFO - Dependencies all met for <TaskInstance: hello_world.dummy_task 2018-04-18 12:00:00 [None]>
[2018-04-19 21:09:27,013] {jobs.py:1608} DagFileProcessor316 INFO - Creating / updating <TaskInstance: hello_world.dummy_task 2018-04-18 12:00:00 [scheduled]> in ORM
[2018-04-19 21:09:27,018] {models.py:322} DagFileProcessor316 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:27,018] {models.py:328} DagFileProcessor316 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:27.018383
[2018-04-19 21:09:27,021] {jobs.py:351} DagFileProcessor316 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.198 seconds
[2018-04-19 21:09:28,138] {jobs.py:343} DagFileProcessor317 INFO - Started process (PID=3857) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:28,143] {jobs.py:534} DagFileProcessor317 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:28,144] {jobs.py:1521} DagFileProcessor317 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:28,144] {models.py:167} DagFileProcessor317 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:28,250] {jobs.py:1535} DagFileProcessor317 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:28,268] {jobs.py:1169} DagFileProcessor317 INFO - Processing hello_world
[2018-04-19 21:09:28,277] {jobs.py:860} DagFileProcessor317 INFO - Examining DAG run <DagRun hello_world @ 2018-04-18 12:00:00: scheduled__2018-04-18T12:00:00, externally triggered: False>
[2018-04-19 21:09:28,282] {models.py:4024} DagFileProcessor317 INFO - Updating state for <DagRun hello_world @ 2018-04-18 12:00:00: scheduled__2018-04-18T12:00:00, externally triggered: False> considering 2 task(s)
/Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/venv_hello_world_workflow/lib/python3.6/site-packages/airflow/ti_deps/deps/base_ti_dep.py:94: DeprecationWarning: generator '_get_dep_statuses' raised StopIteration
  for dep_status in self._get_dep_statuses(ti, session, dep_context):
[2018-04-19 21:09:28,295] {jobs.py:566} DagFileProcessor317 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:09:28,300] {models.py:322} DagFileProcessor317 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:28,301] {models.py:328} DagFileProcessor317 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:28.300885
[2018-04-19 21:09:28,303] {jobs.py:351} DagFileProcessor317 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.165 seconds
[2018-04-19 21:09:35,207] {jobs.py:343} DagFileProcessor318 INFO - Started process (PID=3861) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:35,212] {jobs.py:534} DagFileProcessor318 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:35,213] {jobs.py:1521} DagFileProcessor318 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:35,214] {models.py:167} DagFileProcessor318 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:35,320] {jobs.py:1535} DagFileProcessor318 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:35,339] {jobs.py:1169} DagFileProcessor318 INFO - Processing hello_world
[2018-04-19 21:09:35,351] {jobs.py:860} DagFileProcessor318 INFO - Examining DAG run <DagRun hello_world @ 2018-04-18 12:00:00: scheduled__2018-04-18T12:00:00, externally triggered: False>
[2018-04-19 21:09:35,355] {models.py:4024} DagFileProcessor318 INFO - Updating state for <DagRun hello_world @ 2018-04-18 12:00:00: scheduled__2018-04-18T12:00:00, externally triggered: False> considering 2 task(s)
/Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/venv_hello_world_workflow/lib/python3.6/site-packages/airflow/ti_deps/deps/base_ti_dep.py:94: DeprecationWarning: generator '_get_dep_statuses' raised StopIteration
  for dep_status in self._get_dep_statuses(ti, session, dep_context):
[2018-04-19 21:09:35,364] {jobs.py:860} DagFileProcessor318 INFO - Examining DAG run <DagRun hello_world @ 2018-04-19 21:09:28.712532: manual__2018-04-19T21:09:28.712532, externally triggered: True>
[2018-04-19 21:09:35,368] {models.py:4024} DagFileProcessor318 INFO - Updating state for <DagRun hello_world @ 2018-04-19 21:09:28.712532: manual__2018-04-19T21:09:28.712532, externally triggered: True> considering 2 task(s)
[2018-04-19 21:09:35,382] {jobs.py:566} DagFileProcessor318 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
/Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/venv_hello_world_workflow/lib/python3.6/site-packages/airflow/models.py:1140: DeprecationWarning: generator 'get_dep_statuses' raised StopIteration
  dep_context):
[2018-04-19 21:09:35,395] {models.py:1126} DagFileProcessor318 INFO - Dependencies all met for <TaskInstance: hello_world.hello_task 2018-04-18 12:00:00 [None]>
[2018-04-19 21:09:35,396] {jobs.py:1608} DagFileProcessor318 INFO - Creating / updating <TaskInstance: hello_world.hello_task 2018-04-18 12:00:00 [scheduled]> in ORM
[2018-04-19 21:09:35,401] {models.py:1126} DagFileProcessor318 INFO - Dependencies all met for <TaskInstance: hello_world.dummy_task 2018-04-19 21:09:28.712532 [None]>
[2018-04-19 21:09:35,401] {jobs.py:1608} DagFileProcessor318 INFO - Creating / updating <TaskInstance: hello_world.dummy_task 2018-04-19 21:09:28.712532 [scheduled]> in ORM
[2018-04-19 21:09:35,405] {models.py:322} DagFileProcessor318 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:35,406] {models.py:328} DagFileProcessor318 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:35.405822
[2018-04-19 21:09:35,408] {jobs.py:351} DagFileProcessor318 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.202 seconds
[2018-04-19 21:09:48,129] {jobs.py:343} DagFileProcessor319 INFO - Started process (PID=3867) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:48,134] {jobs.py:534} DagFileProcessor319 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:48,135] {jobs.py:1521} DagFileProcessor319 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:48,136] {models.py:167} DagFileProcessor319 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:48,237] {jobs.py:1535} DagFileProcessor319 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:48,259] {jobs.py:1169} DagFileProcessor319 INFO - Processing hello_world
[2018-04-19 21:09:48,268] {jobs.py:860} DagFileProcessor319 INFO - Examining DAG run <DagRun hello_world @ 2018-04-18 12:00:00: scheduled__2018-04-18T12:00:00, externally triggered: False>
[2018-04-19 21:09:48,272] {models.py:4024} DagFileProcessor319 INFO - Updating state for <DagRun hello_world @ 2018-04-18 12:00:00: scheduled__2018-04-18T12:00:00, externally triggered: False> considering 2 task(s)
[2018-04-19 21:09:48,274] {models.py:4070} DagFileProcessor319 INFO - Marking run <DagRun hello_world @ 2018-04-18 12:00:00: scheduled__2018-04-18T12:00:00, externally triggered: False> successful
[2018-04-19 21:09:48,280] {jobs.py:860} DagFileProcessor319 INFO - Examining DAG run <DagRun hello_world @ 2018-04-19 21:09:28.712532: manual__2018-04-19T21:09:28.712532, externally triggered: True>
[2018-04-19 21:09:48,283] {models.py:4024} DagFileProcessor319 INFO - Updating state for <DagRun hello_world @ 2018-04-19 21:09:28.712532: manual__2018-04-19T21:09:28.712532, externally triggered: True> considering 2 task(s)
/Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/venv_hello_world_workflow/lib/python3.6/site-packages/airflow/ti_deps/deps/base_ti_dep.py:94: DeprecationWarning: generator '_get_dep_statuses' raised StopIteration
  for dep_status in self._get_dep_statuses(ti, session, dep_context):
[2018-04-19 21:09:48,294] {jobs.py:566} DagFileProcessor319 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
/Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/venv_hello_world_workflow/lib/python3.6/site-packages/airflow/models.py:1140: DeprecationWarning: generator 'get_dep_statuses' raised StopIteration
  dep_context):
[2018-04-19 21:09:48,304] {models.py:1126} DagFileProcessor319 INFO - Dependencies all met for <TaskInstance: hello_world.hello_task 2018-04-19 21:09:28.712532 [None]>
[2018-04-19 21:09:48,305] {jobs.py:1608} DagFileProcessor319 INFO - Creating / updating <TaskInstance: hello_world.hello_task 2018-04-19 21:09:28.712532 [scheduled]> in ORM
[2018-04-19 21:09:48,310] {models.py:322} DagFileProcessor319 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:48,310] {models.py:328} DagFileProcessor319 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:48.310770
[2018-04-19 21:09:48,314] {jobs.py:351} DagFileProcessor319 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.185 seconds
[2018-04-19 21:09:55,138] {jobs.py:343} DagFileProcessor320 INFO - Started process (PID=3871) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:55,143] {jobs.py:534} DagFileProcessor320 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:55,145] {jobs.py:1521} DagFileProcessor320 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:55,146] {models.py:167} DagFileProcessor320 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:55,243] {jobs.py:1535} DagFileProcessor320 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:55,262] {jobs.py:1169} DagFileProcessor320 INFO - Processing hello_world
[2018-04-19 21:09:55,271] {jobs.py:860} DagFileProcessor320 INFO - Examining DAG run <DagRun hello_world @ 2018-04-19 21:09:28.712532: manual__2018-04-19T21:09:28.712532, externally triggered: True>
[2018-04-19 21:09:55,275] {models.py:4024} DagFileProcessor320 INFO - Updating state for <DagRun hello_world @ 2018-04-19 21:09:28.712532: manual__2018-04-19T21:09:28.712532, externally triggered: True> considering 2 task(s)
[2018-04-19 21:09:55,277] {models.py:4070} DagFileProcessor320 INFO - Marking run <DagRun hello_world @ 2018-04-19 21:09:28.712532: manual__2018-04-19T21:09:28.712532, externally triggered: True> successful
[2018-04-19 21:09:55,283] {jobs.py:566} DagFileProcessor320 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:09:55,291] {models.py:322} DagFileProcessor320 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:55,292] {models.py:328} DagFileProcessor320 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:55.292031
[2018-04-19 21:09:55,295] {jobs.py:351} DagFileProcessor320 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:09:56,372] {jobs.py:343} DagFileProcessor321 INFO - Started process (PID=3873) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:56,378] {jobs.py:534} DagFileProcessor321 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:56,379] {jobs.py:1521} DagFileProcessor321 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:56,380] {models.py:167} DagFileProcessor321 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:56,497] {jobs.py:1535} DagFileProcessor321 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:56,520] {jobs.py:1169} DagFileProcessor321 INFO - Processing hello_world
[2018-04-19 21:09:56,529] {jobs.py:566} DagFileProcessor321 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:09:56,535] {models.py:322} DagFileProcessor321 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:56,535] {models.py:328} DagFileProcessor321 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:56.535349
[2018-04-19 21:09:56,538] {jobs.py:351} DagFileProcessor321 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.167 seconds
[2018-04-19 21:09:57,599] {jobs.py:343} DagFileProcessor322 INFO - Started process (PID=3880) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:57,604] {jobs.py:534} DagFileProcessor322 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:57,605] {jobs.py:1521} DagFileProcessor322 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:57,605] {models.py:167} DagFileProcessor322 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:57,712] {jobs.py:1535} DagFileProcessor322 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:57,729] {jobs.py:1169} DagFileProcessor322 INFO - Processing hello_world
[2018-04-19 21:09:57,738] {jobs.py:566} DagFileProcessor322 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:09:57,743] {models.py:322} DagFileProcessor322 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:57,744] {models.py:328} DagFileProcessor322 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:57.744190
[2018-04-19 21:09:57,747] {jobs.py:351} DagFileProcessor322 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:09:58,833] {jobs.py:343} DagFileProcessor323 INFO - Started process (PID=3881) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:58,839] {jobs.py:534} DagFileProcessor323 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:09:58,844] {jobs.py:1521} DagFileProcessor323 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:09:58,845] {models.py:167} DagFileProcessor323 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:58,950] {jobs.py:1535} DagFileProcessor323 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:09:58,971] {jobs.py:1169} DagFileProcessor323 INFO - Processing hello_world
[2018-04-19 21:09:58,978] {jobs.py:566} DagFileProcessor323 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:09:58,984] {models.py:322} DagFileProcessor323 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:09:58,984] {models.py:328} DagFileProcessor323 INFO - Failing jobs without heartbeat after 2018-04-19 21:04:58.984449
[2018-04-19 21:09:58,987] {jobs.py:351} DagFileProcessor323 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.155 seconds
[2018-04-19 21:10:00,055] {jobs.py:343} DagFileProcessor324 INFO - Started process (PID=3882) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:00,061] {jobs.py:534} DagFileProcessor324 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:00,062] {jobs.py:1521} DagFileProcessor324 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:00,062] {models.py:167} DagFileProcessor324 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:00,170] {jobs.py:1535} DagFileProcessor324 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:00,189] {jobs.py:1169} DagFileProcessor324 INFO - Processing hello_world
[2018-04-19 21:10:00,198] {jobs.py:566} DagFileProcessor324 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:00,203] {models.py:322} DagFileProcessor324 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:00,203] {models.py:328} DagFileProcessor324 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:00.203780
[2018-04-19 21:10:00,207] {jobs.py:351} DagFileProcessor324 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:10:01,289] {jobs.py:343} DagFileProcessor325 INFO - Started process (PID=3883) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:01,294] {jobs.py:534} DagFileProcessor325 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:01,295] {jobs.py:1521} DagFileProcessor325 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:01,296] {models.py:167} DagFileProcessor325 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:01,401] {jobs.py:1535} DagFileProcessor325 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:01,421] {jobs.py:1169} DagFileProcessor325 INFO - Processing hello_world
[2018-04-19 21:10:01,429] {jobs.py:566} DagFileProcessor325 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:01,434] {models.py:322} DagFileProcessor325 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:01,435] {models.py:328} DagFileProcessor325 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:01.434901
[2018-04-19 21:10:01,438] {jobs.py:351} DagFileProcessor325 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:10:02,511] {jobs.py:343} DagFileProcessor326 INFO - Started process (PID=3884) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:02,516] {jobs.py:534} DagFileProcessor326 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:02,517] {jobs.py:1521} DagFileProcessor326 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:02,518] {models.py:167} DagFileProcessor326 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:02,632] {jobs.py:1535} DagFileProcessor326 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:02,653] {jobs.py:1169} DagFileProcessor326 INFO - Processing hello_world
[2018-04-19 21:10:02,661] {jobs.py:566} DagFileProcessor326 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:02,667] {models.py:322} DagFileProcessor326 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:02,667] {models.py:328} DagFileProcessor326 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:02.667672
[2018-04-19 21:10:02,671] {jobs.py:351} DagFileProcessor326 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:10:03,744] {jobs.py:343} DagFileProcessor327 INFO - Started process (PID=3886) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:03,749] {jobs.py:534} DagFileProcessor327 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:03,750] {jobs.py:1521} DagFileProcessor327 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:03,750] {models.py:167} DagFileProcessor327 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:03,858] {jobs.py:1535} DagFileProcessor327 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:03,879] {jobs.py:1169} DagFileProcessor327 INFO - Processing hello_world
[2018-04-19 21:10:03,888] {jobs.py:566} DagFileProcessor327 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:03,893] {models.py:322} DagFileProcessor327 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:03,894] {models.py:328} DagFileProcessor327 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:03.894115
[2018-04-19 21:10:03,897] {jobs.py:351} DagFileProcessor327 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:10:04,971] {jobs.py:343} DagFileProcessor328 INFO - Started process (PID=3887) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:04,976] {jobs.py:534} DagFileProcessor328 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:04,977] {jobs.py:1521} DagFileProcessor328 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:04,978] {models.py:167} DagFileProcessor328 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:05,080] {jobs.py:1535} DagFileProcessor328 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:05,099] {jobs.py:1169} DagFileProcessor328 INFO - Processing hello_world
[2018-04-19 21:10:05,109] {jobs.py:566} DagFileProcessor328 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:05,115] {models.py:322} DagFileProcessor328 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:05,116] {models.py:328} DagFileProcessor328 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:05.115913
[2018-04-19 21:10:05,119] {jobs.py:351} DagFileProcessor328 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:10:06,195] {jobs.py:343} DagFileProcessor329 INFO - Started process (PID=3888) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:06,200] {jobs.py:534} DagFileProcessor329 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:06,202] {jobs.py:1521} DagFileProcessor329 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:06,202] {models.py:167} DagFileProcessor329 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:06,304] {jobs.py:1535} DagFileProcessor329 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:06,322] {jobs.py:1169} DagFileProcessor329 INFO - Processing hello_world
[2018-04-19 21:10:06,331] {jobs.py:566} DagFileProcessor329 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:06,336] {models.py:322} DagFileProcessor329 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:06,336] {models.py:328} DagFileProcessor329 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:06.336631
[2018-04-19 21:10:06,340] {jobs.py:351} DagFileProcessor329 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:10:07,427] {jobs.py:343} DagFileProcessor330 INFO - Started process (PID=3889) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:07,432] {jobs.py:534} DagFileProcessor330 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:07,433] {jobs.py:1521} DagFileProcessor330 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:07,433] {models.py:167} DagFileProcessor330 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:07,534] {jobs.py:1535} DagFileProcessor330 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:07,553] {jobs.py:1169} DagFileProcessor330 INFO - Processing hello_world
[2018-04-19 21:10:07,562] {jobs.py:566} DagFileProcessor330 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:07,568] {models.py:322} DagFileProcessor330 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:07,568] {models.py:328} DagFileProcessor330 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:07.568570
[2018-04-19 21:10:07,571] {jobs.py:351} DagFileProcessor330 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:10:08,653] {jobs.py:343} DagFileProcessor331 INFO - Started process (PID=3890) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:08,658] {jobs.py:534} DagFileProcessor331 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:08,659] {jobs.py:1521} DagFileProcessor331 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:08,660] {models.py:167} DagFileProcessor331 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:08,761] {jobs.py:1535} DagFileProcessor331 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:08,779] {jobs.py:1169} DagFileProcessor331 INFO - Processing hello_world
[2018-04-19 21:10:08,788] {jobs.py:566} DagFileProcessor331 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:08,793] {models.py:322} DagFileProcessor331 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:08,794] {models.py:328} DagFileProcessor331 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:08.793845
[2018-04-19 21:10:08,797] {jobs.py:351} DagFileProcessor331 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:10:09,890] {jobs.py:343} DagFileProcessor332 INFO - Started process (PID=3891) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:09,897] {jobs.py:534} DagFileProcessor332 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:09,898] {jobs.py:1521} DagFileProcessor332 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:09,899] {models.py:167} DagFileProcessor332 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:10,012] {jobs.py:1535} DagFileProcessor332 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:10,031] {jobs.py:1169} DagFileProcessor332 INFO - Processing hello_world
[2018-04-19 21:10:10,042] {jobs.py:566} DagFileProcessor332 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:10,047] {models.py:322} DagFileProcessor332 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:10,048] {models.py:328} DagFileProcessor332 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:10.047926
[2018-04-19 21:10:10,051] {jobs.py:351} DagFileProcessor332 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.161 seconds
[2018-04-19 21:10:11,117] {jobs.py:343} DagFileProcessor333 INFO - Started process (PID=3892) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:11,123] {jobs.py:534} DagFileProcessor333 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:11,124] {jobs.py:1521} DagFileProcessor333 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:11,125] {models.py:167} DagFileProcessor333 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:11,235] {jobs.py:1535} DagFileProcessor333 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:11,256] {jobs.py:1169} DagFileProcessor333 INFO - Processing hello_world
[2018-04-19 21:10:11,264] {jobs.py:566} DagFileProcessor333 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:11,269] {models.py:322} DagFileProcessor333 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:11,270] {models.py:328} DagFileProcessor333 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:11.270176
[2018-04-19 21:10:11,273] {jobs.py:351} DagFileProcessor333 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:10:12,344] {jobs.py:343} DagFileProcessor334 INFO - Started process (PID=3893) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:12,349] {jobs.py:534} DagFileProcessor334 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:12,350] {jobs.py:1521} DagFileProcessor334 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:12,351] {models.py:167} DagFileProcessor334 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:12,461] {jobs.py:1535} DagFileProcessor334 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:12,479] {jobs.py:1169} DagFileProcessor334 INFO - Processing hello_world
[2018-04-19 21:10:12,488] {jobs.py:566} DagFileProcessor334 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:12,494] {models.py:322} DagFileProcessor334 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:12,494] {models.py:328} DagFileProcessor334 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:12.494547
[2018-04-19 21:10:12,498] {jobs.py:351} DagFileProcessor334 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:10:13,578] {jobs.py:343} DagFileProcessor335 INFO - Started process (PID=3895) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:13,583] {jobs.py:534} DagFileProcessor335 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:13,584] {jobs.py:1521} DagFileProcessor335 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:13,584] {models.py:167} DagFileProcessor335 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:13,683] {jobs.py:1535} DagFileProcessor335 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:13,701] {jobs.py:1169} DagFileProcessor335 INFO - Processing hello_world
[2018-04-19 21:10:13,709] {jobs.py:566} DagFileProcessor335 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:13,714] {models.py:322} DagFileProcessor335 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:13,715] {models.py:328} DagFileProcessor335 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:13.714878
[2018-04-19 21:10:13,718] {jobs.py:351} DagFileProcessor335 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.140 seconds
[2018-04-19 21:10:14,809] {jobs.py:343} DagFileProcessor336 INFO - Started process (PID=3896) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:14,813] {jobs.py:534} DagFileProcessor336 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:14,814] {jobs.py:1521} DagFileProcessor336 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:14,815] {models.py:167} DagFileProcessor336 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:14,921] {jobs.py:1535} DagFileProcessor336 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:14,942] {jobs.py:1169} DagFileProcessor336 INFO - Processing hello_world
[2018-04-19 21:10:14,951] {jobs.py:566} DagFileProcessor336 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:14,956] {models.py:322} DagFileProcessor336 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:14,957] {models.py:328} DagFileProcessor336 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:14.956970
[2018-04-19 21:10:14,960] {jobs.py:351} DagFileProcessor336 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:10:16,034] {jobs.py:343} DagFileProcessor337 INFO - Started process (PID=3897) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:16,039] {jobs.py:534} DagFileProcessor337 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:16,040] {jobs.py:1521} DagFileProcessor337 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:16,040] {models.py:167} DagFileProcessor337 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:16,143] {jobs.py:1535} DagFileProcessor337 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:16,163] {jobs.py:1169} DagFileProcessor337 INFO - Processing hello_world
[2018-04-19 21:10:16,171] {jobs.py:566} DagFileProcessor337 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:16,176] {models.py:322} DagFileProcessor337 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:16,177] {models.py:328} DagFileProcessor337 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:16.176829
[2018-04-19 21:10:16,180] {jobs.py:351} DagFileProcessor337 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:10:17,259] {jobs.py:343} DagFileProcessor338 INFO - Started process (PID=3898) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:17,264] {jobs.py:534} DagFileProcessor338 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:17,265] {jobs.py:1521} DagFileProcessor338 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:17,265] {models.py:167} DagFileProcessor338 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:17,366] {jobs.py:1535} DagFileProcessor338 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:17,385] {jobs.py:1169} DagFileProcessor338 INFO - Processing hello_world
[2018-04-19 21:10:17,393] {jobs.py:566} DagFileProcessor338 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:17,398] {models.py:322} DagFileProcessor338 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:17,399] {models.py:328} DagFileProcessor338 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:17.399024
[2018-04-19 21:10:17,402] {jobs.py:351} DagFileProcessor338 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:10:18,489] {jobs.py:343} DagFileProcessor339 INFO - Started process (PID=3899) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:18,494] {jobs.py:534} DagFileProcessor339 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:18,495] {jobs.py:1521} DagFileProcessor339 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:18,495] {models.py:167} DagFileProcessor339 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:18,597] {jobs.py:1535} DagFileProcessor339 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:18,616] {jobs.py:1169} DagFileProcessor339 INFO - Processing hello_world
[2018-04-19 21:10:18,625] {jobs.py:566} DagFileProcessor339 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:18,631] {models.py:322} DagFileProcessor339 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:18,631] {models.py:328} DagFileProcessor339 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:18.631576
[2018-04-19 21:10:18,635] {jobs.py:351} DagFileProcessor339 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:10:19,723] {jobs.py:343} DagFileProcessor340 INFO - Started process (PID=3900) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:19,728] {jobs.py:534} DagFileProcessor340 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:19,729] {jobs.py:1521} DagFileProcessor340 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:19,729] {models.py:167} DagFileProcessor340 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:19,845] {jobs.py:1535} DagFileProcessor340 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:19,864] {jobs.py:1169} DagFileProcessor340 INFO - Processing hello_world
[2018-04-19 21:10:19,873] {jobs.py:566} DagFileProcessor340 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:19,878] {models.py:322} DagFileProcessor340 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:19,879] {models.py:328} DagFileProcessor340 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:19.878868
[2018-04-19 21:10:19,882] {jobs.py:351} DagFileProcessor340 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:10:20,951] {jobs.py:343} DagFileProcessor341 INFO - Started process (PID=3901) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:20,956] {jobs.py:534} DagFileProcessor341 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:20,957] {jobs.py:1521} DagFileProcessor341 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:20,958] {models.py:167} DagFileProcessor341 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:21,066] {jobs.py:1535} DagFileProcessor341 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:21,085] {jobs.py:1169} DagFileProcessor341 INFO - Processing hello_world
[2018-04-19 21:10:21,094] {jobs.py:566} DagFileProcessor341 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:21,099] {models.py:322} DagFileProcessor341 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:21,100] {models.py:328} DagFileProcessor341 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:21.100151
[2018-04-19 21:10:21,104] {jobs.py:351} DagFileProcessor341 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:10:22,182] {jobs.py:343} DagFileProcessor342 INFO - Started process (PID=3902) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:22,187] {jobs.py:534} DagFileProcessor342 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:22,188] {jobs.py:1521} DagFileProcessor342 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:22,188] {models.py:167} DagFileProcessor342 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:22,292] {jobs.py:1535} DagFileProcessor342 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:22,313] {jobs.py:1169} DagFileProcessor342 INFO - Processing hello_world
[2018-04-19 21:10:22,322] {jobs.py:566} DagFileProcessor342 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:22,329] {models.py:322} DagFileProcessor342 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:22,330] {models.py:328} DagFileProcessor342 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:22.330139
[2018-04-19 21:10:22,333] {jobs.py:351} DagFileProcessor342 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:10:23,414] {jobs.py:343} DagFileProcessor343 INFO - Started process (PID=3904) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:23,419] {jobs.py:534} DagFileProcessor343 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:23,421] {jobs.py:1521} DagFileProcessor343 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:23,421] {models.py:167} DagFileProcessor343 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:23,532] {jobs.py:1535} DagFileProcessor343 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:23,551] {jobs.py:1169} DagFileProcessor343 INFO - Processing hello_world
[2018-04-19 21:10:23,562] {jobs.py:566} DagFileProcessor343 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:23,567] {models.py:322} DagFileProcessor343 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:23,568] {models.py:328} DagFileProcessor343 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:23.568038
[2018-04-19 21:10:23,571] {jobs.py:351} DagFileProcessor343 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:10:24,640] {jobs.py:343} DagFileProcessor344 INFO - Started process (PID=3906) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:24,645] {jobs.py:534} DagFileProcessor344 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:24,646] {jobs.py:1521} DagFileProcessor344 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:24,647] {models.py:167} DagFileProcessor344 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:24,750] {jobs.py:1535} DagFileProcessor344 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:24,772] {jobs.py:1169} DagFileProcessor344 INFO - Processing hello_world
[2018-04-19 21:10:24,780] {jobs.py:566} DagFileProcessor344 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:24,785] {models.py:322} DagFileProcessor344 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:24,786] {models.py:328} DagFileProcessor344 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:24.786106
[2018-04-19 21:10:24,789] {jobs.py:351} DagFileProcessor344 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:10:25,874] {jobs.py:343} DagFileProcessor345 INFO - Started process (PID=3907) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:25,878] {jobs.py:534} DagFileProcessor345 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:25,880] {jobs.py:1521} DagFileProcessor345 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:25,880] {models.py:167} DagFileProcessor345 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:25,985] {jobs.py:1535} DagFileProcessor345 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:26,006] {jobs.py:1169} DagFileProcessor345 INFO - Processing hello_world
[2018-04-19 21:10:26,015] {jobs.py:566} DagFileProcessor345 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:26,023] {models.py:322} DagFileProcessor345 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:26,023] {models.py:328} DagFileProcessor345 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:26.023686
[2018-04-19 21:10:26,027] {jobs.py:351} DagFileProcessor345 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:10:27,100] {jobs.py:343} DagFileProcessor346 INFO - Started process (PID=3908) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:27,110] {jobs.py:534} DagFileProcessor346 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:27,111] {jobs.py:1521} DagFileProcessor346 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:27,112] {models.py:167} DagFileProcessor346 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:27,219] {jobs.py:1535} DagFileProcessor346 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:27,239] {jobs.py:1169} DagFileProcessor346 INFO - Processing hello_world
[2018-04-19 21:10:27,248] {jobs.py:566} DagFileProcessor346 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:27,254] {models.py:322} DagFileProcessor346 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:27,254] {models.py:328} DagFileProcessor346 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:27.254351
[2018-04-19 21:10:27,258] {jobs.py:351} DagFileProcessor346 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.158 seconds
[2018-04-19 21:10:28,326] {jobs.py:343} DagFileProcessor347 INFO - Started process (PID=3909) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:28,331] {jobs.py:534} DagFileProcessor347 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:28,332] {jobs.py:1521} DagFileProcessor347 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:28,332] {models.py:167} DagFileProcessor347 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:28,445] {jobs.py:1535} DagFileProcessor347 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:28,465] {jobs.py:1169} DagFileProcessor347 INFO - Processing hello_world
[2018-04-19 21:10:28,473] {jobs.py:566} DagFileProcessor347 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:28,481] {models.py:322} DagFileProcessor347 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:28,482] {models.py:328} DagFileProcessor347 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:28.482001
[2018-04-19 21:10:28,486] {jobs.py:351} DagFileProcessor347 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.160 seconds
[2018-04-19 21:10:29,552] {jobs.py:343} DagFileProcessor348 INFO - Started process (PID=3910) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:29,561] {jobs.py:534} DagFileProcessor348 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:29,562] {jobs.py:1521} DagFileProcessor348 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:29,562] {models.py:167} DagFileProcessor348 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:29,665] {jobs.py:1535} DagFileProcessor348 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:29,683] {jobs.py:1169} DagFileProcessor348 INFO - Processing hello_world
[2018-04-19 21:10:29,692] {jobs.py:566} DagFileProcessor348 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:29,698] {models.py:322} DagFileProcessor348 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:29,698] {models.py:328} DagFileProcessor348 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:29.698667
[2018-04-19 21:10:29,702] {jobs.py:351} DagFileProcessor348 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:10:30,786] {jobs.py:343} DagFileProcessor349 INFO - Started process (PID=3911) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:30,791] {jobs.py:534} DagFileProcessor349 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:30,792] {jobs.py:1521} DagFileProcessor349 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:30,793] {models.py:167} DagFileProcessor349 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:30,899] {jobs.py:1535} DagFileProcessor349 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:30,921] {jobs.py:1169} DagFileProcessor349 INFO - Processing hello_world
[2018-04-19 21:10:30,931] {jobs.py:566} DagFileProcessor349 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:30,936] {models.py:322} DagFileProcessor349 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:30,937] {models.py:328} DagFileProcessor349 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:30.937283
[2018-04-19 21:10:30,940] {jobs.py:351} DagFileProcessor349 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.155 seconds
[2018-04-19 21:10:32,018] {jobs.py:343} DagFileProcessor350 INFO - Started process (PID=3912) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:32,023] {jobs.py:534} DagFileProcessor350 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:32,024] {jobs.py:1521} DagFileProcessor350 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:32,024] {models.py:167} DagFileProcessor350 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:32,131] {jobs.py:1535} DagFileProcessor350 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:32,152] {jobs.py:1169} DagFileProcessor350 INFO - Processing hello_world
[2018-04-19 21:10:32,160] {jobs.py:566} DagFileProcessor350 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:32,166] {models.py:322} DagFileProcessor350 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:32,167] {models.py:328} DagFileProcessor350 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:32.167095
[2018-04-19 21:10:32,170] {jobs.py:351} DagFileProcessor350 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:10:33,241] {jobs.py:343} DagFileProcessor351 INFO - Started process (PID=3914) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:33,245] {jobs.py:534} DagFileProcessor351 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:33,246] {jobs.py:1521} DagFileProcessor351 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:33,247] {models.py:167} DagFileProcessor351 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:33,347] {jobs.py:1535} DagFileProcessor351 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:33,366] {jobs.py:1169} DagFileProcessor351 INFO - Processing hello_world
[2018-04-19 21:10:33,374] {jobs.py:566} DagFileProcessor351 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:33,379] {models.py:322} DagFileProcessor351 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:33,380] {models.py:328} DagFileProcessor351 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:33.380182
[2018-04-19 21:10:33,383] {jobs.py:351} DagFileProcessor351 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:10:34,468] {jobs.py:343} DagFileProcessor352 INFO - Started process (PID=3915) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:34,473] {jobs.py:534} DagFileProcessor352 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:34,474] {jobs.py:1521} DagFileProcessor352 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:34,474] {models.py:167} DagFileProcessor352 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:34,576] {jobs.py:1535} DagFileProcessor352 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:34,596] {jobs.py:1169} DagFileProcessor352 INFO - Processing hello_world
[2018-04-19 21:10:34,604] {jobs.py:566} DagFileProcessor352 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:34,609] {models.py:322} DagFileProcessor352 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:34,610] {models.py:328} DagFileProcessor352 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:34.610242
[2018-04-19 21:10:34,613] {jobs.py:351} DagFileProcessor352 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:10:35,701] {jobs.py:343} DagFileProcessor353 INFO - Started process (PID=3916) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:35,706] {jobs.py:534} DagFileProcessor353 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:35,708] {jobs.py:1521} DagFileProcessor353 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:35,709] {models.py:167} DagFileProcessor353 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:35,818] {jobs.py:1535} DagFileProcessor353 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:35,837] {jobs.py:1169} DagFileProcessor353 INFO - Processing hello_world
[2018-04-19 21:10:35,845] {jobs.py:566} DagFileProcessor353 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:35,851] {models.py:322} DagFileProcessor353 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:35,852] {models.py:328} DagFileProcessor353 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:35.852171
[2018-04-19 21:10:35,855] {jobs.py:351} DagFileProcessor353 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:10:36,931] {jobs.py:343} DagFileProcessor354 INFO - Started process (PID=3917) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:36,936] {jobs.py:534} DagFileProcessor354 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:36,937] {jobs.py:1521} DagFileProcessor354 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:36,937] {models.py:167} DagFileProcessor354 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:37,042] {jobs.py:1535} DagFileProcessor354 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:37,064] {jobs.py:1169} DagFileProcessor354 INFO - Processing hello_world
[2018-04-19 21:10:37,076] {jobs.py:566} DagFileProcessor354 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:37,083] {models.py:322} DagFileProcessor354 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:37,083] {models.py:328} DagFileProcessor354 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:37.083613
[2018-04-19 21:10:37,087] {jobs.py:351} DagFileProcessor354 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:10:38,168] {jobs.py:343} DagFileProcessor355 INFO - Started process (PID=3918) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:38,173] {jobs.py:534} DagFileProcessor355 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:38,174] {jobs.py:1521} DagFileProcessor355 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:38,175] {models.py:167} DagFileProcessor355 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:38,282] {jobs.py:1535} DagFileProcessor355 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:38,300] {jobs.py:1169} DagFileProcessor355 INFO - Processing hello_world
[2018-04-19 21:10:38,309] {jobs.py:566} DagFileProcessor355 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:38,314] {models.py:322} DagFileProcessor355 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:38,315] {models.py:328} DagFileProcessor355 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:38.315150
[2018-04-19 21:10:38,319] {jobs.py:351} DagFileProcessor355 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:10:39,399] {jobs.py:343} DagFileProcessor356 INFO - Started process (PID=3919) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:39,404] {jobs.py:534} DagFileProcessor356 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:39,405] {jobs.py:1521} DagFileProcessor356 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:39,405] {models.py:167} DagFileProcessor356 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:39,511] {jobs.py:1535} DagFileProcessor356 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:39,532] {jobs.py:1169} DagFileProcessor356 INFO - Processing hello_world
[2018-04-19 21:10:39,541] {jobs.py:566} DagFileProcessor356 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:39,546] {models.py:322} DagFileProcessor356 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:39,546] {models.py:328} DagFileProcessor356 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:39.546463
[2018-04-19 21:10:39,549] {jobs.py:351} DagFileProcessor356 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:10:40,625] {jobs.py:343} DagFileProcessor357 INFO - Started process (PID=3920) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:40,631] {jobs.py:534} DagFileProcessor357 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:40,632] {jobs.py:1521} DagFileProcessor357 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:40,632] {models.py:167} DagFileProcessor357 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:40,747] {jobs.py:1535} DagFileProcessor357 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:40,769] {jobs.py:1169} DagFileProcessor357 INFO - Processing hello_world
[2018-04-19 21:10:40,778] {jobs.py:566} DagFileProcessor357 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:40,783] {models.py:322} DagFileProcessor357 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:40,784] {models.py:328} DagFileProcessor357 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:40.784318
[2018-04-19 21:10:40,787] {jobs.py:351} DagFileProcessor357 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.162 seconds
[2018-04-19 21:10:41,850] {jobs.py:343} DagFileProcessor358 INFO - Started process (PID=3921) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:41,856] {jobs.py:534} DagFileProcessor358 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:41,857] {jobs.py:1521} DagFileProcessor358 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:41,857] {models.py:167} DagFileProcessor358 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:41,966] {jobs.py:1535} DagFileProcessor358 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:41,989] {jobs.py:1169} DagFileProcessor358 INFO - Processing hello_world
[2018-04-19 21:10:42,000] {jobs.py:566} DagFileProcessor358 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:42,007] {models.py:322} DagFileProcessor358 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:42,008] {models.py:328} DagFileProcessor358 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:42.007862
[2018-04-19 21:10:42,012] {jobs.py:351} DagFileProcessor358 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.162 seconds
[2018-04-19 21:10:43,072] {jobs.py:343} DagFileProcessor359 INFO - Started process (PID=3923) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:43,077] {jobs.py:534} DagFileProcessor359 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:43,078] {jobs.py:1521} DagFileProcessor359 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:43,078] {models.py:167} DagFileProcessor359 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:43,181] {jobs.py:1535} DagFileProcessor359 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:43,200] {jobs.py:1169} DagFileProcessor359 INFO - Processing hello_world
[2018-04-19 21:10:43,209] {jobs.py:566} DagFileProcessor359 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:43,214] {models.py:322} DagFileProcessor359 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:43,214] {models.py:328} DagFileProcessor359 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:43.214556
[2018-04-19 21:10:43,217] {jobs.py:351} DagFileProcessor359 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:10:44,306] {jobs.py:343} DagFileProcessor360 INFO - Started process (PID=3924) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:44,312] {jobs.py:534} DagFileProcessor360 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:44,314] {jobs.py:1521} DagFileProcessor360 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:44,315] {models.py:167} DagFileProcessor360 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:44,432] {jobs.py:1535} DagFileProcessor360 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:44,452] {jobs.py:1169} DagFileProcessor360 INFO - Processing hello_world
[2018-04-19 21:10:44,461] {jobs.py:566} DagFileProcessor360 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:44,467] {models.py:322} DagFileProcessor360 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:44,468] {models.py:328} DagFileProcessor360 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:44.467942
[2018-04-19 21:10:44,471] {jobs.py:351} DagFileProcessor360 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.165 seconds
[2018-04-19 21:10:45,540] {jobs.py:343} DagFileProcessor361 INFO - Started process (PID=3925) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:45,545] {jobs.py:534} DagFileProcessor361 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:45,546] {jobs.py:1521} DagFileProcessor361 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:45,546] {models.py:167} DagFileProcessor361 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:45,652] {jobs.py:1535} DagFileProcessor361 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:45,673] {jobs.py:1169} DagFileProcessor361 INFO - Processing hello_world
[2018-04-19 21:10:45,682] {jobs.py:566} DagFileProcessor361 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:45,688] {models.py:322} DagFileProcessor361 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:45,688] {models.py:328} DagFileProcessor361 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:45.688758
[2018-04-19 21:10:45,692] {jobs.py:351} DagFileProcessor361 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:10:46,768] {jobs.py:343} DagFileProcessor362 INFO - Started process (PID=3926) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:46,773] {jobs.py:534} DagFileProcessor362 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:46,774] {jobs.py:1521} DagFileProcessor362 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:46,774] {models.py:167} DagFileProcessor362 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:46,881] {jobs.py:1535} DagFileProcessor362 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:46,902] {jobs.py:1169} DagFileProcessor362 INFO - Processing hello_world
[2018-04-19 21:10:46,912] {jobs.py:566} DagFileProcessor362 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:46,919] {models.py:322} DagFileProcessor362 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:46,919] {models.py:328} DagFileProcessor362 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:46.919419
[2018-04-19 21:10:46,923] {jobs.py:351} DagFileProcessor362 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:10:47,994] {jobs.py:343} DagFileProcessor363 INFO - Started process (PID=3927) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:47,999] {jobs.py:534} DagFileProcessor363 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:48,000] {jobs.py:1521} DagFileProcessor363 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:48,001] {models.py:167} DagFileProcessor363 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:48,102] {jobs.py:1535} DagFileProcessor363 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:48,121] {jobs.py:1169} DagFileProcessor363 INFO - Processing hello_world
[2018-04-19 21:10:48,129] {jobs.py:566} DagFileProcessor363 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:48,135] {models.py:322} DagFileProcessor363 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:48,135] {models.py:328} DagFileProcessor363 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:48.135668
[2018-04-19 21:10:48,139] {jobs.py:351} DagFileProcessor363 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:10:49,223] {jobs.py:343} DagFileProcessor364 INFO - Started process (PID=3928) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:49,228] {jobs.py:534} DagFileProcessor364 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:49,229] {jobs.py:1521} DagFileProcessor364 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:49,229] {models.py:167} DagFileProcessor364 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:49,328] {jobs.py:1535} DagFileProcessor364 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:49,351] {jobs.py:1169} DagFileProcessor364 INFO - Processing hello_world
[2018-04-19 21:10:49,360] {jobs.py:566} DagFileProcessor364 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:49,366] {models.py:322} DagFileProcessor364 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:49,367] {models.py:328} DagFileProcessor364 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:49.366866
[2018-04-19 21:10:49,370] {jobs.py:351} DagFileProcessor364 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:10:50,460] {jobs.py:343} DagFileProcessor365 INFO - Started process (PID=3929) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:50,465] {jobs.py:534} DagFileProcessor365 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:50,466] {jobs.py:1521} DagFileProcessor365 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:50,467] {models.py:167} DagFileProcessor365 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:50,571] {jobs.py:1535} DagFileProcessor365 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:50,591] {jobs.py:1169} DagFileProcessor365 INFO - Processing hello_world
[2018-04-19 21:10:50,600] {jobs.py:566} DagFileProcessor365 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:50,606] {models.py:322} DagFileProcessor365 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:50,606] {models.py:328} DagFileProcessor365 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:50.606403
[2018-04-19 21:10:50,610] {jobs.py:351} DagFileProcessor365 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:10:51,682] {jobs.py:343} DagFileProcessor366 INFO - Started process (PID=3930) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:51,687] {jobs.py:534} DagFileProcessor366 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:51,688] {jobs.py:1521} DagFileProcessor366 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:51,688] {models.py:167} DagFileProcessor366 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:51,797] {jobs.py:1535} DagFileProcessor366 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:51,821] {jobs.py:1169} DagFileProcessor366 INFO - Processing hello_world
[2018-04-19 21:10:51,830] {jobs.py:566} DagFileProcessor366 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:51,836] {models.py:322} DagFileProcessor366 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:51,836] {models.py:328} DagFileProcessor366 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:51.836497
[2018-04-19 21:10:51,840] {jobs.py:351} DagFileProcessor366 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.158 seconds
[2018-04-19 21:10:52,903] {jobs.py:343} DagFileProcessor367 INFO - Started process (PID=3932) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:52,908] {jobs.py:534} DagFileProcessor367 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:52,909] {jobs.py:1521} DagFileProcessor367 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:52,909] {models.py:167} DagFileProcessor367 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:53,009] {jobs.py:1535} DagFileProcessor367 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:53,028] {jobs.py:1169} DagFileProcessor367 INFO - Processing hello_world
[2018-04-19 21:10:53,037] {jobs.py:566} DagFileProcessor367 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:53,042] {models.py:322} DagFileProcessor367 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:53,042] {models.py:328} DagFileProcessor367 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:53.042806
[2018-04-19 21:10:53,046] {jobs.py:351} DagFileProcessor367 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:10:54,134] {jobs.py:343} DagFileProcessor368 INFO - Started process (PID=3933) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:54,139] {jobs.py:534} DagFileProcessor368 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:54,140] {jobs.py:1521} DagFileProcessor368 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:54,140] {models.py:167} DagFileProcessor368 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:54,247] {jobs.py:1535} DagFileProcessor368 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:54,266] {jobs.py:1169} DagFileProcessor368 INFO - Processing hello_world
[2018-04-19 21:10:54,275] {jobs.py:566} DagFileProcessor368 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:54,281] {models.py:322} DagFileProcessor368 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:54,281] {models.py:328} DagFileProcessor368 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:54.281677
[2018-04-19 21:10:54,285] {jobs.py:351} DagFileProcessor368 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:10:55,359] {jobs.py:343} DagFileProcessor369 INFO - Started process (PID=3934) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:55,364] {jobs.py:534} DagFileProcessor369 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:55,365] {jobs.py:1521} DagFileProcessor369 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:55,366] {models.py:167} DagFileProcessor369 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:55,476] {jobs.py:1535} DagFileProcessor369 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:55,495] {jobs.py:1169} DagFileProcessor369 INFO - Processing hello_world
[2018-04-19 21:10:55,504] {jobs.py:566} DagFileProcessor369 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:55,509] {models.py:322} DagFileProcessor369 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:55,510] {models.py:328} DagFileProcessor369 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:55.510238
[2018-04-19 21:10:55,515] {jobs.py:351} DagFileProcessor369 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:10:56,595] {jobs.py:343} DagFileProcessor370 INFO - Started process (PID=3935) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:56,604] {jobs.py:534} DagFileProcessor370 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:56,605] {jobs.py:1521} DagFileProcessor370 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:56,606] {models.py:167} DagFileProcessor370 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:56,713] {jobs.py:1535} DagFileProcessor370 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:56,733] {jobs.py:1169} DagFileProcessor370 INFO - Processing hello_world
[2018-04-19 21:10:56,742] {jobs.py:566} DagFileProcessor370 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:56,748] {models.py:322} DagFileProcessor370 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:56,749] {models.py:328} DagFileProcessor370 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:56.749222
[2018-04-19 21:10:56,752] {jobs.py:351} DagFileProcessor370 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:10:57,824] {jobs.py:343} DagFileProcessor371 INFO - Started process (PID=3936) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:57,829] {jobs.py:534} DagFileProcessor371 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:57,830] {jobs.py:1521} DagFileProcessor371 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:57,830] {models.py:167} DagFileProcessor371 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:57,930] {jobs.py:1535} DagFileProcessor371 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:57,952] {jobs.py:1169} DagFileProcessor371 INFO - Processing hello_world
[2018-04-19 21:10:57,960] {jobs.py:566} DagFileProcessor371 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:57,965] {models.py:322} DagFileProcessor371 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:57,966] {models.py:328} DagFileProcessor371 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:57.966026
[2018-04-19 21:10:57,970] {jobs.py:351} DagFileProcessor371 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:10:59,049] {jobs.py:343} DagFileProcessor372 INFO - Started process (PID=3937) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:59,054] {jobs.py:534} DagFileProcessor372 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:10:59,055] {jobs.py:1521} DagFileProcessor372 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:10:59,055] {models.py:167} DagFileProcessor372 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:59,161] {jobs.py:1535} DagFileProcessor372 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:10:59,179] {jobs.py:1169} DagFileProcessor372 INFO - Processing hello_world
[2018-04-19 21:10:59,188] {jobs.py:566} DagFileProcessor372 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:10:59,193] {models.py:322} DagFileProcessor372 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:10:59,194] {models.py:328} DagFileProcessor372 INFO - Failing jobs without heartbeat after 2018-04-19 21:05:59.194155
[2018-04-19 21:10:59,199] {jobs.py:351} DagFileProcessor372 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:11:00,275] {jobs.py:343} DagFileProcessor373 INFO - Started process (PID=3938) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:00,280] {jobs.py:534} DagFileProcessor373 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:00,282] {jobs.py:1521} DagFileProcessor373 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:00,282] {models.py:167} DagFileProcessor373 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:00,381] {jobs.py:1535} DagFileProcessor373 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:00,401] {jobs.py:1169} DagFileProcessor373 INFO - Processing hello_world
[2018-04-19 21:11:00,411] {jobs.py:566} DagFileProcessor373 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:00,416] {models.py:322} DagFileProcessor373 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:00,417] {models.py:328} DagFileProcessor373 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:00.417315
[2018-04-19 21:11:00,420] {jobs.py:351} DagFileProcessor373 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:11:01,507] {jobs.py:343} DagFileProcessor374 INFO - Started process (PID=3939) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:01,511] {jobs.py:534} DagFileProcessor374 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:01,513] {jobs.py:1521} DagFileProcessor374 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:01,513] {models.py:167} DagFileProcessor374 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:01,619] {jobs.py:1535} DagFileProcessor374 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:01,642] {jobs.py:1169} DagFileProcessor374 INFO - Processing hello_world
[2018-04-19 21:11:01,651] {jobs.py:566} DagFileProcessor374 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:01,656] {models.py:322} DagFileProcessor374 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:01,657] {models.py:328} DagFileProcessor374 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:01.657207
[2018-04-19 21:11:01,661] {jobs.py:351} DagFileProcessor374 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:11:02,741] {jobs.py:343} DagFileProcessor375 INFO - Started process (PID=3940) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:02,746] {jobs.py:534} DagFileProcessor375 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:02,747] {jobs.py:1521} DagFileProcessor375 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:02,748] {models.py:167} DagFileProcessor375 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:02,854] {jobs.py:1535} DagFileProcessor375 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:02,877] {jobs.py:1169} DagFileProcessor375 INFO - Processing hello_world
[2018-04-19 21:11:02,887] {jobs.py:566} DagFileProcessor375 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:02,893] {models.py:322} DagFileProcessor375 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:02,894] {models.py:328} DagFileProcessor375 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:02.894042
[2018-04-19 21:11:02,898] {jobs.py:351} DagFileProcessor375 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:11:03,968] {jobs.py:343} DagFileProcessor376 INFO - Started process (PID=3942) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:03,973] {jobs.py:534} DagFileProcessor376 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:03,974] {jobs.py:1521} DagFileProcessor376 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:03,974] {models.py:167} DagFileProcessor376 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:04,083] {jobs.py:1535} DagFileProcessor376 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:04,102] {jobs.py:1169} DagFileProcessor376 INFO - Processing hello_world
[2018-04-19 21:11:04,111] {jobs.py:566} DagFileProcessor376 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:04,117] {models.py:322} DagFileProcessor376 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:04,118] {models.py:328} DagFileProcessor376 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:04.118077
[2018-04-19 21:11:04,121] {jobs.py:351} DagFileProcessor376 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:11:05,197] {jobs.py:343} DagFileProcessor377 INFO - Started process (PID=3943) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:05,202] {jobs.py:534} DagFileProcessor377 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:05,203] {jobs.py:1521} DagFileProcessor377 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:05,204] {models.py:167} DagFileProcessor377 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:05,302] {jobs.py:1535} DagFileProcessor377 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:05,323] {jobs.py:1169} DagFileProcessor377 INFO - Processing hello_world
[2018-04-19 21:11:05,331] {jobs.py:566} DagFileProcessor377 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:05,336] {models.py:322} DagFileProcessor377 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:05,337] {models.py:328} DagFileProcessor377 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:05.337099
[2018-04-19 21:11:05,340] {jobs.py:351} DagFileProcessor377 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:11:06,427] {jobs.py:343} DagFileProcessor378 INFO - Started process (PID=3944) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:06,432] {jobs.py:534} DagFileProcessor378 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:06,433] {jobs.py:1521} DagFileProcessor378 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:06,433] {models.py:167} DagFileProcessor378 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:06,538] {jobs.py:1535} DagFileProcessor378 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:06,557] {jobs.py:1169} DagFileProcessor378 INFO - Processing hello_world
[2018-04-19 21:11:06,566] {jobs.py:566} DagFileProcessor378 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:06,572] {models.py:322} DagFileProcessor378 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:06,572] {models.py:328} DagFileProcessor378 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:06.572655
[2018-04-19 21:11:06,576] {jobs.py:351} DagFileProcessor378 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:11:07,643] {jobs.py:343} DagFileProcessor379 INFO - Started process (PID=3945) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:07,648] {jobs.py:534} DagFileProcessor379 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:07,650] {jobs.py:1521} DagFileProcessor379 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:07,650] {models.py:167} DagFileProcessor379 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:07,749] {jobs.py:1535} DagFileProcessor379 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:07,768] {jobs.py:1169} DagFileProcessor379 INFO - Processing hello_world
[2018-04-19 21:11:07,776] {jobs.py:566} DagFileProcessor379 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:07,781] {models.py:322} DagFileProcessor379 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:07,782] {models.py:328} DagFileProcessor379 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:07.782313
[2018-04-19 21:11:07,785] {jobs.py:351} DagFileProcessor379 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.142 seconds
[2018-04-19 21:11:08,875] {jobs.py:343} DagFileProcessor380 INFO - Started process (PID=3946) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:08,881] {jobs.py:534} DagFileProcessor380 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:08,884] {jobs.py:1521} DagFileProcessor380 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:08,884] {models.py:167} DagFileProcessor380 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:09,007] {jobs.py:1535} DagFileProcessor380 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:09,026] {jobs.py:1169} DagFileProcessor380 INFO - Processing hello_world
[2018-04-19 21:11:09,036] {jobs.py:566} DagFileProcessor380 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:09,041] {models.py:322} DagFileProcessor380 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:09,042] {models.py:328} DagFileProcessor380 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:09.042020
[2018-04-19 21:11:09,045] {jobs.py:351} DagFileProcessor380 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.171 seconds
[2018-04-19 21:11:10,109] {jobs.py:343} DagFileProcessor381 INFO - Started process (PID=3947) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:10,114] {jobs.py:534} DagFileProcessor381 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:10,115] {jobs.py:1521} DagFileProcessor381 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:10,116] {models.py:167} DagFileProcessor381 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:10,240] {jobs.py:1535} DagFileProcessor381 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:10,261] {jobs.py:1169} DagFileProcessor381 INFO - Processing hello_world
[2018-04-19 21:11:10,270] {jobs.py:566} DagFileProcessor381 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:10,275] {models.py:322} DagFileProcessor381 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:10,275] {models.py:328} DagFileProcessor381 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:10.275562
[2018-04-19 21:11:10,279] {jobs.py:351} DagFileProcessor381 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.170 seconds
[2018-04-19 21:11:11,333] {jobs.py:343} DagFileProcessor382 INFO - Started process (PID=3948) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:11,338] {jobs.py:534} DagFileProcessor382 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:11,339] {jobs.py:1521} DagFileProcessor382 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:11,339] {models.py:167} DagFileProcessor382 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:11,444] {jobs.py:1535} DagFileProcessor382 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:11,462] {jobs.py:1169} DagFileProcessor382 INFO - Processing hello_world
[2018-04-19 21:11:11,472] {jobs.py:566} DagFileProcessor382 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:11,477] {models.py:322} DagFileProcessor382 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:11,478] {models.py:328} DagFileProcessor382 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:11.478136
[2018-04-19 21:11:11,481] {jobs.py:351} DagFileProcessor382 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:11:12,560] {jobs.py:343} DagFileProcessor383 INFO - Started process (PID=3956) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:12,565] {jobs.py:534} DagFileProcessor383 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:12,567] {jobs.py:1521} DagFileProcessor383 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:12,567] {models.py:167} DagFileProcessor383 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:12,678] {jobs.py:1535} DagFileProcessor383 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:12,696] {jobs.py:1169} DagFileProcessor383 INFO - Processing hello_world
[2018-04-19 21:11:12,704] {jobs.py:566} DagFileProcessor383 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:12,710] {models.py:322} DagFileProcessor383 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:12,711] {models.py:328} DagFileProcessor383 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:12.710996
[2018-04-19 21:11:12,714] {jobs.py:351} DagFileProcessor383 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.155 seconds
[2018-04-19 21:11:13,779] {jobs.py:343} DagFileProcessor384 INFO - Started process (PID=3958) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:13,784] {jobs.py:534} DagFileProcessor384 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:13,786] {jobs.py:1521} DagFileProcessor384 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:13,786] {models.py:167} DagFileProcessor384 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:13,891] {jobs.py:1535} DagFileProcessor384 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:13,911] {jobs.py:1169} DagFileProcessor384 INFO - Processing hello_world
[2018-04-19 21:11:13,921] {jobs.py:566} DagFileProcessor384 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:13,926] {models.py:322} DagFileProcessor384 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:13,927] {models.py:328} DagFileProcessor384 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:13.927211
[2018-04-19 21:11:13,930] {jobs.py:351} DagFileProcessor384 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:11:15,015] {jobs.py:343} DagFileProcessor385 INFO - Started process (PID=3961) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:15,020] {jobs.py:534} DagFileProcessor385 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:15,021] {jobs.py:1521} DagFileProcessor385 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:15,021] {models.py:167} DagFileProcessor385 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:15,127] {jobs.py:1535} DagFileProcessor385 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:15,148] {jobs.py:1169} DagFileProcessor385 INFO - Processing hello_world
[2018-04-19 21:11:15,161] {jobs.py:566} DagFileProcessor385 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:15,167] {models.py:322} DagFileProcessor385 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:15,168] {models.py:328} DagFileProcessor385 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:15.167826
[2018-04-19 21:11:15,171] {jobs.py:351} DagFileProcessor385 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:11:16,248] {jobs.py:343} DagFileProcessor386 INFO - Started process (PID=3962) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:16,253] {jobs.py:534} DagFileProcessor386 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:16,254] {jobs.py:1521} DagFileProcessor386 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:16,255] {models.py:167} DagFileProcessor386 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:16,356] {jobs.py:1535} DagFileProcessor386 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:16,375] {jobs.py:1169} DagFileProcessor386 INFO - Processing hello_world
[2018-04-19 21:11:16,384] {jobs.py:566} DagFileProcessor386 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:16,389] {models.py:322} DagFileProcessor386 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:16,390] {models.py:328} DagFileProcessor386 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:16.389949
[2018-04-19 21:11:16,393] {jobs.py:351} DagFileProcessor386 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:11:17,482] {jobs.py:343} DagFileProcessor387 INFO - Started process (PID=3963) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:17,487] {jobs.py:534} DagFileProcessor387 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:17,489] {jobs.py:1521} DagFileProcessor387 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:17,490] {models.py:167} DagFileProcessor387 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:17,600] {jobs.py:1535} DagFileProcessor387 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:17,619] {jobs.py:1169} DagFileProcessor387 INFO - Processing hello_world
[2018-04-19 21:11:17,627] {jobs.py:566} DagFileProcessor387 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:17,632] {models.py:322} DagFileProcessor387 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:17,632] {models.py:328} DagFileProcessor387 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:17.632799
[2018-04-19 21:11:17,636] {jobs.py:351} DagFileProcessor387 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:11:18,703] {jobs.py:343} DagFileProcessor388 INFO - Started process (PID=3964) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:18,707] {jobs.py:534} DagFileProcessor388 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:18,708] {jobs.py:1521} DagFileProcessor388 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:18,709] {models.py:167} DagFileProcessor388 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:18,807] {jobs.py:1535} DagFileProcessor388 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:18,826] {jobs.py:1169} DagFileProcessor388 INFO - Processing hello_world
[2018-04-19 21:11:18,835] {jobs.py:566} DagFileProcessor388 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:18,840] {models.py:322} DagFileProcessor388 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:18,841] {models.py:328} DagFileProcessor388 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:18.840837
[2018-04-19 21:11:18,844] {jobs.py:351} DagFileProcessor388 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:11:19,932] {jobs.py:343} DagFileProcessor389 INFO - Started process (PID=3965) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:19,937] {jobs.py:534} DagFileProcessor389 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:19,938] {jobs.py:1521} DagFileProcessor389 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:19,939] {models.py:167} DagFileProcessor389 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:20,046] {jobs.py:1535} DagFileProcessor389 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:20,067] {jobs.py:1169} DagFileProcessor389 INFO - Processing hello_world
[2018-04-19 21:11:20,078] {jobs.py:566} DagFileProcessor389 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:20,084] {models.py:322} DagFileProcessor389 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:20,084] {models.py:328} DagFileProcessor389 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:20.084499
[2018-04-19 21:11:20,087] {jobs.py:351} DagFileProcessor389 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.155 seconds
[2018-04-19 21:11:21,163] {jobs.py:343} DagFileProcessor390 INFO - Started process (PID=3966) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:21,168] {jobs.py:534} DagFileProcessor390 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:21,169] {jobs.py:1521} DagFileProcessor390 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:21,170] {models.py:167} DagFileProcessor390 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:21,272] {jobs.py:1535} DagFileProcessor390 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:21,292] {jobs.py:1169} DagFileProcessor390 INFO - Processing hello_world
[2018-04-19 21:11:21,300] {jobs.py:566} DagFileProcessor390 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:21,306] {models.py:322} DagFileProcessor390 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:21,306] {models.py:328} DagFileProcessor390 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:21.306414
[2018-04-19 21:11:21,309] {jobs.py:351} DagFileProcessor390 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:11:22,395] {jobs.py:343} DagFileProcessor391 INFO - Started process (PID=3967) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:22,399] {jobs.py:534} DagFileProcessor391 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:22,400] {jobs.py:1521} DagFileProcessor391 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:22,401] {models.py:167} DagFileProcessor391 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:22,506] {jobs.py:1535} DagFileProcessor391 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:22,525] {jobs.py:1169} DagFileProcessor391 INFO - Processing hello_world
[2018-04-19 21:11:22,533] {jobs.py:566} DagFileProcessor391 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:22,538] {models.py:322} DagFileProcessor391 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:22,539] {models.py:328} DagFileProcessor391 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:22.539305
[2018-04-19 21:11:22,542] {jobs.py:351} DagFileProcessor391 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:11:23,618] {jobs.py:343} DagFileProcessor392 INFO - Started process (PID=3969) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:23,623] {jobs.py:534} DagFileProcessor392 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:23,624] {jobs.py:1521} DagFileProcessor392 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:23,624] {models.py:167} DagFileProcessor392 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:23,729] {jobs.py:1535} DagFileProcessor392 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:23,750] {jobs.py:1169} DagFileProcessor392 INFO - Processing hello_world
[2018-04-19 21:11:23,760] {jobs.py:566} DagFileProcessor392 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:23,765] {models.py:322} DagFileProcessor392 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:23,766] {models.py:328} DagFileProcessor392 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:23.765938
[2018-04-19 21:11:23,770] {jobs.py:351} DagFileProcessor392 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:11:24,847] {jobs.py:343} DagFileProcessor393 INFO - Started process (PID=3970) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:24,852] {jobs.py:534} DagFileProcessor393 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:24,854] {jobs.py:1521} DagFileProcessor393 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:24,854] {models.py:167} DagFileProcessor393 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:24,968] {jobs.py:1535} DagFileProcessor393 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:24,988] {jobs.py:1169} DagFileProcessor393 INFO - Processing hello_world
[2018-04-19 21:11:24,999] {jobs.py:566} DagFileProcessor393 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:25,006] {models.py:322} DagFileProcessor393 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:25,006] {models.py:328} DagFileProcessor393 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:25.006663
[2018-04-19 21:11:25,012] {jobs.py:351} DagFileProcessor393 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.165 seconds
[2018-04-19 21:11:26,077] {jobs.py:343} DagFileProcessor394 INFO - Started process (PID=3971) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:26,082] {jobs.py:534} DagFileProcessor394 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:26,083] {jobs.py:1521} DagFileProcessor394 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:26,084] {models.py:167} DagFileProcessor394 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:26,185] {jobs.py:1535} DagFileProcessor394 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:26,208] {jobs.py:1169} DagFileProcessor394 INFO - Processing hello_world
[2018-04-19 21:11:26,220] {jobs.py:566} DagFileProcessor394 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:26,225] {models.py:322} DagFileProcessor394 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:26,225] {models.py:328} DagFileProcessor394 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:26.225758
[2018-04-19 21:11:26,229] {jobs.py:351} DagFileProcessor394 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:11:27,308] {jobs.py:343} DagFileProcessor395 INFO - Started process (PID=3972) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:27,313] {jobs.py:534} DagFileProcessor395 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:27,314] {jobs.py:1521} DagFileProcessor395 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:27,315] {models.py:167} DagFileProcessor395 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:27,433] {jobs.py:1535} DagFileProcessor395 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:27,453] {jobs.py:1169} DagFileProcessor395 INFO - Processing hello_world
[2018-04-19 21:11:27,462] {jobs.py:566} DagFileProcessor395 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:27,468] {models.py:322} DagFileProcessor395 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:27,469] {models.py:328} DagFileProcessor395 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:27.469470
[2018-04-19 21:11:27,473] {jobs.py:351} DagFileProcessor395 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.165 seconds
[2018-04-19 21:11:28,547] {jobs.py:343} DagFileProcessor396 INFO - Started process (PID=3973) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:28,552] {jobs.py:534} DagFileProcessor396 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:28,553] {jobs.py:1521} DagFileProcessor396 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:28,554] {models.py:167} DagFileProcessor396 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:28,674] {jobs.py:1535} DagFileProcessor396 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:28,693] {jobs.py:1169} DagFileProcessor396 INFO - Processing hello_world
[2018-04-19 21:11:28,702] {jobs.py:566} DagFileProcessor396 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:28,709] {models.py:322} DagFileProcessor396 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:28,710] {models.py:328} DagFileProcessor396 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:28.710007
[2018-04-19 21:11:28,714] {jobs.py:351} DagFileProcessor396 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.167 seconds
[2018-04-19 21:11:29,773] {jobs.py:343} DagFileProcessor397 INFO - Started process (PID=3974) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:29,778] {jobs.py:534} DagFileProcessor397 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:29,779] {jobs.py:1521} DagFileProcessor397 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:29,779] {models.py:167} DagFileProcessor397 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:29,889] {jobs.py:1535} DagFileProcessor397 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:29,908] {jobs.py:1169} DagFileProcessor397 INFO - Processing hello_world
[2018-04-19 21:11:29,917] {jobs.py:566} DagFileProcessor397 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:29,924] {models.py:322} DagFileProcessor397 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:29,924] {models.py:328} DagFileProcessor397 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:29.924537
[2018-04-19 21:11:29,928] {jobs.py:351} DagFileProcessor397 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.155 seconds
[2018-04-19 21:11:31,003] {jobs.py:343} DagFileProcessor398 INFO - Started process (PID=3975) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:31,008] {jobs.py:534} DagFileProcessor398 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:31,009] {jobs.py:1521} DagFileProcessor398 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:31,009] {models.py:167} DagFileProcessor398 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:31,138] {jobs.py:1535} DagFileProcessor398 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:31,157] {jobs.py:1169} DagFileProcessor398 INFO - Processing hello_world
[2018-04-19 21:11:31,167] {jobs.py:566} DagFileProcessor398 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:31,173] {models.py:322} DagFileProcessor398 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:31,174] {models.py:328} DagFileProcessor398 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:31.173815
[2018-04-19 21:11:31,177] {jobs.py:351} DagFileProcessor398 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.174 seconds
[2018-04-19 21:11:32,235] {jobs.py:343} DagFileProcessor399 INFO - Started process (PID=3976) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:32,240] {jobs.py:534} DagFileProcessor399 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:32,242] {jobs.py:1521} DagFileProcessor399 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:32,242] {models.py:167} DagFileProcessor399 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:32,360] {jobs.py:1535} DagFileProcessor399 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:32,381] {jobs.py:1169} DagFileProcessor399 INFO - Processing hello_world
[2018-04-19 21:11:32,396] {jobs.py:566} DagFileProcessor399 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:32,404] {models.py:322} DagFileProcessor399 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:32,404] {models.py:328} DagFileProcessor399 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:32.404413
[2018-04-19 21:11:32,409] {jobs.py:351} DagFileProcessor399 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.174 seconds
[2018-04-19 21:11:33,470] {jobs.py:343} DagFileProcessor400 INFO - Started process (PID=3978) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:33,476] {jobs.py:534} DagFileProcessor400 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:33,477] {jobs.py:1521} DagFileProcessor400 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:33,477] {models.py:167} DagFileProcessor400 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:33,584] {jobs.py:1535} DagFileProcessor400 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:33,604] {jobs.py:1169} DagFileProcessor400 INFO - Processing hello_world
[2018-04-19 21:11:33,613] {jobs.py:566} DagFileProcessor400 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:33,618] {models.py:322} DagFileProcessor400 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:33,619] {models.py:328} DagFileProcessor400 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:33.619027
[2018-04-19 21:11:33,622] {jobs.py:351} DagFileProcessor400 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:11:34,700] {jobs.py:343} DagFileProcessor401 INFO - Started process (PID=3979) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:34,706] {jobs.py:534} DagFileProcessor401 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:34,708] {jobs.py:1521} DagFileProcessor401 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:34,708] {models.py:167} DagFileProcessor401 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:34,821] {jobs.py:1535} DagFileProcessor401 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:34,843] {jobs.py:1169} DagFileProcessor401 INFO - Processing hello_world
[2018-04-19 21:11:34,853] {jobs.py:566} DagFileProcessor401 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:34,860] {models.py:322} DagFileProcessor401 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:34,861] {models.py:328} DagFileProcessor401 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:34.861315
[2018-04-19 21:11:34,866] {jobs.py:351} DagFileProcessor401 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.166 seconds
[2018-04-19 21:11:35,929] {jobs.py:343} DagFileProcessor402 INFO - Started process (PID=3980) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:35,934] {jobs.py:534} DagFileProcessor402 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:35,935] {jobs.py:1521} DagFileProcessor402 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:35,936] {models.py:167} DagFileProcessor402 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:36,045] {jobs.py:1535} DagFileProcessor402 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:36,065] {jobs.py:1169} DagFileProcessor402 INFO - Processing hello_world
[2018-04-19 21:11:36,074] {jobs.py:566} DagFileProcessor402 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:36,080] {models.py:322} DagFileProcessor402 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:36,080] {models.py:328} DagFileProcessor402 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:36.080396
[2018-04-19 21:11:36,085] {jobs.py:351} DagFileProcessor402 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:11:37,159] {jobs.py:343} DagFileProcessor403 INFO - Started process (PID=3981) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:37,164] {jobs.py:534} DagFileProcessor403 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:37,165] {jobs.py:1521} DagFileProcessor403 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:37,165] {models.py:167} DagFileProcessor403 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:37,267] {jobs.py:1535} DagFileProcessor403 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:37,287] {jobs.py:1169} DagFileProcessor403 INFO - Processing hello_world
[2018-04-19 21:11:37,296] {jobs.py:566} DagFileProcessor403 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:37,301] {models.py:322} DagFileProcessor403 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:37,302] {models.py:328} DagFileProcessor403 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:37.302150
[2018-04-19 21:11:37,305] {jobs.py:351} DagFileProcessor403 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:11:38,395] {jobs.py:343} DagFileProcessor404 INFO - Started process (PID=3982) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:38,400] {jobs.py:534} DagFileProcessor404 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:38,401] {jobs.py:1521} DagFileProcessor404 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:38,401] {models.py:167} DagFileProcessor404 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:38,503] {jobs.py:1535} DagFileProcessor404 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:38,526] {jobs.py:1169} DagFileProcessor404 INFO - Processing hello_world
[2018-04-19 21:11:38,534] {jobs.py:566} DagFileProcessor404 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:38,540] {models.py:322} DagFileProcessor404 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:38,540] {models.py:328} DagFileProcessor404 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:38.540320
[2018-04-19 21:11:38,581] {jobs.py:351} DagFileProcessor404 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.187 seconds
[2018-04-19 21:11:39,632] {jobs.py:343} DagFileProcessor405 INFO - Started process (PID=3983) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:39,641] {jobs.py:534} DagFileProcessor405 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:39,642] {jobs.py:1521} DagFileProcessor405 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:39,642] {models.py:167} DagFileProcessor405 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:39,757] {jobs.py:1535} DagFileProcessor405 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:39,782] {jobs.py:1169} DagFileProcessor405 INFO - Processing hello_world
[2018-04-19 21:11:39,795] {jobs.py:566} DagFileProcessor405 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:39,861] {models.py:322} DagFileProcessor405 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:39,862] {models.py:328} DagFileProcessor405 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:39.862229
[2018-04-19 21:11:39,866] {jobs.py:351} DagFileProcessor405 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.234 seconds
[2018-04-19 21:11:40,962] {jobs.py:343} DagFileProcessor406 INFO - Started process (PID=3984) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:40,967] {jobs.py:534} DagFileProcessor406 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:40,969] {jobs.py:1521} DagFileProcessor406 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:40,969] {models.py:167} DagFileProcessor406 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:41,094] {jobs.py:1535} DagFileProcessor406 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:41,115] {jobs.py:1169} DagFileProcessor406 INFO - Processing hello_world
[2018-04-19 21:11:41,180] {jobs.py:566} DagFileProcessor406 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:41,186] {models.py:322} DagFileProcessor406 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:41,187] {models.py:328} DagFileProcessor406 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:41.187030
[2018-04-19 21:11:41,191] {jobs.py:351} DagFileProcessor406 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.229 seconds
[2018-04-19 21:11:42,287] {jobs.py:343} DagFileProcessor407 INFO - Started process (PID=3985) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:42,293] {jobs.py:534} DagFileProcessor407 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:42,294] {jobs.py:1521} DagFileProcessor407 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:42,295] {models.py:167} DagFileProcessor407 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:42,408] {jobs.py:1535} DagFileProcessor407 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:42,426] {jobs.py:1169} DagFileProcessor407 INFO - Processing hello_world
[2018-04-19 21:11:42,478] {jobs.py:566} DagFileProcessor407 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:42,483] {models.py:322} DagFileProcessor407 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:42,483] {models.py:328} DagFileProcessor407 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:42.483530
[2018-04-19 21:11:42,487] {jobs.py:351} DagFileProcessor407 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.199 seconds
[2018-04-19 21:11:43,613] {jobs.py:343} DagFileProcessor408 INFO - Started process (PID=3987) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:43,620] {jobs.py:534} DagFileProcessor408 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:43,621] {jobs.py:1521} DagFileProcessor408 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:43,621] {models.py:167} DagFileProcessor408 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:43,730] {jobs.py:1535} DagFileProcessor408 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:43,788] {jobs.py:1169} DagFileProcessor408 INFO - Processing hello_world
[2018-04-19 21:11:43,796] {jobs.py:566} DagFileProcessor408 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:43,801] {models.py:322} DagFileProcessor408 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:43,802] {models.py:328} DagFileProcessor408 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:43.801975
[2018-04-19 21:11:43,805] {jobs.py:351} DagFileProcessor408 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.192 seconds
[2018-04-19 21:11:44,840] {jobs.py:343} DagFileProcessor409 INFO - Started process (PID=3988) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:44,845] {jobs.py:534} DagFileProcessor409 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:44,847] {jobs.py:1521} DagFileProcessor409 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:44,847] {models.py:167} DagFileProcessor409 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:44,958] {jobs.py:1535} DagFileProcessor409 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:45,014] {jobs.py:1169} DagFileProcessor409 INFO - Processing hello_world
[2018-04-19 21:11:45,022] {jobs.py:566} DagFileProcessor409 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:45,027] {models.py:322} DagFileProcessor409 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:45,028] {models.py:328} DagFileProcessor409 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:45.028049
[2018-04-19 21:11:45,031] {jobs.py:351} DagFileProcessor409 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.192 seconds
[2018-04-19 21:11:46,064] {jobs.py:343} DagFileProcessor410 INFO - Started process (PID=3989) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:46,069] {jobs.py:534} DagFileProcessor410 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:46,070] {jobs.py:1521} DagFileProcessor410 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:46,070] {models.py:167} DagFileProcessor410 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:46,218] {jobs.py:1535} DagFileProcessor410 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:46,235] {jobs.py:1169} DagFileProcessor410 INFO - Processing hello_world
[2018-04-19 21:11:46,243] {jobs.py:566} DagFileProcessor410 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:46,248] {models.py:322} DagFileProcessor410 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:46,248] {models.py:328} DagFileProcessor410 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:46.248794
[2018-04-19 21:11:46,252] {jobs.py:351} DagFileProcessor410 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.188 seconds
[2018-04-19 21:11:47,314] {jobs.py:343} DagFileProcessor411 INFO - Started process (PID=3990) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:47,319] {jobs.py:534} DagFileProcessor411 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:47,320] {jobs.py:1521} DagFileProcessor411 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:47,320] {models.py:167} DagFileProcessor411 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:47,419] {jobs.py:1535} DagFileProcessor411 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:47,437] {jobs.py:1169} DagFileProcessor411 INFO - Processing hello_world
[2018-04-19 21:11:47,446] {jobs.py:566} DagFileProcessor411 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:47,451] {models.py:322} DagFileProcessor411 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:47,452] {models.py:328} DagFileProcessor411 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:47.451867
[2018-04-19 21:11:47,455] {jobs.py:351} DagFileProcessor411 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:11:48,547] {jobs.py:343} DagFileProcessor412 INFO - Started process (PID=3991) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:48,552] {jobs.py:534} DagFileProcessor412 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:48,553] {jobs.py:1521} DagFileProcessor412 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:48,554] {models.py:167} DagFileProcessor412 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:48,664] {jobs.py:1535} DagFileProcessor412 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:48,683] {jobs.py:1169} DagFileProcessor412 INFO - Processing hello_world
[2018-04-19 21:11:48,693] {jobs.py:566} DagFileProcessor412 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:48,699] {models.py:322} DagFileProcessor412 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:48,699] {models.py:328} DagFileProcessor412 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:48.699527
[2018-04-19 21:11:48,703] {jobs.py:351} DagFileProcessor412 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:11:49,770] {jobs.py:343} DagFileProcessor413 INFO - Started process (PID=3992) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:49,775] {jobs.py:534} DagFileProcessor413 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:49,776] {jobs.py:1521} DagFileProcessor413 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:49,776] {models.py:167} DagFileProcessor413 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:49,901] {jobs.py:1535} DagFileProcessor413 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:49,920] {jobs.py:1169} DagFileProcessor413 INFO - Processing hello_world
[2018-04-19 21:11:49,930] {jobs.py:566} DagFileProcessor413 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:49,936] {models.py:322} DagFileProcessor413 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:49,936] {models.py:328} DagFileProcessor413 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:49.936667
[2018-04-19 21:11:49,940] {jobs.py:351} DagFileProcessor413 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.171 seconds
[2018-04-19 21:11:51,001] {jobs.py:343} DagFileProcessor414 INFO - Started process (PID=3993) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:51,007] {jobs.py:534} DagFileProcessor414 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:51,008] {jobs.py:1521} DagFileProcessor414 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:51,009] {models.py:167} DagFileProcessor414 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:51,127] {jobs.py:1535} DagFileProcessor414 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:51,146] {jobs.py:1169} DagFileProcessor414 INFO - Processing hello_world
[2018-04-19 21:11:51,156] {jobs.py:566} DagFileProcessor414 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:51,162] {models.py:322} DagFileProcessor414 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:51,162] {models.py:328} DagFileProcessor414 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:51.162561
[2018-04-19 21:11:51,165] {jobs.py:351} DagFileProcessor414 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.164 seconds
[2018-04-19 21:11:52,234] {jobs.py:343} DagFileProcessor415 INFO - Started process (PID=3994) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:52,239] {jobs.py:534} DagFileProcessor415 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:52,241] {jobs.py:1521} DagFileProcessor415 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:52,242] {models.py:167} DagFileProcessor415 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:52,343] {jobs.py:1535} DagFileProcessor415 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:52,362] {jobs.py:1169} DagFileProcessor415 INFO - Processing hello_world
[2018-04-19 21:11:52,371] {jobs.py:566} DagFileProcessor415 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:52,376] {models.py:322} DagFileProcessor415 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:52,377] {models.py:328} DagFileProcessor415 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:52.377253
[2018-04-19 21:11:52,380] {jobs.py:351} DagFileProcessor415 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:11:53,462] {jobs.py:343} DagFileProcessor416 INFO - Started process (PID=3996) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:53,467] {jobs.py:534} DagFileProcessor416 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:53,468] {jobs.py:1521} DagFileProcessor416 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:53,469] {models.py:167} DagFileProcessor416 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:53,571] {jobs.py:1535} DagFileProcessor416 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:53,591] {jobs.py:1169} DagFileProcessor416 INFO - Processing hello_world
[2018-04-19 21:11:53,599] {jobs.py:566} DagFileProcessor416 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:53,605] {models.py:322} DagFileProcessor416 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:53,605] {models.py:328} DagFileProcessor416 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:53.605623
[2018-04-19 21:11:53,609] {jobs.py:351} DagFileProcessor416 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:11:54,694] {jobs.py:343} DagFileProcessor417 INFO - Started process (PID=3997) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:54,699] {jobs.py:534} DagFileProcessor417 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:54,700] {jobs.py:1521} DagFileProcessor417 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:54,700] {models.py:167} DagFileProcessor417 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:54,803] {jobs.py:1535} DagFileProcessor417 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:54,825] {jobs.py:1169} DagFileProcessor417 INFO - Processing hello_world
[2018-04-19 21:11:54,834] {jobs.py:566} DagFileProcessor417 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:54,841] {models.py:322} DagFileProcessor417 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:54,842] {models.py:328} DagFileProcessor417 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:54.841890
[2018-04-19 21:11:54,845] {jobs.py:351} DagFileProcessor417 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:11:55,925] {jobs.py:343} DagFileProcessor418 INFO - Started process (PID=3998) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:55,930] {jobs.py:534} DagFileProcessor418 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:55,931] {jobs.py:1521} DagFileProcessor418 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:55,932] {models.py:167} DagFileProcessor418 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:56,040] {jobs.py:1535} DagFileProcessor418 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:56,083] {jobs.py:1169} DagFileProcessor418 INFO - Processing hello_world
[2018-04-19 21:11:56,092] {jobs.py:566} DagFileProcessor418 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:56,099] {models.py:322} DagFileProcessor418 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:56,099] {models.py:328} DagFileProcessor418 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:56.099631
[2018-04-19 21:11:56,103] {jobs.py:351} DagFileProcessor418 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.178 seconds
[2018-04-19 21:11:57,164] {jobs.py:343} DagFileProcessor419 INFO - Started process (PID=3999) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:57,169] {jobs.py:534} DagFileProcessor419 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:57,170] {jobs.py:1521} DagFileProcessor419 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:57,170] {models.py:167} DagFileProcessor419 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:57,280] {jobs.py:1535} DagFileProcessor419 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:57,299] {jobs.py:1169} DagFileProcessor419 INFO - Processing hello_world
[2018-04-19 21:11:57,308] {jobs.py:566} DagFileProcessor419 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:57,313] {models.py:322} DagFileProcessor419 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:57,314] {models.py:328} DagFileProcessor419 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:57.314009
[2018-04-19 21:11:57,317] {jobs.py:351} DagFileProcessor419 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:11:58,398] {jobs.py:343} DagFileProcessor420 INFO - Started process (PID=4000) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:58,403] {jobs.py:534} DagFileProcessor420 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:58,404] {jobs.py:1521} DagFileProcessor420 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:58,405] {models.py:167} DagFileProcessor420 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:58,508] {jobs.py:1535} DagFileProcessor420 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:58,526] {jobs.py:1169} DagFileProcessor420 INFO - Processing hello_world
[2018-04-19 21:11:58,534] {jobs.py:566} DagFileProcessor420 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:58,539] {models.py:322} DagFileProcessor420 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:58,540] {models.py:328} DagFileProcessor420 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:58.540306
[2018-04-19 21:11:58,543] {jobs.py:351} DagFileProcessor420 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:11:59,629] {jobs.py:343} DagFileProcessor421 INFO - Started process (PID=4001) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:59,635] {jobs.py:534} DagFileProcessor421 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:11:59,637] {jobs.py:1521} DagFileProcessor421 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:11:59,637] {models.py:167} DagFileProcessor421 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:59,744] {jobs.py:1535} DagFileProcessor421 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:11:59,762] {jobs.py:1169} DagFileProcessor421 INFO - Processing hello_world
[2018-04-19 21:11:59,771] {jobs.py:566} DagFileProcessor421 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:11:59,776] {models.py:322} DagFileProcessor421 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:11:59,777] {models.py:328} DagFileProcessor421 INFO - Failing jobs without heartbeat after 2018-04-19 21:06:59.777269
[2018-04-19 21:11:59,781] {jobs.py:351} DagFileProcessor421 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:12:00,858] {jobs.py:343} DagFileProcessor422 INFO - Started process (PID=4002) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:00,863] {jobs.py:534} DagFileProcessor422 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:00,864] {jobs.py:1521} DagFileProcessor422 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:00,864] {models.py:167} DagFileProcessor422 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:00,969] {jobs.py:1535} DagFileProcessor422 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:00,989] {jobs.py:1169} DagFileProcessor422 INFO - Processing hello_world
[2018-04-19 21:12:00,998] {jobs.py:566} DagFileProcessor422 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:01,004] {models.py:322} DagFileProcessor422 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:01,004] {models.py:328} DagFileProcessor422 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:01.004557
[2018-04-19 21:12:01,008] {jobs.py:351} DagFileProcessor422 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:12:02,085] {jobs.py:343} DagFileProcessor423 INFO - Started process (PID=4003) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:02,090] {jobs.py:534} DagFileProcessor423 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:02,091] {jobs.py:1521} DagFileProcessor423 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:02,091] {models.py:167} DagFileProcessor423 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:02,197] {jobs.py:1535} DagFileProcessor423 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:02,217] {jobs.py:1169} DagFileProcessor423 INFO - Processing hello_world
[2018-04-19 21:12:02,226] {jobs.py:566} DagFileProcessor423 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:02,232] {models.py:322} DagFileProcessor423 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:02,232] {models.py:328} DagFileProcessor423 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:02.232769
[2018-04-19 21:12:02,236] {jobs.py:351} DagFileProcessor423 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:12:03,322] {jobs.py:343} DagFileProcessor424 INFO - Started process (PID=4005) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:03,327] {jobs.py:534} DagFileProcessor424 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:03,328] {jobs.py:1521} DagFileProcessor424 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:03,328] {models.py:167} DagFileProcessor424 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:03,433] {jobs.py:1535} DagFileProcessor424 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:03,453] {jobs.py:1169} DagFileProcessor424 INFO - Processing hello_world
[2018-04-19 21:12:03,462] {jobs.py:566} DagFileProcessor424 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:03,467] {models.py:322} DagFileProcessor424 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:03,468] {models.py:328} DagFileProcessor424 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:03.468208
[2018-04-19 21:12:03,472] {jobs.py:351} DagFileProcessor424 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:12:04,555] {jobs.py:343} DagFileProcessor425 INFO - Started process (PID=4006) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:04,560] {jobs.py:534} DagFileProcessor425 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:04,561] {jobs.py:1521} DagFileProcessor425 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:04,562] {models.py:167} DagFileProcessor425 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:04,666] {jobs.py:1535} DagFileProcessor425 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:04,686] {jobs.py:1169} DagFileProcessor425 INFO - Processing hello_world
[2018-04-19 21:12:04,695] {jobs.py:566} DagFileProcessor425 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:04,700] {models.py:322} DagFileProcessor425 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:04,700] {models.py:328} DagFileProcessor425 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:04.700668
[2018-04-19 21:12:04,704] {jobs.py:351} DagFileProcessor425 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:12:05,778] {jobs.py:343} DagFileProcessor426 INFO - Started process (PID=4007) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:05,783] {jobs.py:534} DagFileProcessor426 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:05,784] {jobs.py:1521} DagFileProcessor426 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:05,785] {models.py:167} DagFileProcessor426 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:05,887] {jobs.py:1535} DagFileProcessor426 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:05,907] {jobs.py:1169} DagFileProcessor426 INFO - Processing hello_world
[2018-04-19 21:12:05,915] {jobs.py:566} DagFileProcessor426 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:05,920] {models.py:322} DagFileProcessor426 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:05,921] {models.py:328} DagFileProcessor426 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:05.920992
[2018-04-19 21:12:05,924] {jobs.py:351} DagFileProcessor426 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:12:07,007] {jobs.py:343} DagFileProcessor427 INFO - Started process (PID=4008) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:07,012] {jobs.py:534} DagFileProcessor427 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:07,014] {jobs.py:1521} DagFileProcessor427 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:07,014] {models.py:167} DagFileProcessor427 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:07,118] {jobs.py:1535} DagFileProcessor427 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:07,136] {jobs.py:1169} DagFileProcessor427 INFO - Processing hello_world
[2018-04-19 21:12:07,144] {jobs.py:566} DagFileProcessor427 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:07,149] {models.py:322} DagFileProcessor427 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:07,150] {models.py:328} DagFileProcessor427 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:07.150113
[2018-04-19 21:12:07,153] {jobs.py:351} DagFileProcessor427 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:12:08,240] {jobs.py:343} DagFileProcessor428 INFO - Started process (PID=4009) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:08,245] {jobs.py:534} DagFileProcessor428 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:08,246] {jobs.py:1521} DagFileProcessor428 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:08,246] {models.py:167} DagFileProcessor428 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:08,347] {jobs.py:1535} DagFileProcessor428 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:08,367] {jobs.py:1169} DagFileProcessor428 INFO - Processing hello_world
[2018-04-19 21:12:08,375] {jobs.py:566} DagFileProcessor428 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:08,380] {models.py:322} DagFileProcessor428 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:08,381] {models.py:328} DagFileProcessor428 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:08.381215
[2018-04-19 21:12:08,384] {jobs.py:351} DagFileProcessor428 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:12:09,478] {jobs.py:343} DagFileProcessor429 INFO - Started process (PID=4010) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:09,483] {jobs.py:534} DagFileProcessor429 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:09,484] {jobs.py:1521} DagFileProcessor429 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:09,485] {models.py:167} DagFileProcessor429 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:09,589] {jobs.py:1535} DagFileProcessor429 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:09,608] {jobs.py:1169} DagFileProcessor429 INFO - Processing hello_world
[2018-04-19 21:12:09,616] {jobs.py:566} DagFileProcessor429 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:09,622] {models.py:322} DagFileProcessor429 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:09,623] {models.py:328} DagFileProcessor429 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:09.622840
[2018-04-19 21:12:09,627] {jobs.py:351} DagFileProcessor429 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:12:10,703] {jobs.py:343} DagFileProcessor430 INFO - Started process (PID=4011) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:10,708] {jobs.py:534} DagFileProcessor430 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:10,709] {jobs.py:1521} DagFileProcessor430 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:10,709] {models.py:167} DagFileProcessor430 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:10,822] {jobs.py:1535} DagFileProcessor430 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:10,845] {jobs.py:1169} DagFileProcessor430 INFO - Processing hello_world
[2018-04-19 21:12:10,853] {jobs.py:566} DagFileProcessor430 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:10,861] {models.py:322} DagFileProcessor430 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:10,861] {models.py:328} DagFileProcessor430 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:10.861793
[2018-04-19 21:12:10,865] {jobs.py:351} DagFileProcessor430 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.162 seconds
[2018-04-19 21:12:11,932] {jobs.py:343} DagFileProcessor431 INFO - Started process (PID=4012) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:11,937] {jobs.py:534} DagFileProcessor431 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:11,939] {jobs.py:1521} DagFileProcessor431 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:11,940] {models.py:167} DagFileProcessor431 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:12,043] {jobs.py:1535} DagFileProcessor431 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:12,063] {jobs.py:1169} DagFileProcessor431 INFO - Processing hello_world
[2018-04-19 21:12:12,072] {jobs.py:566} DagFileProcessor431 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:12,077] {models.py:322} DagFileProcessor431 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:12,078] {models.py:328} DagFileProcessor431 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:12.078045
[2018-04-19 21:12:12,081] {jobs.py:351} DagFileProcessor431 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:12:13,159] {jobs.py:343} DagFileProcessor432 INFO - Started process (PID=4014) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:13,164] {jobs.py:534} DagFileProcessor432 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:13,166] {jobs.py:1521} DagFileProcessor432 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:13,166] {models.py:167} DagFileProcessor432 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:13,272] {jobs.py:1535} DagFileProcessor432 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:13,293] {jobs.py:1169} DagFileProcessor432 INFO - Processing hello_world
[2018-04-19 21:12:13,302] {jobs.py:566} DagFileProcessor432 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:13,307] {models.py:322} DagFileProcessor432 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:13,308] {models.py:328} DagFileProcessor432 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:13.307910
[2018-04-19 21:12:13,311] {jobs.py:351} DagFileProcessor432 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:12:14,391] {jobs.py:343} DagFileProcessor433 INFO - Started process (PID=4015) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:14,398] {jobs.py:534} DagFileProcessor433 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:14,399] {jobs.py:1521} DagFileProcessor433 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:14,399] {models.py:167} DagFileProcessor433 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:14,516] {jobs.py:1535} DagFileProcessor433 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:14,538] {jobs.py:1169} DagFileProcessor433 INFO - Processing hello_world
[2018-04-19 21:12:14,548] {jobs.py:566} DagFileProcessor433 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:14,554] {models.py:322} DagFileProcessor433 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:14,555] {models.py:328} DagFileProcessor433 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:14.554868
[2018-04-19 21:12:14,558] {jobs.py:351} DagFileProcessor433 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.167 seconds
[2018-04-19 21:12:15,625] {jobs.py:343} DagFileProcessor434 INFO - Started process (PID=4016) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:15,631] {jobs.py:534} DagFileProcessor434 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:15,632] {jobs.py:1521} DagFileProcessor434 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:15,632] {models.py:167} DagFileProcessor434 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:15,740] {jobs.py:1535} DagFileProcessor434 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:15,761] {jobs.py:1169} DagFileProcessor434 INFO - Processing hello_world
[2018-04-19 21:12:15,770] {jobs.py:566} DagFileProcessor434 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:15,776] {models.py:322} DagFileProcessor434 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:15,777] {models.py:328} DagFileProcessor434 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:15.777065
[2018-04-19 21:12:15,781] {jobs.py:351} DagFileProcessor434 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:12:16,855] {jobs.py:343} DagFileProcessor435 INFO - Started process (PID=4017) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:16,860] {jobs.py:534} DagFileProcessor435 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:16,861] {jobs.py:1521} DagFileProcessor435 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:16,861] {models.py:167} DagFileProcessor435 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:16,971] {jobs.py:1535} DagFileProcessor435 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:16,990] {jobs.py:1169} DagFileProcessor435 INFO - Processing hello_world
[2018-04-19 21:12:16,999] {jobs.py:566} DagFileProcessor435 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:17,004] {models.py:322} DagFileProcessor435 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:17,005] {models.py:328} DagFileProcessor435 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:17.005002
[2018-04-19 21:12:17,008] {jobs.py:351} DagFileProcessor435 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:12:18,078] {jobs.py:343} DagFileProcessor436 INFO - Started process (PID=4018) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:18,083] {jobs.py:534} DagFileProcessor436 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:18,084] {jobs.py:1521} DagFileProcessor436 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:18,084] {models.py:167} DagFileProcessor436 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:18,197] {jobs.py:1535} DagFileProcessor436 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:18,220] {jobs.py:1169} DagFileProcessor436 INFO - Processing hello_world
[2018-04-19 21:12:18,231] {jobs.py:566} DagFileProcessor436 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:18,237] {models.py:322} DagFileProcessor436 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:18,238] {models.py:328} DagFileProcessor436 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:18.238211
[2018-04-19 21:12:18,243] {jobs.py:351} DagFileProcessor436 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.165 seconds
[2018-04-19 21:12:19,297] {jobs.py:343} DagFileProcessor437 INFO - Started process (PID=4019) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:19,301] {jobs.py:534} DagFileProcessor437 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:19,303] {jobs.py:1521} DagFileProcessor437 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:19,303] {models.py:167} DagFileProcessor437 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:19,409] {jobs.py:1535} DagFileProcessor437 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:19,427] {jobs.py:1169} DagFileProcessor437 INFO - Processing hello_world
[2018-04-19 21:12:19,436] {jobs.py:566} DagFileProcessor437 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:19,441] {models.py:322} DagFileProcessor437 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:19,442] {models.py:328} DagFileProcessor437 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:19.442141
[2018-04-19 21:12:19,445] {jobs.py:351} DagFileProcessor437 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:12:20,526] {jobs.py:343} DagFileProcessor438 INFO - Started process (PID=4020) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:20,531] {jobs.py:534} DagFileProcessor438 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:20,532] {jobs.py:1521} DagFileProcessor438 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:20,532] {models.py:167} DagFileProcessor438 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:20,639] {jobs.py:1535} DagFileProcessor438 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:20,657] {jobs.py:1169} DagFileProcessor438 INFO - Processing hello_world
[2018-04-19 21:12:20,667] {jobs.py:566} DagFileProcessor438 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:20,673] {models.py:322} DagFileProcessor438 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:20,673] {models.py:328} DagFileProcessor438 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:20.673541
[2018-04-19 21:12:20,677] {jobs.py:351} DagFileProcessor438 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:12:21,757] {jobs.py:343} DagFileProcessor439 INFO - Started process (PID=4022) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:21,762] {jobs.py:534} DagFileProcessor439 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:21,763] {jobs.py:1521} DagFileProcessor439 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:21,764] {models.py:167} DagFileProcessor439 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:21,884] {jobs.py:1535} DagFileProcessor439 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:21,902] {jobs.py:1169} DagFileProcessor439 INFO - Processing hello_world
[2018-04-19 21:12:21,911] {jobs.py:566} DagFileProcessor439 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:21,916] {models.py:322} DagFileProcessor439 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:21,917] {models.py:328} DagFileProcessor439 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:21.917072
[2018-04-19 21:12:21,920] {jobs.py:351} DagFileProcessor439 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.163 seconds
[2018-04-19 21:12:22,983] {jobs.py:343} DagFileProcessor440 INFO - Started process (PID=4029) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:22,988] {jobs.py:534} DagFileProcessor440 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:22,989] {jobs.py:1521} DagFileProcessor440 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:22,990] {models.py:167} DagFileProcessor440 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:23,099] {jobs.py:1535} DagFileProcessor440 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:23,117] {jobs.py:1169} DagFileProcessor440 INFO - Processing hello_world
[2018-04-19 21:12:23,126] {jobs.py:566} DagFileProcessor440 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:23,131] {models.py:322} DagFileProcessor440 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:23,132] {models.py:328} DagFileProcessor440 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:23.132054
[2018-04-19 21:12:23,135] {jobs.py:351} DagFileProcessor440 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:12:24,211] {jobs.py:343} DagFileProcessor441 INFO - Started process (PID=4031) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:24,216] {jobs.py:534} DagFileProcessor441 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:24,218] {jobs.py:1521} DagFileProcessor441 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:24,218] {models.py:167} DagFileProcessor441 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:24,324] {jobs.py:1535} DagFileProcessor441 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:24,343] {jobs.py:1169} DagFileProcessor441 INFO - Processing hello_world
[2018-04-19 21:12:24,351] {jobs.py:566} DagFileProcessor441 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:24,356] {models.py:322} DagFileProcessor441 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:24,357] {models.py:328} DagFileProcessor441 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:24.357072
[2018-04-19 21:12:24,360] {jobs.py:351} DagFileProcessor441 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:12:25,441] {jobs.py:343} DagFileProcessor442 INFO - Started process (PID=4032) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:25,446] {jobs.py:534} DagFileProcessor442 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:25,447] {jobs.py:1521} DagFileProcessor442 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:25,447] {models.py:167} DagFileProcessor442 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:25,553] {jobs.py:1535} DagFileProcessor442 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:25,574] {jobs.py:1169} DagFileProcessor442 INFO - Processing hello_world
[2018-04-19 21:12:25,583] {jobs.py:566} DagFileProcessor442 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:25,588] {models.py:322} DagFileProcessor442 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:25,588] {models.py:328} DagFileProcessor442 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:25.588798
[2018-04-19 21:12:25,592] {jobs.py:351} DagFileProcessor442 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:12:26,664] {jobs.py:343} DagFileProcessor443 INFO - Started process (PID=4033) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:26,673] {jobs.py:534} DagFileProcessor443 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:26,674] {jobs.py:1521} DagFileProcessor443 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:26,674] {models.py:167} DagFileProcessor443 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:26,779] {jobs.py:1535} DagFileProcessor443 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:26,796] {jobs.py:1169} DagFileProcessor443 INFO - Processing hello_world
[2018-04-19 21:12:26,805] {jobs.py:566} DagFileProcessor443 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:26,811] {models.py:322} DagFileProcessor443 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:26,811] {models.py:328} DagFileProcessor443 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:26.811485
[2018-04-19 21:12:26,814] {jobs.py:351} DagFileProcessor443 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:12:27,899] {jobs.py:343} DagFileProcessor444 INFO - Started process (PID=4034) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:27,904] {jobs.py:534} DagFileProcessor444 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:27,906] {jobs.py:1521} DagFileProcessor444 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:27,906] {models.py:167} DagFileProcessor444 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:28,021] {jobs.py:1535} DagFileProcessor444 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:28,041] {jobs.py:1169} DagFileProcessor444 INFO - Processing hello_world
[2018-04-19 21:12:28,050] {jobs.py:566} DagFileProcessor444 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:28,056] {models.py:322} DagFileProcessor444 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:28,056] {models.py:328} DagFileProcessor444 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:28.056599
[2018-04-19 21:12:28,060] {jobs.py:351} DagFileProcessor444 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.161 seconds
[2018-04-19 21:12:29,127] {jobs.py:343} DagFileProcessor445 INFO - Started process (PID=4035) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:29,132] {jobs.py:534} DagFileProcessor445 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:29,133] {jobs.py:1521} DagFileProcessor445 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:29,133] {models.py:167} DagFileProcessor445 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:29,238] {jobs.py:1535} DagFileProcessor445 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:29,257] {jobs.py:1169} DagFileProcessor445 INFO - Processing hello_world
[2018-04-19 21:12:29,265] {jobs.py:566} DagFileProcessor445 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:29,271] {models.py:322} DagFileProcessor445 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:29,271] {models.py:328} DagFileProcessor445 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:29.271407
[2018-04-19 21:12:29,274] {jobs.py:351} DagFileProcessor445 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:12:30,358] {jobs.py:343} DagFileProcessor446 INFO - Started process (PID=4036) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:30,364] {jobs.py:534} DagFileProcessor446 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:30,365] {jobs.py:1521} DagFileProcessor446 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:30,366] {models.py:167} DagFileProcessor446 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:30,487] {jobs.py:1535} DagFileProcessor446 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:30,510] {jobs.py:1169} DagFileProcessor446 INFO - Processing hello_world
[2018-04-19 21:12:30,521] {jobs.py:566} DagFileProcessor446 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:30,527] {models.py:322} DagFileProcessor446 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:30,528] {models.py:328} DagFileProcessor446 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:30.527835
[2018-04-19 21:12:30,532] {jobs.py:351} DagFileProcessor446 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.174 seconds
[2018-04-19 21:12:31,588] {jobs.py:343} DagFileProcessor447 INFO - Started process (PID=4037) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:31,592] {jobs.py:534} DagFileProcessor447 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:31,594] {jobs.py:1521} DagFileProcessor447 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:31,594] {models.py:167} DagFileProcessor447 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:31,706] {jobs.py:1535} DagFileProcessor447 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:31,725] {jobs.py:1169} DagFileProcessor447 INFO - Processing hello_world
[2018-04-19 21:12:31,734] {jobs.py:566} DagFileProcessor447 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:31,740] {models.py:322} DagFileProcessor447 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:31,740] {models.py:328} DagFileProcessor447 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:31.740593
[2018-04-19 21:12:31,744] {jobs.py:351} DagFileProcessor447 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:12:32,817] {jobs.py:343} DagFileProcessor448 INFO - Started process (PID=4038) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:32,822] {jobs.py:534} DagFileProcessor448 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:32,823] {jobs.py:1521} DagFileProcessor448 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:32,824] {models.py:167} DagFileProcessor448 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:32,928] {jobs.py:1535} DagFileProcessor448 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:32,948] {jobs.py:1169} DagFileProcessor448 INFO - Processing hello_world
[2018-04-19 21:12:32,957] {jobs.py:566} DagFileProcessor448 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:32,965] {models.py:322} DagFileProcessor448 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:32,965] {models.py:328} DagFileProcessor448 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:32.965630
[2018-04-19 21:12:32,969] {jobs.py:351} DagFileProcessor448 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:12:34,056] {jobs.py:343} DagFileProcessor449 INFO - Started process (PID=4040) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:34,061] {jobs.py:534} DagFileProcessor449 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:34,063] {jobs.py:1521} DagFileProcessor449 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:34,063] {models.py:167} DagFileProcessor449 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:34,171] {jobs.py:1535} DagFileProcessor449 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:34,189] {jobs.py:1169} DagFileProcessor449 INFO - Processing hello_world
[2018-04-19 21:12:34,199] {jobs.py:566} DagFileProcessor449 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:34,204] {models.py:322} DagFileProcessor449 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:34,205] {models.py:328} DagFileProcessor449 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:34.205228
[2018-04-19 21:12:34,208] {jobs.py:351} DagFileProcessor449 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:12:35,290] {jobs.py:343} DagFileProcessor450 INFO - Started process (PID=4041) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:35,295] {jobs.py:534} DagFileProcessor450 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:35,296] {jobs.py:1521} DagFileProcessor450 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:35,297] {models.py:167} DagFileProcessor450 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:35,412] {jobs.py:1535} DagFileProcessor450 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:35,434] {jobs.py:1169} DagFileProcessor450 INFO - Processing hello_world
[2018-04-19 21:12:35,443] {jobs.py:566} DagFileProcessor450 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:35,450] {models.py:322} DagFileProcessor450 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:35,450] {models.py:328} DagFileProcessor450 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:35.450399
[2018-04-19 21:12:35,454] {jobs.py:351} DagFileProcessor450 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.165 seconds
[2018-04-19 21:12:36,519] {jobs.py:343} DagFileProcessor451 INFO - Started process (PID=4042) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:36,523] {jobs.py:534} DagFileProcessor451 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:36,525] {jobs.py:1521} DagFileProcessor451 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:36,525] {models.py:167} DagFileProcessor451 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:36,638] {jobs.py:1535} DagFileProcessor451 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:36,659] {jobs.py:1169} DagFileProcessor451 INFO - Processing hello_world
[2018-04-19 21:12:36,669] {jobs.py:566} DagFileProcessor451 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:36,674] {models.py:322} DagFileProcessor451 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:36,675] {models.py:328} DagFileProcessor451 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:36.675054
[2018-04-19 21:12:36,679] {jobs.py:351} DagFileProcessor451 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.160 seconds
[2018-04-19 21:12:37,740] {jobs.py:343} DagFileProcessor452 INFO - Started process (PID=4043) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:37,745] {jobs.py:534} DagFileProcessor452 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:37,747] {jobs.py:1521} DagFileProcessor452 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:37,747] {models.py:167} DagFileProcessor452 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:37,860] {jobs.py:1535} DagFileProcessor452 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:37,880] {jobs.py:1169} DagFileProcessor452 INFO - Processing hello_world
[2018-04-19 21:12:37,890] {jobs.py:566} DagFileProcessor452 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:37,896] {models.py:322} DagFileProcessor452 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:37,897] {models.py:328} DagFileProcessor452 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:37.896806
[2018-04-19 21:12:37,901] {jobs.py:351} DagFileProcessor452 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.161 seconds
[2018-04-19 21:12:38,971] {jobs.py:343} DagFileProcessor453 INFO - Started process (PID=4044) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:38,976] {jobs.py:534} DagFileProcessor453 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:38,977] {jobs.py:1521} DagFileProcessor453 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:38,978] {models.py:167} DagFileProcessor453 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:39,097] {jobs.py:1535} DagFileProcessor453 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:39,120] {jobs.py:1169} DagFileProcessor453 INFO - Processing hello_world
[2018-04-19 21:12:39,134] {jobs.py:566} DagFileProcessor453 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:39,143] {models.py:322} DagFileProcessor453 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:39,144] {models.py:328} DagFileProcessor453 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:39.143810
[2018-04-19 21:12:39,149] {jobs.py:351} DagFileProcessor453 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.178 seconds
[2018-04-19 21:12:40,202] {jobs.py:343} DagFileProcessor454 INFO - Started process (PID=4045) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:40,207] {jobs.py:534} DagFileProcessor454 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:40,208] {jobs.py:1521} DagFileProcessor454 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:40,208] {models.py:167} DagFileProcessor454 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:40,324] {jobs.py:1535} DagFileProcessor454 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:40,344] {jobs.py:1169} DagFileProcessor454 INFO - Processing hello_world
[2018-04-19 21:12:40,355] {jobs.py:566} DagFileProcessor454 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:40,360] {models.py:322} DagFileProcessor454 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:40,361] {models.py:328} DagFileProcessor454 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:40.360867
[2018-04-19 21:12:40,365] {jobs.py:351} DagFileProcessor454 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.163 seconds
[2018-04-19 21:12:41,424] {jobs.py:343} DagFileProcessor455 INFO - Started process (PID=4046) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:41,429] {jobs.py:534} DagFileProcessor455 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:41,430] {jobs.py:1521} DagFileProcessor455 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:41,431] {models.py:167} DagFileProcessor455 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:41,543] {jobs.py:1535} DagFileProcessor455 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:41,564] {jobs.py:1169} DagFileProcessor455 INFO - Processing hello_world
[2018-04-19 21:12:41,572] {jobs.py:566} DagFileProcessor455 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:41,578] {models.py:322} DagFileProcessor455 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:41,579] {models.py:328} DagFileProcessor455 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:41.579234
[2018-04-19 21:12:41,583] {jobs.py:351} DagFileProcessor455 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:12:42,657] {jobs.py:343} DagFileProcessor456 INFO - Started process (PID=4047) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:42,662] {jobs.py:534} DagFileProcessor456 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:42,663] {jobs.py:1521} DagFileProcessor456 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:42,664] {models.py:167} DagFileProcessor456 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:42,775] {jobs.py:1535} DagFileProcessor456 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:42,795] {jobs.py:1169} DagFileProcessor456 INFO - Processing hello_world
[2018-04-19 21:12:42,805] {jobs.py:566} DagFileProcessor456 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:42,810] {models.py:322} DagFileProcessor456 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:42,811] {models.py:328} DagFileProcessor456 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:42.811046
[2018-04-19 21:12:42,816] {jobs.py:351} DagFileProcessor456 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:12:43,887] {jobs.py:343} DagFileProcessor457 INFO - Started process (PID=4049) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:43,892] {jobs.py:534} DagFileProcessor457 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:43,893] {jobs.py:1521} DagFileProcessor457 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:43,893] {models.py:167} DagFileProcessor457 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:44,009] {jobs.py:1535} DagFileProcessor457 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:44,029] {jobs.py:1169} DagFileProcessor457 INFO - Processing hello_world
[2018-04-19 21:12:44,038] {jobs.py:566} DagFileProcessor457 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:44,044] {models.py:322} DagFileProcessor457 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:44,044] {models.py:328} DagFileProcessor457 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:44.044519
[2018-04-19 21:12:44,050] {jobs.py:351} DagFileProcessor457 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.163 seconds
[2018-04-19 21:12:45,110] {jobs.py:343} DagFileProcessor458 INFO - Started process (PID=4050) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:45,116] {jobs.py:534} DagFileProcessor458 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:45,117] {jobs.py:1521} DagFileProcessor458 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:45,117] {models.py:167} DagFileProcessor458 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:45,231] {jobs.py:1535} DagFileProcessor458 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:45,251] {jobs.py:1169} DagFileProcessor458 INFO - Processing hello_world
[2018-04-19 21:12:45,260] {jobs.py:566} DagFileProcessor458 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:45,267] {models.py:322} DagFileProcessor458 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:45,268] {models.py:328} DagFileProcessor458 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:45.268136
[2018-04-19 21:12:45,271] {jobs.py:351} DagFileProcessor458 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.161 seconds
[2018-04-19 21:12:46,349] {jobs.py:343} DagFileProcessor459 INFO - Started process (PID=4051) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:46,353] {jobs.py:534} DagFileProcessor459 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:46,355] {jobs.py:1521} DagFileProcessor459 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:46,355] {models.py:167} DagFileProcessor459 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:46,471] {jobs.py:1535} DagFileProcessor459 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:46,492] {jobs.py:1169} DagFileProcessor459 INFO - Processing hello_world
[2018-04-19 21:12:46,502] {jobs.py:566} DagFileProcessor459 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:46,508] {models.py:322} DagFileProcessor459 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:46,508] {models.py:328} DagFileProcessor459 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:46.508621
[2018-04-19 21:12:46,514] {jobs.py:351} DagFileProcessor459 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.165 seconds
[2018-04-19 21:12:47,578] {jobs.py:343} DagFileProcessor460 INFO - Started process (PID=4052) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:47,584] {jobs.py:534} DagFileProcessor460 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:47,586] {jobs.py:1521} DagFileProcessor460 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:47,586] {models.py:167} DagFileProcessor460 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:47,701] {jobs.py:1535} DagFileProcessor460 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:47,719] {jobs.py:1169} DagFileProcessor460 INFO - Processing hello_world
[2018-04-19 21:12:47,731] {jobs.py:566} DagFileProcessor460 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:47,738] {models.py:322} DagFileProcessor460 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:47,739] {models.py:328} DagFileProcessor460 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:47.739187
[2018-04-19 21:12:47,742] {jobs.py:351} DagFileProcessor460 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.164 seconds
[2018-04-19 21:12:48,809] {jobs.py:343} DagFileProcessor461 INFO - Started process (PID=4053) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:48,815] {jobs.py:534} DagFileProcessor461 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:48,816] {jobs.py:1521} DagFileProcessor461 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:48,816] {models.py:167} DagFileProcessor461 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:48,924] {jobs.py:1535} DagFileProcessor461 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:48,946] {jobs.py:1169} DagFileProcessor461 INFO - Processing hello_world
[2018-04-19 21:12:48,955] {jobs.py:566} DagFileProcessor461 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:48,960] {models.py:322} DagFileProcessor461 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:48,961] {models.py:328} DagFileProcessor461 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:48.961123
[2018-04-19 21:12:48,965] {jobs.py:351} DagFileProcessor461 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:12:50,034] {jobs.py:343} DagFileProcessor462 INFO - Started process (PID=4054) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:50,039] {jobs.py:534} DagFileProcessor462 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:50,040] {jobs.py:1521} DagFileProcessor462 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:50,040] {models.py:167} DagFileProcessor462 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:50,169] {jobs.py:1535} DagFileProcessor462 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:50,196] {jobs.py:1169} DagFileProcessor462 INFO - Processing hello_world
[2018-04-19 21:12:50,210] {jobs.py:566} DagFileProcessor462 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:50,219] {models.py:322} DagFileProcessor462 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:50,220] {models.py:328} DagFileProcessor462 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:50.220101
[2018-04-19 21:12:50,225] {jobs.py:351} DagFileProcessor462 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.191 seconds
[2018-04-19 21:12:51,251] {jobs.py:343} DagFileProcessor463 INFO - Started process (PID=4065) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:51,255] {jobs.py:534} DagFileProcessor463 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:51,257] {jobs.py:1521} DagFileProcessor463 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:51,257] {models.py:167} DagFileProcessor463 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:51,442] {jobs.py:1535} DagFileProcessor463 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:51,483] {jobs.py:1169} DagFileProcessor463 INFO - Processing hello_world
[2018-04-19 21:12:51,510] {jobs.py:566} DagFileProcessor463 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:51,524] {models.py:322} DagFileProcessor463 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:51,526] {models.py:328} DagFileProcessor463 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:51.525804
[2018-04-19 21:12:51,533] {jobs.py:351} DagFileProcessor463 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.282 seconds
[2018-04-19 21:12:52,608] {jobs.py:343} DagFileProcessor464 INFO - Started process (PID=4070) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:52,613] {jobs.py:534} DagFileProcessor464 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:52,615] {jobs.py:1521} DagFileProcessor464 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:52,615] {models.py:167} DagFileProcessor464 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:52,751] {jobs.py:1535} DagFileProcessor464 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:52,776] {jobs.py:1169} DagFileProcessor464 INFO - Processing hello_world
[2018-04-19 21:12:52,790] {jobs.py:566} DagFileProcessor464 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:52,797] {models.py:322} DagFileProcessor464 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:52,798] {models.py:328} DagFileProcessor464 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:52.798194
[2018-04-19 21:12:52,802] {jobs.py:351} DagFileProcessor464 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.194 seconds
[2018-04-19 21:12:53,934] {jobs.py:343} DagFileProcessor465 INFO - Started process (PID=4072) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:53,939] {jobs.py:534} DagFileProcessor465 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:53,941] {jobs.py:1521} DagFileProcessor465 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:53,941] {models.py:167} DagFileProcessor465 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:54,103] {jobs.py:1535} DagFileProcessor465 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:54,132] {jobs.py:1169} DagFileProcessor465 INFO - Processing hello_world
[2018-04-19 21:12:54,148] {jobs.py:566} DagFileProcessor465 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:54,157] {models.py:322} DagFileProcessor465 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:54,158] {models.py:328} DagFileProcessor465 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:54.157851
[2018-04-19 21:12:54,163] {jobs.py:351} DagFileProcessor465 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.229 seconds
[2018-04-19 21:12:55,269] {jobs.py:343} DagFileProcessor466 INFO - Started process (PID=4073) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:55,275] {jobs.py:534} DagFileProcessor466 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:55,277] {jobs.py:1521} DagFileProcessor466 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:55,277] {models.py:167} DagFileProcessor466 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:55,422] {jobs.py:1535} DagFileProcessor466 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:55,443] {jobs.py:1169} DagFileProcessor466 INFO - Processing hello_world
[2018-04-19 21:12:55,455] {jobs.py:566} DagFileProcessor466 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:55,462] {models.py:322} DagFileProcessor466 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:55,463] {models.py:328} DagFileProcessor466 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:55.463015
[2018-04-19 21:12:55,470] {jobs.py:351} DagFileProcessor466 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.201 seconds
[2018-04-19 21:12:56,593] {jobs.py:343} DagFileProcessor467 INFO - Started process (PID=4074) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:56,598] {jobs.py:534} DagFileProcessor467 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:56,599] {jobs.py:1521} DagFileProcessor467 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:56,600] {models.py:167} DagFileProcessor467 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:56,718] {jobs.py:1535} DagFileProcessor467 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:56,738] {jobs.py:1169} DagFileProcessor467 INFO - Processing hello_world
[2018-04-19 21:12:56,747] {jobs.py:566} DagFileProcessor467 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:56,754] {models.py:322} DagFileProcessor467 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:56,755] {models.py:328} DagFileProcessor467 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:56.755162
[2018-04-19 21:12:56,759] {jobs.py:351} DagFileProcessor467 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.166 seconds
[2018-04-19 21:12:57,827] {jobs.py:343} DagFileProcessor468 INFO - Started process (PID=4075) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:57,832] {jobs.py:534} DagFileProcessor468 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:57,834] {jobs.py:1521} DagFileProcessor468 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:57,834] {models.py:167} DagFileProcessor468 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:57,945] {jobs.py:1535} DagFileProcessor468 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:57,966] {jobs.py:1169} DagFileProcessor468 INFO - Processing hello_world
[2018-04-19 21:12:57,977] {jobs.py:566} DagFileProcessor468 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:57,985] {models.py:322} DagFileProcessor468 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:57,986] {models.py:328} DagFileProcessor468 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:57.985973
[2018-04-19 21:12:57,991] {jobs.py:351} DagFileProcessor468 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.163 seconds
[2018-04-19 21:12:59,051] {jobs.py:343} DagFileProcessor469 INFO - Started process (PID=4077) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:59,056] {jobs.py:534} DagFileProcessor469 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:12:59,057] {jobs.py:1521} DagFileProcessor469 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:12:59,058] {models.py:167} DagFileProcessor469 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:59,172] {jobs.py:1535} DagFileProcessor469 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:12:59,193] {jobs.py:1169} DagFileProcessor469 INFO - Processing hello_world
[2018-04-19 21:12:59,204] {jobs.py:566} DagFileProcessor469 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:12:59,210] {models.py:322} DagFileProcessor469 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:12:59,210] {models.py:328} DagFileProcessor469 INFO - Failing jobs without heartbeat after 2018-04-19 21:07:59.210634
[2018-04-19 21:12:59,214] {jobs.py:351} DagFileProcessor469 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.163 seconds
[2018-04-19 21:13:00,283] {jobs.py:343} DagFileProcessor470 INFO - Started process (PID=4078) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:00,288] {jobs.py:534} DagFileProcessor470 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:00,290] {jobs.py:1521} DagFileProcessor470 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:00,290] {models.py:167} DagFileProcessor470 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:00,406] {jobs.py:1535} DagFileProcessor470 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:00,427] {jobs.py:1169} DagFileProcessor470 INFO - Processing hello_world
[2018-04-19 21:13:00,437] {jobs.py:566} DagFileProcessor470 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:00,443] {models.py:322} DagFileProcessor470 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:00,444] {models.py:328} DagFileProcessor470 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:00.444075
[2018-04-19 21:13:00,447] {jobs.py:351} DagFileProcessor470 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.164 seconds
[2018-04-19 21:13:01,515] {jobs.py:343} DagFileProcessor471 INFO - Started process (PID=4079) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:01,521] {jobs.py:534} DagFileProcessor471 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:01,523] {jobs.py:1521} DagFileProcessor471 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:01,523] {models.py:167} DagFileProcessor471 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:01,640] {jobs.py:1535} DagFileProcessor471 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:01,659] {jobs.py:1169} DagFileProcessor471 INFO - Processing hello_world
[2018-04-19 21:13:01,670] {jobs.py:566} DagFileProcessor471 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:01,676] {models.py:322} DagFileProcessor471 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:01,677] {models.py:328} DagFileProcessor471 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:01.676813
[2018-04-19 21:13:01,680] {jobs.py:351} DagFileProcessor471 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.165 seconds
[2018-04-19 21:13:02,745] {jobs.py:343} DagFileProcessor472 INFO - Started process (PID=4080) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:02,751] {jobs.py:534} DagFileProcessor472 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:02,752] {jobs.py:1521} DagFileProcessor472 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:02,752] {models.py:167} DagFileProcessor472 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:02,868] {jobs.py:1535} DagFileProcessor472 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:02,889] {jobs.py:1169} DagFileProcessor472 INFO - Processing hello_world
[2018-04-19 21:13:02,903] {jobs.py:566} DagFileProcessor472 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:02,908] {models.py:322} DagFileProcessor472 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:02,909] {models.py:328} DagFileProcessor472 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:02.909133
[2018-04-19 21:13:02,913] {jobs.py:351} DagFileProcessor472 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.168 seconds
[2018-04-19 21:13:03,985] {jobs.py:343} DagFileProcessor473 INFO - Started process (PID=4082) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:03,990] {jobs.py:534} DagFileProcessor473 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:03,991] {jobs.py:1521} DagFileProcessor473 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:03,992] {models.py:167} DagFileProcessor473 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:04,094] {jobs.py:1535} DagFileProcessor473 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:04,113] {jobs.py:1169} DagFileProcessor473 INFO - Processing hello_world
[2018-04-19 21:13:04,122] {jobs.py:566} DagFileProcessor473 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:04,127] {models.py:322} DagFileProcessor473 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:04,127] {models.py:328} DagFileProcessor473 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:04.127614
[2018-04-19 21:13:04,131] {jobs.py:351} DagFileProcessor473 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:13:05,210] {jobs.py:343} DagFileProcessor474 INFO - Started process (PID=4083) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:05,215] {jobs.py:534} DagFileProcessor474 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:05,216] {jobs.py:1521} DagFileProcessor474 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:05,217] {models.py:167} DagFileProcessor474 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:05,319] {jobs.py:1535} DagFileProcessor474 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:05,338] {jobs.py:1169} DagFileProcessor474 INFO - Processing hello_world
[2018-04-19 21:13:05,347] {jobs.py:566} DagFileProcessor474 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:05,352] {models.py:322} DagFileProcessor474 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:05,352] {models.py:328} DagFileProcessor474 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:05.352512
[2018-04-19 21:13:05,356] {jobs.py:351} DagFileProcessor474 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:13:06,436] {jobs.py:343} DagFileProcessor475 INFO - Started process (PID=4084) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:06,441] {jobs.py:534} DagFileProcessor475 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:06,443] {jobs.py:1521} DagFileProcessor475 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:06,444] {models.py:167} DagFileProcessor475 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:06,545] {jobs.py:1535} DagFileProcessor475 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:06,565] {jobs.py:1169} DagFileProcessor475 INFO - Processing hello_world
[2018-04-19 21:13:06,573] {jobs.py:566} DagFileProcessor475 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:06,578] {models.py:322} DagFileProcessor475 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:06,579] {models.py:328} DagFileProcessor475 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:06.579007
[2018-04-19 21:13:06,582] {jobs.py:351} DagFileProcessor475 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:13:07,663] {jobs.py:343} DagFileProcessor476 INFO - Started process (PID=4085) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:07,669] {jobs.py:534} DagFileProcessor476 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:07,670] {jobs.py:1521} DagFileProcessor476 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:07,671] {models.py:167} DagFileProcessor476 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:07,778] {jobs.py:1535} DagFileProcessor476 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:07,799] {jobs.py:1169} DagFileProcessor476 INFO - Processing hello_world
[2018-04-19 21:13:07,807] {jobs.py:566} DagFileProcessor476 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:07,813] {models.py:322} DagFileProcessor476 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:07,813] {models.py:328} DagFileProcessor476 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:07.813322
[2018-04-19 21:13:07,816] {jobs.py:351} DagFileProcessor476 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:13:08,898] {jobs.py:343} DagFileProcessor477 INFO - Started process (PID=4086) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:08,904] {jobs.py:534} DagFileProcessor477 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:08,906] {jobs.py:1521} DagFileProcessor477 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:08,906] {models.py:167} DagFileProcessor477 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:09,027] {jobs.py:1535} DagFileProcessor477 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:09,047] {jobs.py:1169} DagFileProcessor477 INFO - Processing hello_world
[2018-04-19 21:13:09,060] {jobs.py:566} DagFileProcessor477 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:09,066] {models.py:322} DagFileProcessor477 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:09,066] {models.py:328} DagFileProcessor477 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:09.066671
[2018-04-19 21:13:09,071] {jobs.py:351} DagFileProcessor477 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.173 seconds
[2018-04-19 21:13:10,136] {jobs.py:343} DagFileProcessor478 INFO - Started process (PID=4087) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:10,141] {jobs.py:534} DagFileProcessor478 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:10,142] {jobs.py:1521} DagFileProcessor478 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:10,142] {models.py:167} DagFileProcessor478 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:10,245] {jobs.py:1535} DagFileProcessor478 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:10,264] {jobs.py:1169} DagFileProcessor478 INFO - Processing hello_world
[2018-04-19 21:13:10,273] {jobs.py:566} DagFileProcessor478 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:10,278] {models.py:322} DagFileProcessor478 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:10,278] {models.py:328} DagFileProcessor478 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:10.278663
[2018-04-19 21:13:10,282] {jobs.py:351} DagFileProcessor478 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:13:11,371] {jobs.py:343} DagFileProcessor479 INFO - Started process (PID=4088) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:11,376] {jobs.py:534} DagFileProcessor479 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:11,377] {jobs.py:1521} DagFileProcessor479 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:11,377] {models.py:167} DagFileProcessor479 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:11,506] {jobs.py:1535} DagFileProcessor479 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:11,524] {jobs.py:1169} DagFileProcessor479 INFO - Processing hello_world
[2018-04-19 21:13:11,533] {jobs.py:566} DagFileProcessor479 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:11,538] {models.py:322} DagFileProcessor479 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:11,539] {models.py:328} DagFileProcessor479 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:11.539215
[2018-04-19 21:13:11,542] {jobs.py:351} DagFileProcessor479 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.172 seconds
[2018-04-19 21:13:12,602] {jobs.py:343} DagFileProcessor480 INFO - Started process (PID=4089) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:12,608] {jobs.py:534} DagFileProcessor480 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:12,610] {jobs.py:1521} DagFileProcessor480 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:12,610] {models.py:167} DagFileProcessor480 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:12,721] {jobs.py:1535} DagFileProcessor480 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:12,742] {jobs.py:1169} DagFileProcessor480 INFO - Processing hello_world
[2018-04-19 21:13:12,750] {jobs.py:566} DagFileProcessor480 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:12,756] {models.py:322} DagFileProcessor480 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:12,757] {models.py:328} DagFileProcessor480 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:12.756904
[2018-04-19 21:13:12,760] {jobs.py:351} DagFileProcessor480 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:13:13,830] {jobs.py:343} DagFileProcessor481 INFO - Started process (PID=4091) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:13,836] {jobs.py:534} DagFileProcessor481 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:13,837] {jobs.py:1521} DagFileProcessor481 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:13,838] {models.py:167} DagFileProcessor481 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:13,949] {jobs.py:1535} DagFileProcessor481 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:13,972] {jobs.py:1169} DagFileProcessor481 INFO - Processing hello_world
[2018-04-19 21:13:13,980] {jobs.py:566} DagFileProcessor481 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:13,987] {models.py:322} DagFileProcessor481 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:13,987] {models.py:328} DagFileProcessor481 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:13.987460
[2018-04-19 21:13:13,991] {jobs.py:351} DagFileProcessor481 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.161 seconds
[2018-04-19 21:13:15,055] {jobs.py:343} DagFileProcessor482 INFO - Started process (PID=4092) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:15,060] {jobs.py:534} DagFileProcessor482 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:15,061] {jobs.py:1521} DagFileProcessor482 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:15,062] {models.py:167} DagFileProcessor482 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:15,175] {jobs.py:1535} DagFileProcessor482 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:15,194] {jobs.py:1169} DagFileProcessor482 INFO - Processing hello_world
[2018-04-19 21:13:15,204] {jobs.py:566} DagFileProcessor482 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:15,210] {models.py:322} DagFileProcessor482 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:15,210] {models.py:328} DagFileProcessor482 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:15.210682
[2018-04-19 21:13:15,214] {jobs.py:351} DagFileProcessor482 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:13:16,294] {jobs.py:343} DagFileProcessor483 INFO - Started process (PID=4093) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:16,299] {jobs.py:534} DagFileProcessor483 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:16,300] {jobs.py:1521} DagFileProcessor483 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:16,300] {models.py:167} DagFileProcessor483 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:16,402] {jobs.py:1535} DagFileProcessor483 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:16,422] {jobs.py:1169} DagFileProcessor483 INFO - Processing hello_world
[2018-04-19 21:13:16,431] {jobs.py:566} DagFileProcessor483 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:16,437] {models.py:322} DagFileProcessor483 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:16,438] {models.py:328} DagFileProcessor483 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:16.437843
[2018-04-19 21:13:16,441] {jobs.py:351} DagFileProcessor483 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:13:17,522] {jobs.py:343} DagFileProcessor484 INFO - Started process (PID=4094) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:17,528] {jobs.py:534} DagFileProcessor484 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:17,529] {jobs.py:1521} DagFileProcessor484 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:17,529] {models.py:167} DagFileProcessor484 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:17,646] {jobs.py:1535} DagFileProcessor484 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:17,667] {jobs.py:1169} DagFileProcessor484 INFO - Processing hello_world
[2018-04-19 21:13:17,678] {jobs.py:566} DagFileProcessor484 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:17,684] {models.py:322} DagFileProcessor484 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:17,685] {models.py:328} DagFileProcessor484 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:17.685020
[2018-04-19 21:13:17,690] {jobs.py:351} DagFileProcessor484 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.168 seconds
[2018-04-19 21:13:18,748] {jobs.py:343} DagFileProcessor485 INFO - Started process (PID=4095) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:18,753] {jobs.py:534} DagFileProcessor485 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:18,755] {jobs.py:1521} DagFileProcessor485 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:18,756] {models.py:167} DagFileProcessor485 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:18,859] {jobs.py:1535} DagFileProcessor485 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:18,881] {jobs.py:1169} DagFileProcessor485 INFO - Processing hello_world
[2018-04-19 21:13:18,893] {jobs.py:566} DagFileProcessor485 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:18,899] {models.py:322} DagFileProcessor485 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:18,899] {models.py:328} DagFileProcessor485 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:18.899629
[2018-04-19 21:13:18,904] {jobs.py:351} DagFileProcessor485 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.155 seconds
[2018-04-19 21:13:19,973] {jobs.py:343} DagFileProcessor486 INFO - Started process (PID=4096) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:19,978] {jobs.py:534} DagFileProcessor486 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:19,979] {jobs.py:1521} DagFileProcessor486 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:19,979] {models.py:167} DagFileProcessor486 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:20,091] {jobs.py:1535} DagFileProcessor486 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:20,112] {jobs.py:1169} DagFileProcessor486 INFO - Processing hello_world
[2018-04-19 21:13:20,121] {jobs.py:566} DagFileProcessor486 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:20,127] {models.py:322} DagFileProcessor486 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:20,128] {models.py:328} DagFileProcessor486 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:20.127926
[2018-04-19 21:13:20,131] {jobs.py:351} DagFileProcessor486 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:13:21,203] {jobs.py:343} DagFileProcessor487 INFO - Started process (PID=4097) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:21,209] {jobs.py:534} DagFileProcessor487 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:21,210] {jobs.py:1521} DagFileProcessor487 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:21,210] {models.py:167} DagFileProcessor487 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:21,316] {jobs.py:1535} DagFileProcessor487 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:21,337] {jobs.py:1169} DagFileProcessor487 INFO - Processing hello_world
[2018-04-19 21:13:21,345] {jobs.py:566} DagFileProcessor487 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:21,351] {models.py:322} DagFileProcessor487 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:21,351] {models.py:328} DagFileProcessor487 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:21.351478
[2018-04-19 21:13:21,355] {jobs.py:351} DagFileProcessor487 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:13:22,438] {jobs.py:343} DagFileProcessor488 INFO - Started process (PID=4105) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:22,443] {jobs.py:534} DagFileProcessor488 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:22,444] {jobs.py:1521} DagFileProcessor488 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:22,445] {models.py:167} DagFileProcessor488 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:22,547] {jobs.py:1535} DagFileProcessor488 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:22,566] {jobs.py:1169} DagFileProcessor488 INFO - Processing hello_world
[2018-04-19 21:13:22,574] {jobs.py:566} DagFileProcessor488 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:22,579] {models.py:322} DagFileProcessor488 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:22,580] {models.py:328} DagFileProcessor488 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:22.580016
[2018-04-19 21:13:22,584] {jobs.py:351} DagFileProcessor488 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:13:23,667] {jobs.py:343} DagFileProcessor489 INFO - Started process (PID=4107) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:23,674] {jobs.py:534} DagFileProcessor489 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:23,675] {jobs.py:1521} DagFileProcessor489 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:23,675] {models.py:167} DagFileProcessor489 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:23,792] {jobs.py:1535} DagFileProcessor489 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:23,812] {jobs.py:1169} DagFileProcessor489 INFO - Processing hello_world
[2018-04-19 21:13:23,822] {jobs.py:566} DagFileProcessor489 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:23,827] {models.py:322} DagFileProcessor489 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:23,828] {models.py:328} DagFileProcessor489 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:23.828166
[2018-04-19 21:13:23,832] {jobs.py:351} DagFileProcessor489 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.165 seconds
[2018-04-19 21:13:24,899] {jobs.py:343} DagFileProcessor490 INFO - Started process (PID=4108) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:24,904] {jobs.py:534} DagFileProcessor490 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:24,906] {jobs.py:1521} DagFileProcessor490 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:24,907] {models.py:167} DagFileProcessor490 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:25,011] {jobs.py:1535} DagFileProcessor490 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:25,029] {jobs.py:1169} DagFileProcessor490 INFO - Processing hello_world
[2018-04-19 21:13:25,038] {jobs.py:566} DagFileProcessor490 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:25,044] {models.py:322} DagFileProcessor490 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:25,045] {models.py:328} DagFileProcessor490 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:25.045123
[2018-04-19 21:13:25,048] {jobs.py:351} DagFileProcessor490 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:13:26,138] {jobs.py:343} DagFileProcessor491 INFO - Started process (PID=4109) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:26,171] {jobs.py:534} DagFileProcessor491 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:26,173] {jobs.py:1521} DagFileProcessor491 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:26,173] {models.py:167} DagFileProcessor491 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:26,316] {jobs.py:1535} DagFileProcessor491 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:26,346] {jobs.py:1169} DagFileProcessor491 INFO - Processing hello_world
[2018-04-19 21:13:26,356] {jobs.py:566} DagFileProcessor491 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:26,362] {models.py:322} DagFileProcessor491 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:26,363] {models.py:328} DagFileProcessor491 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:26.362874
[2018-04-19 21:13:26,367] {jobs.py:351} DagFileProcessor491 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.229 seconds
[2018-04-19 21:13:27,478] {jobs.py:343} DagFileProcessor492 INFO - Started process (PID=4110) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:27,484] {jobs.py:534} DagFileProcessor492 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:27,485] {jobs.py:1521} DagFileProcessor492 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:27,485] {models.py:167} DagFileProcessor492 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:27,591] {jobs.py:1535} DagFileProcessor492 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:27,610] {jobs.py:1169} DagFileProcessor492 INFO - Processing hello_world
[2018-04-19 21:13:27,618] {jobs.py:566} DagFileProcessor492 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:27,624] {models.py:322} DagFileProcessor492 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:27,625] {models.py:328} DagFileProcessor492 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:27.625355
[2018-04-19 21:13:27,629] {jobs.py:351} DagFileProcessor492 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:13:28,714] {jobs.py:343} DagFileProcessor493 INFO - Started process (PID=4111) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:28,719] {jobs.py:534} DagFileProcessor493 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:28,720] {jobs.py:1521} DagFileProcessor493 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:28,720] {models.py:167} DagFileProcessor493 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:28,840] {jobs.py:1535} DagFileProcessor493 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:28,859] {jobs.py:1169} DagFileProcessor493 INFO - Processing hello_world
[2018-04-19 21:13:28,868] {jobs.py:566} DagFileProcessor493 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:28,874] {models.py:322} DagFileProcessor493 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:28,875] {models.py:328} DagFileProcessor493 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:28.875275
[2018-04-19 21:13:28,879] {jobs.py:351} DagFileProcessor493 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.165 seconds
[2018-04-19 21:13:29,939] {jobs.py:343} DagFileProcessor494 INFO - Started process (PID=4112) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:29,944] {jobs.py:534} DagFileProcessor494 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:29,945] {jobs.py:1521} DagFileProcessor494 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:29,945] {models.py:167} DagFileProcessor494 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:30,052] {jobs.py:1535} DagFileProcessor494 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:30,071] {jobs.py:1169} DagFileProcessor494 INFO - Processing hello_world
[2018-04-19 21:13:30,079] {jobs.py:566} DagFileProcessor494 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:30,085] {models.py:322} DagFileProcessor494 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:30,086] {models.py:328} DagFileProcessor494 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:30.085797
[2018-04-19 21:13:30,090] {jobs.py:351} DagFileProcessor494 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:13:31,161] {jobs.py:343} DagFileProcessor495 INFO - Started process (PID=4113) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:31,167] {jobs.py:534} DagFileProcessor495 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:31,168] {jobs.py:1521} DagFileProcessor495 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:31,168] {models.py:167} DagFileProcessor495 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:31,328] {jobs.py:1535} DagFileProcessor495 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:31,348] {jobs.py:1169} DagFileProcessor495 INFO - Processing hello_world
[2018-04-19 21:13:31,358] {jobs.py:566} DagFileProcessor495 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:31,364] {models.py:322} DagFileProcessor495 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:31,365] {models.py:328} DagFileProcessor495 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:31.365365
[2018-04-19 21:13:31,369] {jobs.py:351} DagFileProcessor495 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.208 seconds
[2018-04-19 21:13:32,503] {jobs.py:343} DagFileProcessor496 INFO - Started process (PID=4114) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:32,508] {jobs.py:534} DagFileProcessor496 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:32,509] {jobs.py:1521} DagFileProcessor496 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:32,509] {models.py:167} DagFileProcessor496 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:32,624] {jobs.py:1535} DagFileProcessor496 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:32,645] {jobs.py:1169} DagFileProcessor496 INFO - Processing hello_world
[2018-04-19 21:13:32,654] {jobs.py:566} DagFileProcessor496 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:32,660] {models.py:322} DagFileProcessor496 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:32,661] {models.py:328} DagFileProcessor496 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:32.661253
[2018-04-19 21:13:32,665] {jobs.py:351} DagFileProcessor496 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.162 seconds
[2018-04-19 21:13:33,727] {jobs.py:343} DagFileProcessor497 INFO - Started process (PID=4116) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:33,732] {jobs.py:534} DagFileProcessor497 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:33,734] {jobs.py:1521} DagFileProcessor497 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:33,734] {models.py:167} DagFileProcessor497 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:33,847] {jobs.py:1535} DagFileProcessor497 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:33,867] {jobs.py:1169} DagFileProcessor497 INFO - Processing hello_world
[2018-04-19 21:13:33,877] {jobs.py:566} DagFileProcessor497 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:33,882] {models.py:322} DagFileProcessor497 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:33,883] {models.py:328} DagFileProcessor497 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:33.883270
[2018-04-19 21:13:33,887] {jobs.py:351} DagFileProcessor497 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.160 seconds
[2018-04-19 21:13:34,963] {jobs.py:343} DagFileProcessor498 INFO - Started process (PID=4117) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:34,968] {jobs.py:534} DagFileProcessor498 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:34,969] {jobs.py:1521} DagFileProcessor498 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:34,969] {models.py:167} DagFileProcessor498 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:35,073] {jobs.py:1535} DagFileProcessor498 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:35,092] {jobs.py:1169} DagFileProcessor498 INFO - Processing hello_world
[2018-04-19 21:13:35,102] {jobs.py:566} DagFileProcessor498 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:35,108] {models.py:322} DagFileProcessor498 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:35,108] {models.py:328} DagFileProcessor498 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:35.108734
[2018-04-19 21:13:35,112] {jobs.py:351} DagFileProcessor498 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:13:36,190] {jobs.py:343} DagFileProcessor499 INFO - Started process (PID=4118) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:36,195] {jobs.py:534} DagFileProcessor499 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:36,196] {jobs.py:1521} DagFileProcessor499 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:36,197] {models.py:167} DagFileProcessor499 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:36,308] {jobs.py:1535} DagFileProcessor499 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:36,326] {jobs.py:1169} DagFileProcessor499 INFO - Processing hello_world
[2018-04-19 21:13:36,335] {jobs.py:566} DagFileProcessor499 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:36,342] {models.py:322} DagFileProcessor499 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:36,342] {models.py:328} DagFileProcessor499 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:36.342621
[2018-04-19 21:13:36,346] {jobs.py:351} DagFileProcessor499 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:13:37,417] {jobs.py:343} DagFileProcessor500 INFO - Started process (PID=4119) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:37,422] {jobs.py:534} DagFileProcessor500 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:37,423] {jobs.py:1521} DagFileProcessor500 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:37,423] {models.py:167} DagFileProcessor500 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:37,534] {jobs.py:1535} DagFileProcessor500 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:37,554] {jobs.py:1169} DagFileProcessor500 INFO - Processing hello_world
[2018-04-19 21:13:37,563] {jobs.py:566} DagFileProcessor500 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:37,571] {models.py:322} DagFileProcessor500 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:37,572] {models.py:328} DagFileProcessor500 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:37.571996
[2018-04-19 21:13:37,576] {jobs.py:351} DagFileProcessor500 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:13:38,646] {jobs.py:343} DagFileProcessor501 INFO - Started process (PID=4120) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:38,651] {jobs.py:534} DagFileProcessor501 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:38,653] {jobs.py:1521} DagFileProcessor501 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:38,653] {models.py:167} DagFileProcessor501 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:38,759] {jobs.py:1535} DagFileProcessor501 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:38,778] {jobs.py:1169} DagFileProcessor501 INFO - Processing hello_world
[2018-04-19 21:13:38,788] {jobs.py:566} DagFileProcessor501 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:38,794] {models.py:322} DagFileProcessor501 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:38,795] {models.py:328} DagFileProcessor501 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:38.795157
[2018-04-19 21:13:38,799] {jobs.py:351} DagFileProcessor501 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:13:39,880] {jobs.py:343} DagFileProcessor502 INFO - Started process (PID=4121) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:39,885] {jobs.py:534} DagFileProcessor502 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:39,886] {jobs.py:1521} DagFileProcessor502 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:39,886] {models.py:167} DagFileProcessor502 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:39,994] {jobs.py:1535} DagFileProcessor502 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:40,014] {jobs.py:1169} DagFileProcessor502 INFO - Processing hello_world
[2018-04-19 21:13:40,022] {jobs.py:566} DagFileProcessor502 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:40,027] {models.py:322} DagFileProcessor502 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:40,028] {models.py:328} DagFileProcessor502 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:40.027942
[2018-04-19 21:13:40,031] {jobs.py:351} DagFileProcessor502 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:13:41,104] {jobs.py:343} DagFileProcessor503 INFO - Started process (PID=4122) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:41,109] {jobs.py:534} DagFileProcessor503 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:41,110] {jobs.py:1521} DagFileProcessor503 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:41,110] {models.py:167} DagFileProcessor503 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:41,218] {jobs.py:1535} DagFileProcessor503 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:41,239] {jobs.py:1169} DagFileProcessor503 INFO - Processing hello_world
[2018-04-19 21:13:41,247] {jobs.py:566} DagFileProcessor503 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:41,253] {models.py:322} DagFileProcessor503 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:41,253] {models.py:328} DagFileProcessor503 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:41.253539
[2018-04-19 21:13:41,257] {jobs.py:351} DagFileProcessor503 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:13:42,333] {jobs.py:343} DagFileProcessor504 INFO - Started process (PID=4123) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:42,338] {jobs.py:534} DagFileProcessor504 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:42,339] {jobs.py:1521} DagFileProcessor504 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:42,340] {models.py:167} DagFileProcessor504 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:42,445] {jobs.py:1535} DagFileProcessor504 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:42,463] {jobs.py:1169} DagFileProcessor504 INFO - Processing hello_world
[2018-04-19 21:13:42,474] {jobs.py:566} DagFileProcessor504 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:42,479] {models.py:322} DagFileProcessor504 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:42,480] {models.py:328} DagFileProcessor504 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:42.480154
[2018-04-19 21:13:42,484] {jobs.py:351} DagFileProcessor504 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:13:43,558] {jobs.py:343} DagFileProcessor505 INFO - Started process (PID=4125) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:43,563] {jobs.py:534} DagFileProcessor505 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:43,564] {jobs.py:1521} DagFileProcessor505 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:43,564] {models.py:167} DagFileProcessor505 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:43,671] {jobs.py:1535} DagFileProcessor505 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:43,693] {jobs.py:1169} DagFileProcessor505 INFO - Processing hello_world
[2018-04-19 21:13:43,701] {jobs.py:566} DagFileProcessor505 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:43,707] {models.py:322} DagFileProcessor505 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:43,707] {models.py:328} DagFileProcessor505 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:43.707465
[2018-04-19 21:13:43,710] {jobs.py:351} DagFileProcessor505 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:13:44,795] {jobs.py:343} DagFileProcessor506 INFO - Started process (PID=4126) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:44,800] {jobs.py:534} DagFileProcessor506 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:44,801] {jobs.py:1521} DagFileProcessor506 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:44,801] {models.py:167} DagFileProcessor506 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:44,905] {jobs.py:1535} DagFileProcessor506 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:44,924] {jobs.py:1169} DagFileProcessor506 INFO - Processing hello_world
[2018-04-19 21:13:44,933] {jobs.py:566} DagFileProcessor506 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:44,939] {models.py:322} DagFileProcessor506 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:44,939] {models.py:328} DagFileProcessor506 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:44.939597
[2018-04-19 21:13:44,943] {jobs.py:351} DagFileProcessor506 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:13:46,019] {jobs.py:343} DagFileProcessor507 INFO - Started process (PID=4127) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:46,023] {jobs.py:534} DagFileProcessor507 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:46,024] {jobs.py:1521} DagFileProcessor507 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:46,025] {models.py:167} DagFileProcessor507 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:46,135] {jobs.py:1535} DagFileProcessor507 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:46,151] {jobs.py:1169} DagFileProcessor507 INFO - Processing hello_world
[2018-04-19 21:13:46,162] {jobs.py:566} DagFileProcessor507 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:46,168] {models.py:322} DagFileProcessor507 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:46,169] {models.py:328} DagFileProcessor507 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:46.169051
[2018-04-19 21:13:46,172] {jobs.py:351} DagFileProcessor507 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:13:47,248] {jobs.py:343} DagFileProcessor508 INFO - Started process (PID=4128) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:47,253] {jobs.py:534} DagFileProcessor508 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:47,255] {jobs.py:1521} DagFileProcessor508 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:47,255] {models.py:167} DagFileProcessor508 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:47,363] {jobs.py:1535} DagFileProcessor508 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:47,385] {jobs.py:1169} DagFileProcessor508 INFO - Processing hello_world
[2018-04-19 21:13:47,394] {jobs.py:566} DagFileProcessor508 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:47,400] {models.py:322} DagFileProcessor508 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:47,400] {models.py:328} DagFileProcessor508 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:47.400382
[2018-04-19 21:13:47,403] {jobs.py:351} DagFileProcessor508 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.155 seconds
[2018-04-19 21:13:48,475] {jobs.py:343} DagFileProcessor509 INFO - Started process (PID=4129) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:48,480] {jobs.py:534} DagFileProcessor509 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:48,481] {jobs.py:1521} DagFileProcessor509 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:48,482] {models.py:167} DagFileProcessor509 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:48,589] {jobs.py:1535} DagFileProcessor509 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:48,608] {jobs.py:1169} DagFileProcessor509 INFO - Processing hello_world
[2018-04-19 21:13:48,617] {jobs.py:566} DagFileProcessor509 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:48,623] {models.py:322} DagFileProcessor509 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:48,623] {models.py:328} DagFileProcessor509 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:48.623814
[2018-04-19 21:13:48,627] {jobs.py:351} DagFileProcessor509 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:13:49,702] {jobs.py:343} DagFileProcessor510 INFO - Started process (PID=4130) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:49,707] {jobs.py:534} DagFileProcessor510 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:49,708] {jobs.py:1521} DagFileProcessor510 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:49,708] {models.py:167} DagFileProcessor510 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:49,816] {jobs.py:1535} DagFileProcessor510 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:49,837] {jobs.py:1169} DagFileProcessor510 INFO - Processing hello_world
[2018-04-19 21:13:49,847] {jobs.py:566} DagFileProcessor510 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:49,852] {models.py:322} DagFileProcessor510 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:49,852] {models.py:328} DagFileProcessor510 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:49.852789
[2018-04-19 21:13:49,856] {jobs.py:351} DagFileProcessor510 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:13:50,942] {jobs.py:343} DagFileProcessor511 INFO - Started process (PID=4131) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:50,947] {jobs.py:534} DagFileProcessor511 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:50,948] {jobs.py:1521} DagFileProcessor511 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:50,948] {models.py:167} DagFileProcessor511 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:51,056] {jobs.py:1535} DagFileProcessor511 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:51,076] {jobs.py:1169} DagFileProcessor511 INFO - Processing hello_world
[2018-04-19 21:13:51,085] {jobs.py:566} DagFileProcessor511 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:51,091] {models.py:322} DagFileProcessor511 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:51,091] {models.py:328} DagFileProcessor511 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:51.091418
[2018-04-19 21:13:51,095] {jobs.py:351} DagFileProcessor511 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:13:52,166] {jobs.py:343} DagFileProcessor512 INFO - Started process (PID=4132) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:52,171] {jobs.py:534} DagFileProcessor512 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:52,172] {jobs.py:1521} DagFileProcessor512 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:52,172] {models.py:167} DagFileProcessor512 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:52,278] {jobs.py:1535} DagFileProcessor512 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:52,298] {jobs.py:1169} DagFileProcessor512 INFO - Processing hello_world
[2018-04-19 21:13:52,307] {jobs.py:566} DagFileProcessor512 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:52,314] {models.py:322} DagFileProcessor512 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:52,314] {models.py:328} DagFileProcessor512 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:52.314523
[2018-04-19 21:13:52,317] {jobs.py:351} DagFileProcessor512 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:13:53,393] {jobs.py:343} DagFileProcessor513 INFO - Started process (PID=4134) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:53,398] {jobs.py:534} DagFileProcessor513 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:53,399] {jobs.py:1521} DagFileProcessor513 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:53,399] {models.py:167} DagFileProcessor513 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:53,503] {jobs.py:1535} DagFileProcessor513 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:53,524] {jobs.py:1169} DagFileProcessor513 INFO - Processing hello_world
[2018-04-19 21:13:53,533] {jobs.py:566} DagFileProcessor513 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:53,539] {models.py:322} DagFileProcessor513 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:53,540] {models.py:328} DagFileProcessor513 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:53.540095
[2018-04-19 21:13:53,544] {jobs.py:351} DagFileProcessor513 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:13:54,618] {jobs.py:343} DagFileProcessor514 INFO - Started process (PID=4135) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:54,623] {jobs.py:534} DagFileProcessor514 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:54,624] {jobs.py:1521} DagFileProcessor514 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:54,625] {models.py:167} DagFileProcessor514 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:54,729] {jobs.py:1535} DagFileProcessor514 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:54,748] {jobs.py:1169} DagFileProcessor514 INFO - Processing hello_world
[2018-04-19 21:13:54,756] {jobs.py:566} DagFileProcessor514 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:54,762] {models.py:322} DagFileProcessor514 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:54,762] {models.py:328} DagFileProcessor514 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:54.762476
[2018-04-19 21:13:54,766] {jobs.py:351} DagFileProcessor514 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:13:55,851] {jobs.py:343} DagFileProcessor515 INFO - Started process (PID=4136) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:55,856] {jobs.py:534} DagFileProcessor515 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:55,857] {jobs.py:1521} DagFileProcessor515 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:55,857] {models.py:167} DagFileProcessor515 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:55,962] {jobs.py:1535} DagFileProcessor515 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:55,981] {jobs.py:1169} DagFileProcessor515 INFO - Processing hello_world
[2018-04-19 21:13:55,991] {jobs.py:566} DagFileProcessor515 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:55,996] {models.py:322} DagFileProcessor515 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:55,996] {models.py:328} DagFileProcessor515 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:55.996638
[2018-04-19 21:13:56,000] {jobs.py:351} DagFileProcessor515 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:13:57,085] {jobs.py:343} DagFileProcessor516 INFO - Started process (PID=4137) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:57,090] {jobs.py:534} DagFileProcessor516 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:57,091] {jobs.py:1521} DagFileProcessor516 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:57,091] {models.py:167} DagFileProcessor516 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:57,195] {jobs.py:1535} DagFileProcessor516 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:57,214] {jobs.py:1169} DagFileProcessor516 INFO - Processing hello_world
[2018-04-19 21:13:57,223] {jobs.py:566} DagFileProcessor516 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:57,229] {models.py:322} DagFileProcessor516 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:57,229] {models.py:328} DagFileProcessor516 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:57.229408
[2018-04-19 21:13:57,233] {jobs.py:351} DagFileProcessor516 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:13:58,316] {jobs.py:343} DagFileProcessor517 INFO - Started process (PID=4138) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:58,321] {jobs.py:534} DagFileProcessor517 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:58,322] {jobs.py:1521} DagFileProcessor517 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:58,323] {models.py:167} DagFileProcessor517 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:58,427] {jobs.py:1535} DagFileProcessor517 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:58,447] {jobs.py:1169} DagFileProcessor517 INFO - Processing hello_world
[2018-04-19 21:13:58,455] {jobs.py:566} DagFileProcessor517 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:58,461] {models.py:322} DagFileProcessor517 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:58,461] {models.py:328} DagFileProcessor517 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:58.461609
[2018-04-19 21:13:58,465] {jobs.py:351} DagFileProcessor517 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:13:59,540] {jobs.py:343} DagFileProcessor518 INFO - Started process (PID=4139) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:59,545] {jobs.py:534} DagFileProcessor518 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:13:59,546] {jobs.py:1521} DagFileProcessor518 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:13:59,546] {models.py:167} DagFileProcessor518 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:59,652] {jobs.py:1535} DagFileProcessor518 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:13:59,671] {jobs.py:1169} DagFileProcessor518 INFO - Processing hello_world
[2018-04-19 21:13:59,681] {jobs.py:566} DagFileProcessor518 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:13:59,686] {models.py:322} DagFileProcessor518 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:13:59,687] {models.py:328} DagFileProcessor518 INFO - Failing jobs without heartbeat after 2018-04-19 21:08:59.687324
[2018-04-19 21:13:59,692] {jobs.py:351} DagFileProcessor518 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:14:00,773] {jobs.py:343} DagFileProcessor519 INFO - Started process (PID=4140) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:00,778] {jobs.py:534} DagFileProcessor519 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:00,779] {jobs.py:1521} DagFileProcessor519 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:00,779] {models.py:167} DagFileProcessor519 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:00,885] {jobs.py:1535} DagFileProcessor519 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:00,903] {jobs.py:1169} DagFileProcessor519 INFO - Processing hello_world
[2018-04-19 21:14:00,913] {jobs.py:566} DagFileProcessor519 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:00,919] {models.py:322} DagFileProcessor519 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:00,920] {models.py:328} DagFileProcessor519 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:00.919935
[2018-04-19 21:14:00,923] {jobs.py:351} DagFileProcessor519 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:14:02,000] {jobs.py:343} DagFileProcessor520 INFO - Started process (PID=4141) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:02,005] {jobs.py:534} DagFileProcessor520 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:02,006] {jobs.py:1521} DagFileProcessor520 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:02,007] {models.py:167} DagFileProcessor520 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:02,119] {jobs.py:1535} DagFileProcessor520 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:02,140] {jobs.py:1169} DagFileProcessor520 INFO - Processing hello_world
[2018-04-19 21:14:02,149] {jobs.py:566} DagFileProcessor520 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:02,155] {models.py:322} DagFileProcessor520 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:02,156] {models.py:328} DagFileProcessor520 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:02.155828
[2018-04-19 21:14:02,159] {jobs.py:351} DagFileProcessor520 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:14:03,238] {jobs.py:343} DagFileProcessor521 INFO - Started process (PID=4142) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:03,243] {jobs.py:534} DagFileProcessor521 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:03,245] {jobs.py:1521} DagFileProcessor521 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:03,245] {models.py:167} DagFileProcessor521 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:03,351] {jobs.py:1535} DagFileProcessor521 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:03,371] {jobs.py:1169} DagFileProcessor521 INFO - Processing hello_world
[2018-04-19 21:14:03,380] {jobs.py:566} DagFileProcessor521 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:03,386] {models.py:322} DagFileProcessor521 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:03,386] {models.py:328} DagFileProcessor521 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:03.386766
[2018-04-19 21:14:03,390] {jobs.py:351} DagFileProcessor521 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:14:04,469] {jobs.py:343} DagFileProcessor522 INFO - Started process (PID=4144) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:04,474] {jobs.py:534} DagFileProcessor522 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:04,475] {jobs.py:1521} DagFileProcessor522 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:04,475] {models.py:167} DagFileProcessor522 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:04,582] {jobs.py:1535} DagFileProcessor522 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:04,602] {jobs.py:1169} DagFileProcessor522 INFO - Processing hello_world
[2018-04-19 21:14:04,611] {jobs.py:566} DagFileProcessor522 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:04,617] {models.py:322} DagFileProcessor522 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:04,617] {models.py:328} DagFileProcessor522 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:04.617614
[2018-04-19 21:14:04,620] {jobs.py:351} DagFileProcessor522 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:14:05,694] {jobs.py:343} DagFileProcessor523 INFO - Started process (PID=4145) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:05,699] {jobs.py:534} DagFileProcessor523 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:05,700] {jobs.py:1521} DagFileProcessor523 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:05,700] {models.py:167} DagFileProcessor523 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:05,812] {jobs.py:1535} DagFileProcessor523 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:05,831] {jobs.py:1169} DagFileProcessor523 INFO - Processing hello_world
[2018-04-19 21:14:05,839] {jobs.py:566} DagFileProcessor523 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:05,845] {models.py:322} DagFileProcessor523 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:05,845] {models.py:328} DagFileProcessor523 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:05.845637
[2018-04-19 21:14:05,849] {jobs.py:351} DagFileProcessor523 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:14:06,927] {jobs.py:343} DagFileProcessor524 INFO - Started process (PID=4146) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:06,932] {jobs.py:534} DagFileProcessor524 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:06,933] {jobs.py:1521} DagFileProcessor524 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:06,934] {models.py:167} DagFileProcessor524 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:07,040] {jobs.py:1535} DagFileProcessor524 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:07,059] {jobs.py:1169} DagFileProcessor524 INFO - Processing hello_world
[2018-04-19 21:14:07,068] {jobs.py:566} DagFileProcessor524 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:07,074] {models.py:322} DagFileProcessor524 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:07,074] {models.py:328} DagFileProcessor524 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:07.074435
[2018-04-19 21:14:07,078] {jobs.py:351} DagFileProcessor524 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:14:08,160] {jobs.py:343} DagFileProcessor525 INFO - Started process (PID=4147) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:08,165] {jobs.py:534} DagFileProcessor525 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:08,166] {jobs.py:1521} DagFileProcessor525 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:08,166] {models.py:167} DagFileProcessor525 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:08,272] {jobs.py:1535} DagFileProcessor525 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:08,294] {jobs.py:1169} DagFileProcessor525 INFO - Processing hello_world
[2018-04-19 21:14:08,303] {jobs.py:566} DagFileProcessor525 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:08,308] {models.py:322} DagFileProcessor525 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:08,309] {models.py:328} DagFileProcessor525 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:08.308923
[2018-04-19 21:14:08,312] {jobs.py:351} DagFileProcessor525 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:14:09,392] {jobs.py:343} DagFileProcessor526 INFO - Started process (PID=4148) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:09,397] {jobs.py:534} DagFileProcessor526 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:09,398] {jobs.py:1521} DagFileProcessor526 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:09,398] {models.py:167} DagFileProcessor526 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:09,509] {jobs.py:1535} DagFileProcessor526 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:09,530] {jobs.py:1169} DagFileProcessor526 INFO - Processing hello_world
[2018-04-19 21:14:09,539] {jobs.py:566} DagFileProcessor526 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:09,545] {models.py:322} DagFileProcessor526 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:09,546] {models.py:328} DagFileProcessor526 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:09.545861
[2018-04-19 21:14:09,549] {jobs.py:351} DagFileProcessor526 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.158 seconds
[2018-04-19 21:14:10,622] {jobs.py:343} DagFileProcessor527 INFO - Started process (PID=4149) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:10,627] {jobs.py:534} DagFileProcessor527 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:10,628] {jobs.py:1521} DagFileProcessor527 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:10,629] {models.py:167} DagFileProcessor527 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:10,740] {jobs.py:1535} DagFileProcessor527 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:10,760] {jobs.py:1169} DagFileProcessor527 INFO - Processing hello_world
[2018-04-19 21:14:10,769] {jobs.py:566} DagFileProcessor527 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:10,774] {models.py:322} DagFileProcessor527 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:10,775] {models.py:328} DagFileProcessor527 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:10.775379
[2018-04-19 21:14:10,779] {jobs.py:351} DagFileProcessor527 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:14:11,850] {jobs.py:343} DagFileProcessor528 INFO - Started process (PID=4150) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:11,856] {jobs.py:534} DagFileProcessor528 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:11,857] {jobs.py:1521} DagFileProcessor528 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:11,857] {models.py:167} DagFileProcessor528 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:11,965] {jobs.py:1535} DagFileProcessor528 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:11,986] {jobs.py:1169} DagFileProcessor528 INFO - Processing hello_world
[2018-04-19 21:14:11,997] {jobs.py:566} DagFileProcessor528 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:12,004] {models.py:322} DagFileProcessor528 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:12,005] {models.py:328} DagFileProcessor528 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:12.005084
[2018-04-19 21:14:12,009] {jobs.py:351} DagFileProcessor528 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:14:13,078] {jobs.py:343} DagFileProcessor529 INFO - Started process (PID=4151) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:13,083] {jobs.py:534} DagFileProcessor529 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:13,084] {jobs.py:1521} DagFileProcessor529 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:13,084] {models.py:167} DagFileProcessor529 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:13,190] {jobs.py:1535} DagFileProcessor529 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:13,210] {jobs.py:1169} DagFileProcessor529 INFO - Processing hello_world
[2018-04-19 21:14:13,219] {jobs.py:566} DagFileProcessor529 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:13,224] {models.py:322} DagFileProcessor529 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:13,224] {models.py:328} DagFileProcessor529 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:13.224776
[2018-04-19 21:14:13,228] {jobs.py:351} DagFileProcessor529 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:14:14,301] {jobs.py:343} DagFileProcessor530 INFO - Started process (PID=4153) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:14,306] {jobs.py:534} DagFileProcessor530 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:14,307] {jobs.py:1521} DagFileProcessor530 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:14,308] {models.py:167} DagFileProcessor530 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:14,418] {jobs.py:1535} DagFileProcessor530 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:14,439] {jobs.py:1169} DagFileProcessor530 INFO - Processing hello_world
[2018-04-19 21:14:14,449] {jobs.py:566} DagFileProcessor530 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:14,455] {models.py:322} DagFileProcessor530 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:14,456] {models.py:328} DagFileProcessor530 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:14.455948
[2018-04-19 21:14:14,459] {jobs.py:351} DagFileProcessor530 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.158 seconds
[2018-04-19 21:14:15,536] {jobs.py:343} DagFileProcessor531 INFO - Started process (PID=4154) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:15,540] {jobs.py:534} DagFileProcessor531 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:15,542] {jobs.py:1521} DagFileProcessor531 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:15,542] {models.py:167} DagFileProcessor531 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:15,649] {jobs.py:1535} DagFileProcessor531 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:15,670] {jobs.py:1169} DagFileProcessor531 INFO - Processing hello_world
[2018-04-19 21:14:15,680] {jobs.py:566} DagFileProcessor531 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:15,686] {models.py:322} DagFileProcessor531 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:15,686] {models.py:328} DagFileProcessor531 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:15.686742
[2018-04-19 21:14:15,690] {jobs.py:351} DagFileProcessor531 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.155 seconds
[2018-04-19 21:14:16,762] {jobs.py:343} DagFileProcessor532 INFO - Started process (PID=4155) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:16,767] {jobs.py:534} DagFileProcessor532 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:16,768] {jobs.py:1521} DagFileProcessor532 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:16,768] {models.py:167} DagFileProcessor532 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:16,874] {jobs.py:1535} DagFileProcessor532 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:16,892] {jobs.py:1169} DagFileProcessor532 INFO - Processing hello_world
[2018-04-19 21:14:16,902] {jobs.py:566} DagFileProcessor532 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:16,908] {models.py:322} DagFileProcessor532 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:16,908] {models.py:328} DagFileProcessor532 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:16.908773
[2018-04-19 21:14:16,913] {jobs.py:351} DagFileProcessor532 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:14:17,993] {jobs.py:343} DagFileProcessor533 INFO - Started process (PID=4156) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:17,998] {jobs.py:534} DagFileProcessor533 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:17,999] {jobs.py:1521} DagFileProcessor533 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:17,999] {models.py:167} DagFileProcessor533 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:18,103] {jobs.py:1535} DagFileProcessor533 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:18,124] {jobs.py:1169} DagFileProcessor533 INFO - Processing hello_world
[2018-04-19 21:14:18,134] {jobs.py:566} DagFileProcessor533 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:18,139] {models.py:322} DagFileProcessor533 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:18,140] {models.py:328} DagFileProcessor533 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:18.140080
[2018-04-19 21:14:18,143] {jobs.py:351} DagFileProcessor533 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:14:19,220] {jobs.py:343} DagFileProcessor534 INFO - Started process (PID=4157) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:19,225] {jobs.py:534} DagFileProcessor534 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:19,226] {jobs.py:1521} DagFileProcessor534 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:19,226] {models.py:167} DagFileProcessor534 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:19,329] {jobs.py:1535} DagFileProcessor534 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:19,348] {jobs.py:1169} DagFileProcessor534 INFO - Processing hello_world
[2018-04-19 21:14:19,356] {jobs.py:566} DagFileProcessor534 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:19,362] {models.py:322} DagFileProcessor534 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:19,363] {models.py:328} DagFileProcessor534 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:19.363153
[2018-04-19 21:14:19,366] {jobs.py:351} DagFileProcessor534 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:14:20,444] {jobs.py:343} DagFileProcessor535 INFO - Started process (PID=4158) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:20,449] {jobs.py:534} DagFileProcessor535 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:20,450] {jobs.py:1521} DagFileProcessor535 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:20,450] {models.py:167} DagFileProcessor535 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:20,556] {jobs.py:1535} DagFileProcessor535 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:20,573] {jobs.py:1169} DagFileProcessor535 INFO - Processing hello_world
[2018-04-19 21:14:20,582] {jobs.py:566} DagFileProcessor535 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:20,587] {models.py:322} DagFileProcessor535 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:20,588] {models.py:328} DagFileProcessor535 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:20.588163
[2018-04-19 21:14:20,591] {jobs.py:351} DagFileProcessor535 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:14:21,671] {jobs.py:343} DagFileProcessor536 INFO - Started process (PID=4160) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:21,676] {jobs.py:534} DagFileProcessor536 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:21,677] {jobs.py:1521} DagFileProcessor536 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:21,678] {models.py:167} DagFileProcessor536 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:21,799] {jobs.py:1535} DagFileProcessor536 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:21,818] {jobs.py:1169} DagFileProcessor536 INFO - Processing hello_world
[2018-04-19 21:14:21,828] {jobs.py:566} DagFileProcessor536 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:21,834] {models.py:322} DagFileProcessor536 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:21,834] {models.py:328} DagFileProcessor536 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:21.834584
[2018-04-19 21:14:21,838] {jobs.py:351} DagFileProcessor536 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.167 seconds
[2018-04-19 21:14:22,896] {jobs.py:343} DagFileProcessor537 INFO - Started process (PID=4167) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:22,901] {jobs.py:534} DagFileProcessor537 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:22,902] {jobs.py:1521} DagFileProcessor537 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:22,902] {models.py:167} DagFileProcessor537 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:23,016] {jobs.py:1535} DagFileProcessor537 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:23,035] {jobs.py:1169} DagFileProcessor537 INFO - Processing hello_world
[2018-04-19 21:14:23,044] {jobs.py:566} DagFileProcessor537 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:23,050] {models.py:322} DagFileProcessor537 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:23,051] {models.py:328} DagFileProcessor537 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:23.051364
[2018-04-19 21:14:23,055] {jobs.py:351} DagFileProcessor537 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.160 seconds
[2018-04-19 21:14:24,123] {jobs.py:343} DagFileProcessor538 INFO - Started process (PID=4169) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:24,128] {jobs.py:534} DagFileProcessor538 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:24,129] {jobs.py:1521} DagFileProcessor538 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:24,129] {models.py:167} DagFileProcessor538 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:24,236] {jobs.py:1535} DagFileProcessor538 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:24,256] {jobs.py:1169} DagFileProcessor538 INFO - Processing hello_world
[2018-04-19 21:14:24,267] {jobs.py:566} DagFileProcessor538 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:24,272] {models.py:322} DagFileProcessor538 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:24,272] {models.py:328} DagFileProcessor538 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:24.272741
[2018-04-19 21:14:24,276] {jobs.py:351} DagFileProcessor538 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:14:25,353] {jobs.py:343} DagFileProcessor539 INFO - Started process (PID=4170) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:25,359] {jobs.py:534} DagFileProcessor539 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:25,360] {jobs.py:1521} DagFileProcessor539 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:25,361] {models.py:167} DagFileProcessor539 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:25,473] {jobs.py:1535} DagFileProcessor539 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:25,494] {jobs.py:1169} DagFileProcessor539 INFO - Processing hello_world
[2018-04-19 21:14:25,503] {jobs.py:566} DagFileProcessor539 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:25,509] {models.py:322} DagFileProcessor539 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:25,509] {models.py:328} DagFileProcessor539 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:25.509755
[2018-04-19 21:14:25,514] {jobs.py:351} DagFileProcessor539 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.160 seconds
[2018-04-19 21:14:26,577] {jobs.py:343} DagFileProcessor540 INFO - Started process (PID=4171) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:26,586] {jobs.py:534} DagFileProcessor540 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:26,587] {jobs.py:1521} DagFileProcessor540 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:26,587] {models.py:167} DagFileProcessor540 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:26,694] {jobs.py:1535} DagFileProcessor540 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:26,712] {jobs.py:1169} DagFileProcessor540 INFO - Processing hello_world
[2018-04-19 21:14:26,721] {jobs.py:566} DagFileProcessor540 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:26,727] {models.py:322} DagFileProcessor540 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:26,728] {models.py:328} DagFileProcessor540 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:26.728003
[2018-04-19 21:14:26,731] {jobs.py:351} DagFileProcessor540 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.155 seconds
[2018-04-19 21:14:27,814] {jobs.py:343} DagFileProcessor541 INFO - Started process (PID=4172) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:27,819] {jobs.py:534} DagFileProcessor541 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:27,820] {jobs.py:1521} DagFileProcessor541 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:27,821] {models.py:167} DagFileProcessor541 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:27,933] {jobs.py:1535} DagFileProcessor541 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:27,951] {jobs.py:1169} DagFileProcessor541 INFO - Processing hello_world
[2018-04-19 21:14:27,961] {jobs.py:566} DagFileProcessor541 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:27,967] {models.py:322} DagFileProcessor541 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:27,968] {models.py:328} DagFileProcessor541 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:27.967822
[2018-04-19 21:14:27,971] {jobs.py:351} DagFileProcessor541 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:14:29,040] {jobs.py:343} DagFileProcessor542 INFO - Started process (PID=4173) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:29,045] {jobs.py:534} DagFileProcessor542 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:29,046] {jobs.py:1521} DagFileProcessor542 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:29,047] {models.py:167} DagFileProcessor542 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:29,154] {jobs.py:1535} DagFileProcessor542 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:29,173] {jobs.py:1169} DagFileProcessor542 INFO - Processing hello_world
[2018-04-19 21:14:29,182] {jobs.py:566} DagFileProcessor542 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:29,188] {models.py:322} DagFileProcessor542 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:29,189] {models.py:328} DagFileProcessor542 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:29.189159
[2018-04-19 21:14:29,192] {jobs.py:351} DagFileProcessor542 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:14:30,273] {jobs.py:343} DagFileProcessor543 INFO - Started process (PID=4174) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:30,278] {jobs.py:534} DagFileProcessor543 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:30,279] {jobs.py:1521} DagFileProcessor543 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:30,279] {models.py:167} DagFileProcessor543 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:30,385] {jobs.py:1535} DagFileProcessor543 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:30,406] {jobs.py:1169} DagFileProcessor543 INFO - Processing hello_world
[2018-04-19 21:14:30,415] {jobs.py:566} DagFileProcessor543 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:30,420] {models.py:322} DagFileProcessor543 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:30,421] {models.py:328} DagFileProcessor543 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:30.421120
[2018-04-19 21:14:30,424] {jobs.py:351} DagFileProcessor543 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:14:31,507] {jobs.py:343} DagFileProcessor544 INFO - Started process (PID=4175) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:31,512] {jobs.py:534} DagFileProcessor544 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:31,514] {jobs.py:1521} DagFileProcessor544 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:31,514] {models.py:167} DagFileProcessor544 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:31,622] {jobs.py:1535} DagFileProcessor544 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:31,642] {jobs.py:1169} DagFileProcessor544 INFO - Processing hello_world
[2018-04-19 21:14:31,652] {jobs.py:566} DagFileProcessor544 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:31,659] {models.py:322} DagFileProcessor544 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:31,660] {models.py:328} DagFileProcessor544 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:31.659878
[2018-04-19 21:14:31,664] {jobs.py:351} DagFileProcessor544 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:14:32,740] {jobs.py:343} DagFileProcessor545 INFO - Started process (PID=4176) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:32,745] {jobs.py:534} DagFileProcessor545 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:32,746] {jobs.py:1521} DagFileProcessor545 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:32,746] {models.py:167} DagFileProcessor545 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:32,853] {jobs.py:1535} DagFileProcessor545 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:32,872] {jobs.py:1169} DagFileProcessor545 INFO - Processing hello_world
[2018-04-19 21:14:32,880] {jobs.py:566} DagFileProcessor545 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:32,886] {models.py:322} DagFileProcessor545 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:32,886] {models.py:328} DagFileProcessor545 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:32.886386
[2018-04-19 21:14:32,889] {jobs.py:351} DagFileProcessor545 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:14:33,974] {jobs.py:343} DagFileProcessor546 INFO - Started process (PID=4178) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:33,979] {jobs.py:534} DagFileProcessor546 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:33,980] {jobs.py:1521} DagFileProcessor546 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:33,980] {models.py:167} DagFileProcessor546 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:34,085] {jobs.py:1535} DagFileProcessor546 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:34,104] {jobs.py:1169} DagFileProcessor546 INFO - Processing hello_world
[2018-04-19 21:14:34,114] {jobs.py:566} DagFileProcessor546 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:34,120] {models.py:322} DagFileProcessor546 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:34,121] {models.py:328} DagFileProcessor546 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:34.120980
[2018-04-19 21:14:34,124] {jobs.py:351} DagFileProcessor546 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:14:35,204] {jobs.py:343} DagFileProcessor547 INFO - Started process (PID=4179) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:35,209] {jobs.py:534} DagFileProcessor547 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:35,210] {jobs.py:1521} DagFileProcessor547 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:35,210] {models.py:167} DagFileProcessor547 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:35,322] {jobs.py:1535} DagFileProcessor547 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:35,341] {jobs.py:1169} DagFileProcessor547 INFO - Processing hello_world
[2018-04-19 21:14:35,351] {jobs.py:566} DagFileProcessor547 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:35,357] {models.py:322} DagFileProcessor547 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:35,357] {models.py:328} DagFileProcessor547 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:35.357552
[2018-04-19 21:14:35,361] {jobs.py:351} DagFileProcessor547 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:14:36,431] {jobs.py:343} DagFileProcessor548 INFO - Started process (PID=4180) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:36,436] {jobs.py:534} DagFileProcessor548 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:36,437] {jobs.py:1521} DagFileProcessor548 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:36,438] {models.py:167} DagFileProcessor548 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:36,545] {jobs.py:1535} DagFileProcessor548 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:36,561] {jobs.py:1169} DagFileProcessor548 INFO - Processing hello_world
[2018-04-19 21:14:36,570] {jobs.py:566} DagFileProcessor548 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:36,575] {models.py:322} DagFileProcessor548 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:36,576] {models.py:328} DagFileProcessor548 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:36.575914
[2018-04-19 21:14:36,581] {jobs.py:351} DagFileProcessor548 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:14:37,657] {jobs.py:343} DagFileProcessor549 INFO - Started process (PID=4181) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:37,662] {jobs.py:534} DagFileProcessor549 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:37,663] {jobs.py:1521} DagFileProcessor549 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:37,663] {models.py:167} DagFileProcessor549 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:37,785] {jobs.py:1535} DagFileProcessor549 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:37,806] {jobs.py:1169} DagFileProcessor549 INFO - Processing hello_world
[2018-04-19 21:14:37,817] {jobs.py:566} DagFileProcessor549 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:37,824] {models.py:322} DagFileProcessor549 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:37,824] {models.py:328} DagFileProcessor549 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:37.824672
[2018-04-19 21:14:37,829] {jobs.py:351} DagFileProcessor549 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.172 seconds
[2018-04-19 21:14:38,882] {jobs.py:343} DagFileProcessor550 INFO - Started process (PID=4182) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:38,887] {jobs.py:534} DagFileProcessor550 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:38,888] {jobs.py:1521} DagFileProcessor550 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:38,888] {models.py:167} DagFileProcessor550 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:38,998] {jobs.py:1535} DagFileProcessor550 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:39,018] {jobs.py:1169} DagFileProcessor550 INFO - Processing hello_world
[2018-04-19 21:14:39,028] {jobs.py:566} DagFileProcessor550 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:39,033] {models.py:322} DagFileProcessor550 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:39,034] {models.py:328} DagFileProcessor550 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:39.034212
[2018-04-19 21:14:39,037] {jobs.py:351} DagFileProcessor550 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:14:40,116] {jobs.py:343} DagFileProcessor551 INFO - Started process (PID=4183) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:40,122] {jobs.py:534} DagFileProcessor551 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:40,123] {jobs.py:1521} DagFileProcessor551 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:40,123] {models.py:167} DagFileProcessor551 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:40,236] {jobs.py:1535} DagFileProcessor551 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:40,257] {jobs.py:1169} DagFileProcessor551 INFO - Processing hello_world
[2018-04-19 21:14:40,266] {jobs.py:566} DagFileProcessor551 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:40,271] {models.py:322} DagFileProcessor551 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:40,272] {models.py:328} DagFileProcessor551 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:40.272075
[2018-04-19 21:14:40,275] {jobs.py:351} DagFileProcessor551 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.160 seconds
[2018-04-19 21:14:41,339] {jobs.py:343} DagFileProcessor552 INFO - Started process (PID=4184) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:41,344] {jobs.py:534} DagFileProcessor552 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:41,345] {jobs.py:1521} DagFileProcessor552 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:41,345] {models.py:167} DagFileProcessor552 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:41,473] {jobs.py:1535} DagFileProcessor552 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:41,495] {jobs.py:1169} DagFileProcessor552 INFO - Processing hello_world
[2018-04-19 21:14:41,505] {jobs.py:566} DagFileProcessor552 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:41,512] {models.py:322} DagFileProcessor552 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:41,513] {models.py:328} DagFileProcessor552 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:41.513132
[2018-04-19 21:14:41,517] {jobs.py:351} DagFileProcessor552 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.178 seconds
[2018-04-19 21:14:42,565] {jobs.py:343} DagFileProcessor553 INFO - Started process (PID=4185) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:42,570] {jobs.py:534} DagFileProcessor553 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:42,572] {jobs.py:1521} DagFileProcessor553 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:42,572] {models.py:167} DagFileProcessor553 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:42,683] {jobs.py:1535} DagFileProcessor553 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:42,703] {jobs.py:1169} DagFileProcessor553 INFO - Processing hello_world
[2018-04-19 21:14:42,712] {jobs.py:566} DagFileProcessor553 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:42,719] {models.py:322} DagFileProcessor553 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:42,719] {models.py:328} DagFileProcessor553 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:42.719462
[2018-04-19 21:14:42,723] {jobs.py:351} DagFileProcessor553 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.158 seconds
[2018-04-19 21:14:43,787] {jobs.py:343} DagFileProcessor554 INFO - Started process (PID=4187) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:43,791] {jobs.py:534} DagFileProcessor554 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:43,792] {jobs.py:1521} DagFileProcessor554 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:43,793] {models.py:167} DagFileProcessor554 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:43,909] {jobs.py:1535} DagFileProcessor554 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:43,928] {jobs.py:1169} DagFileProcessor554 INFO - Processing hello_world
[2018-04-19 21:14:43,939] {jobs.py:566} DagFileProcessor554 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:43,947] {models.py:322} DagFileProcessor554 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:43,947] {models.py:328} DagFileProcessor554 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:43.947576
[2018-04-19 21:14:43,952] {jobs.py:351} DagFileProcessor554 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.166 seconds
[2018-04-19 21:14:45,009] {jobs.py:343} DagFileProcessor555 INFO - Started process (PID=4188) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:45,015] {jobs.py:534} DagFileProcessor555 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:45,016] {jobs.py:1521} DagFileProcessor555 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:45,016] {models.py:167} DagFileProcessor555 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:45,126] {jobs.py:1535} DagFileProcessor555 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:45,149] {jobs.py:1169} DagFileProcessor555 INFO - Processing hello_world
[2018-04-19 21:14:45,158] {jobs.py:566} DagFileProcessor555 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:45,164] {models.py:322} DagFileProcessor555 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:45,164] {models.py:328} DagFileProcessor555 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:45.164407
[2018-04-19 21:14:45,168] {jobs.py:351} DagFileProcessor555 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.158 seconds
[2018-04-19 21:14:46,246] {jobs.py:343} DagFileProcessor556 INFO - Started process (PID=4189) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:46,251] {jobs.py:534} DagFileProcessor556 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:46,252] {jobs.py:1521} DagFileProcessor556 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:46,252] {models.py:167} DagFileProcessor556 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:46,365] {jobs.py:1535} DagFileProcessor556 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:46,385] {jobs.py:1169} DagFileProcessor556 INFO - Processing hello_world
[2018-04-19 21:14:46,394] {jobs.py:566} DagFileProcessor556 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:46,400] {models.py:322} DagFileProcessor556 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:46,400] {models.py:328} DagFileProcessor556 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:46.400592
[2018-04-19 21:14:46,404] {jobs.py:351} DagFileProcessor556 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.158 seconds
[2018-04-19 21:14:47,470] {jobs.py:343} DagFileProcessor557 INFO - Started process (PID=4190) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:47,476] {jobs.py:534} DagFileProcessor557 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:47,477] {jobs.py:1521} DagFileProcessor557 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:47,477] {models.py:167} DagFileProcessor557 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:47,585] {jobs.py:1535} DagFileProcessor557 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:47,608] {jobs.py:1169} DagFileProcessor557 INFO - Processing hello_world
[2018-04-19 21:14:47,619] {jobs.py:566} DagFileProcessor557 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:47,624] {models.py:322} DagFileProcessor557 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:47,624] {models.py:328} DagFileProcessor557 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:47.624587
[2018-04-19 21:14:47,628] {jobs.py:351} DagFileProcessor557 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:14:48,701] {jobs.py:343} DagFileProcessor558 INFO - Started process (PID=4191) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:48,706] {jobs.py:534} DagFileProcessor558 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:48,707] {jobs.py:1521} DagFileProcessor558 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:48,708] {models.py:167} DagFileProcessor558 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:48,817] {jobs.py:1535} DagFileProcessor558 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:48,836] {jobs.py:1169} DagFileProcessor558 INFO - Processing hello_world
[2018-04-19 21:14:48,844] {jobs.py:566} DagFileProcessor558 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:48,851] {models.py:322} DagFileProcessor558 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:48,852] {models.py:328} DagFileProcessor558 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:48.851893
[2018-04-19 21:14:48,855] {jobs.py:351} DagFileProcessor558 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:14:49,933] {jobs.py:343} DagFileProcessor559 INFO - Started process (PID=4192) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:49,938] {jobs.py:534} DagFileProcessor559 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:49,939] {jobs.py:1521} DagFileProcessor559 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:49,940] {models.py:167} DagFileProcessor559 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:50,049] {jobs.py:1535} DagFileProcessor559 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:50,068] {jobs.py:1169} DagFileProcessor559 INFO - Processing hello_world
[2018-04-19 21:14:50,077] {jobs.py:566} DagFileProcessor559 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:50,084] {models.py:322} DagFileProcessor559 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:50,084] {models.py:328} DagFileProcessor559 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:50.084663
[2018-04-19 21:14:50,088] {jobs.py:351} DagFileProcessor559 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.155 seconds
[2018-04-19 21:14:51,158] {jobs.py:343} DagFileProcessor560 INFO - Started process (PID=4194) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:51,163] {jobs.py:534} DagFileProcessor560 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:51,164] {jobs.py:1521} DagFileProcessor560 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:51,165] {models.py:167} DagFileProcessor560 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:51,272] {jobs.py:1535} DagFileProcessor560 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:51,293] {jobs.py:1169} DagFileProcessor560 INFO - Processing hello_world
[2018-04-19 21:14:51,302] {jobs.py:566} DagFileProcessor560 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:51,308] {models.py:322} DagFileProcessor560 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:51,308] {models.py:328} DagFileProcessor560 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:51.308379
[2018-04-19 21:14:51,311] {jobs.py:351} DagFileProcessor560 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:14:52,386] {jobs.py:343} DagFileProcessor561 INFO - Started process (PID=4195) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:52,391] {jobs.py:534} DagFileProcessor561 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:52,392] {jobs.py:1521} DagFileProcessor561 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:52,393] {models.py:167} DagFileProcessor561 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:52,498] {jobs.py:1535} DagFileProcessor561 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:52,517] {jobs.py:1169} DagFileProcessor561 INFO - Processing hello_world
[2018-04-19 21:14:52,527] {jobs.py:566} DagFileProcessor561 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:52,533] {models.py:322} DagFileProcessor561 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:52,534] {models.py:328} DagFileProcessor561 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:52.534072
[2018-04-19 21:14:52,538] {jobs.py:351} DagFileProcessor561 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:14:53,617] {jobs.py:343} DagFileProcessor562 INFO - Started process (PID=4197) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:53,622] {jobs.py:534} DagFileProcessor562 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:53,623] {jobs.py:1521} DagFileProcessor562 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:53,623] {models.py:167} DagFileProcessor562 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:53,731] {jobs.py:1535} DagFileProcessor562 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:53,749] {jobs.py:1169} DagFileProcessor562 INFO - Processing hello_world
[2018-04-19 21:14:53,758] {jobs.py:566} DagFileProcessor562 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:53,765] {models.py:322} DagFileProcessor562 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:53,766] {models.py:328} DagFileProcessor562 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:53.766096
[2018-04-19 21:14:53,771] {jobs.py:351} DagFileProcessor562 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:14:54,843] {jobs.py:343} DagFileProcessor563 INFO - Started process (PID=4198) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:54,848] {jobs.py:534} DagFileProcessor563 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:54,849] {jobs.py:1521} DagFileProcessor563 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:54,849] {models.py:167} DagFileProcessor563 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:54,959] {jobs.py:1535} DagFileProcessor563 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:54,979] {jobs.py:1169} DagFileProcessor563 INFO - Processing hello_world
[2018-04-19 21:14:54,988] {jobs.py:566} DagFileProcessor563 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:54,993] {models.py:322} DagFileProcessor563 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:54,994] {models.py:328} DagFileProcessor563 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:54.994196
[2018-04-19 21:14:54,998] {jobs.py:351} DagFileProcessor563 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.155 seconds
[2018-04-19 21:14:56,074] {jobs.py:343} DagFileProcessor564 INFO - Started process (PID=4199) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:56,080] {jobs.py:534} DagFileProcessor564 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:56,081] {jobs.py:1521} DagFileProcessor564 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:56,082] {models.py:167} DagFileProcessor564 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:56,204] {jobs.py:1535} DagFileProcessor564 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:56,231] {jobs.py:1169} DagFileProcessor564 INFO - Processing hello_world
[2018-04-19 21:14:56,240] {jobs.py:566} DagFileProcessor564 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:56,245] {models.py:322} DagFileProcessor564 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:56,246] {models.py:328} DagFileProcessor564 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:56.246226
[2018-04-19 21:14:56,250] {jobs.py:351} DagFileProcessor564 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.176 seconds
[2018-04-19 21:14:57,307] {jobs.py:343} DagFileProcessor565 INFO - Started process (PID=4200) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:57,312] {jobs.py:534} DagFileProcessor565 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:57,313] {jobs.py:1521} DagFileProcessor565 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:57,314] {models.py:167} DagFileProcessor565 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:57,421] {jobs.py:1535} DagFileProcessor565 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:57,440] {jobs.py:1169} DagFileProcessor565 INFO - Processing hello_world
[2018-04-19 21:14:57,449] {jobs.py:566} DagFileProcessor565 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:57,455] {models.py:322} DagFileProcessor565 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:57,455] {models.py:328} DagFileProcessor565 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:57.455702
[2018-04-19 21:14:57,459] {jobs.py:351} DagFileProcessor565 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:14:58,537] {jobs.py:343} DagFileProcessor566 INFO - Started process (PID=4201) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:58,541] {jobs.py:534} DagFileProcessor566 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:58,543] {jobs.py:1521} DagFileProcessor566 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:58,543] {models.py:167} DagFileProcessor566 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:58,651] {jobs.py:1535} DagFileProcessor566 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:58,668] {jobs.py:1169} DagFileProcessor566 INFO - Processing hello_world
[2018-04-19 21:14:58,677] {jobs.py:566} DagFileProcessor566 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:58,684] {models.py:322} DagFileProcessor566 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:58,684] {models.py:328} DagFileProcessor566 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:58.684434
[2018-04-19 21:14:58,688] {jobs.py:351} DagFileProcessor566 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:14:59,766] {jobs.py:343} DagFileProcessor567 INFO - Started process (PID=4202) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:59,771] {jobs.py:534} DagFileProcessor567 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:14:59,772] {jobs.py:1521} DagFileProcessor567 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:14:59,772] {models.py:167} DagFileProcessor567 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:59,879] {jobs.py:1535} DagFileProcessor567 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:14:59,901] {jobs.py:1169} DagFileProcessor567 INFO - Processing hello_world
[2018-04-19 21:14:59,909] {jobs.py:566} DagFileProcessor567 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:14:59,915] {models.py:322} DagFileProcessor567 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:14:59,916] {models.py:328} DagFileProcessor567 INFO - Failing jobs without heartbeat after 2018-04-19 21:09:59.916219
[2018-04-19 21:14:59,920] {jobs.py:351} DagFileProcessor567 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:15:00,990] {jobs.py:343} DagFileProcessor568 INFO - Started process (PID=4203) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:00,995] {jobs.py:534} DagFileProcessor568 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:00,996] {jobs.py:1521} DagFileProcessor568 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:00,996] {models.py:167} DagFileProcessor568 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:01,104] {jobs.py:1535} DagFileProcessor568 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:01,164] {jobs.py:1169} DagFileProcessor568 INFO - Processing hello_world
[2018-04-19 21:15:01,172] {jobs.py:566} DagFileProcessor568 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:01,177] {models.py:322} DagFileProcessor568 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:01,178] {models.py:328} DagFileProcessor568 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:01.178123
[2018-04-19 21:15:01,182] {jobs.py:351} DagFileProcessor568 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.192 seconds
[2018-04-19 21:15:02,210] {jobs.py:343} DagFileProcessor569 INFO - Started process (PID=4204) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:02,215] {jobs.py:534} DagFileProcessor569 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:02,216] {jobs.py:1521} DagFileProcessor569 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:02,216] {models.py:167} DagFileProcessor569 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:02,322] {jobs.py:1535} DagFileProcessor569 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:02,342] {jobs.py:1169} DagFileProcessor569 INFO - Processing hello_world
[2018-04-19 21:15:02,351] {jobs.py:566} DagFileProcessor569 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:02,357] {models.py:322} DagFileProcessor569 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:02,357] {models.py:328} DagFileProcessor569 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:02.357420
[2018-04-19 21:15:02,361] {jobs.py:351} DagFileProcessor569 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:15:03,432] {jobs.py:343} DagFileProcessor570 INFO - Started process (PID=4206) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:03,437] {jobs.py:534} DagFileProcessor570 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:03,438] {jobs.py:1521} DagFileProcessor570 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:03,439] {models.py:167} DagFileProcessor570 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:03,543] {jobs.py:1535} DagFileProcessor570 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:03,562] {jobs.py:1169} DagFileProcessor570 INFO - Processing hello_world
[2018-04-19 21:15:03,570] {jobs.py:566} DagFileProcessor570 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:03,576] {models.py:322} DagFileProcessor570 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:03,576] {models.py:328} DagFileProcessor570 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:03.576576
[2018-04-19 21:15:03,580] {jobs.py:351} DagFileProcessor570 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:15:04,670] {jobs.py:343} DagFileProcessor571 INFO - Started process (PID=4207) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:04,675] {jobs.py:534} DagFileProcessor571 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:04,676] {jobs.py:1521} DagFileProcessor571 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:04,676] {models.py:167} DagFileProcessor571 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:04,782] {jobs.py:1535} DagFileProcessor571 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:04,803] {jobs.py:1169} DagFileProcessor571 INFO - Processing hello_world
[2018-04-19 21:15:04,812] {jobs.py:566} DagFileProcessor571 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:04,818] {models.py:322} DagFileProcessor571 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:04,819] {models.py:328} DagFileProcessor571 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:04.819037
[2018-04-19 21:15:04,823] {jobs.py:351} DagFileProcessor571 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:15:05,895] {jobs.py:343} DagFileProcessor572 INFO - Started process (PID=4208) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:05,900] {jobs.py:534} DagFileProcessor572 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:05,901] {jobs.py:1521} DagFileProcessor572 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:05,902] {models.py:167} DagFileProcessor572 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:06,013] {jobs.py:1535} DagFileProcessor572 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:06,035] {jobs.py:1169} DagFileProcessor572 INFO - Processing hello_world
[2018-04-19 21:15:06,045] {jobs.py:566} DagFileProcessor572 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:06,053] {models.py:322} DagFileProcessor572 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:06,053] {models.py:328} DagFileProcessor572 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:06.053688
[2018-04-19 21:15:06,058] {jobs.py:351} DagFileProcessor572 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.163 seconds
[2018-04-19 21:15:07,124] {jobs.py:343} DagFileProcessor573 INFO - Started process (PID=4209) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:07,129] {jobs.py:534} DagFileProcessor573 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:07,130] {jobs.py:1521} DagFileProcessor573 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:07,130] {models.py:167} DagFileProcessor573 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:07,237] {jobs.py:1535} DagFileProcessor573 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:07,258] {jobs.py:1169} DagFileProcessor573 INFO - Processing hello_world
[2018-04-19 21:15:07,268] {jobs.py:566} DagFileProcessor573 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:07,274] {models.py:322} DagFileProcessor573 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:07,275] {models.py:328} DagFileProcessor573 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:07.274996
[2018-04-19 21:15:07,278] {jobs.py:351} DagFileProcessor573 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:15:08,354] {jobs.py:343} DagFileProcessor574 INFO - Started process (PID=4210) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:08,359] {jobs.py:534} DagFileProcessor574 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:08,360] {jobs.py:1521} DagFileProcessor574 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:08,360] {models.py:167} DagFileProcessor574 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:08,467] {jobs.py:1535} DagFileProcessor574 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:08,487] {jobs.py:1169} DagFileProcessor574 INFO - Processing hello_world
[2018-04-19 21:15:08,496] {jobs.py:566} DagFileProcessor574 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:08,502] {models.py:322} DagFileProcessor574 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:08,503] {models.py:328} DagFileProcessor574 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:08.503422
[2018-04-19 21:15:08,507] {jobs.py:351} DagFileProcessor574 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:15:09,578] {jobs.py:343} DagFileProcessor575 INFO - Started process (PID=4211) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:09,583] {jobs.py:534} DagFileProcessor575 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:09,584] {jobs.py:1521} DagFileProcessor575 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:09,585] {models.py:167} DagFileProcessor575 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:09,693] {jobs.py:1535} DagFileProcessor575 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:09,714] {jobs.py:1169} DagFileProcessor575 INFO - Processing hello_world
[2018-04-19 21:15:09,723] {jobs.py:566} DagFileProcessor575 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:09,729] {models.py:322} DagFileProcessor575 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:09,729] {models.py:328} DagFileProcessor575 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:09.729803
[2018-04-19 21:15:09,733] {jobs.py:351} DagFileProcessor575 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.155 seconds
[2018-04-19 21:15:10,821] {jobs.py:343} DagFileProcessor576 INFO - Started process (PID=4212) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:10,826] {jobs.py:534} DagFileProcessor576 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:10,827] {jobs.py:1521} DagFileProcessor576 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:10,827] {models.py:167} DagFileProcessor576 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:10,938] {jobs.py:1535} DagFileProcessor576 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:10,955] {jobs.py:1169} DagFileProcessor576 INFO - Processing hello_world
[2018-04-19 21:15:10,965] {jobs.py:566} DagFileProcessor576 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:10,971] {models.py:322} DagFileProcessor576 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:10,971] {models.py:328} DagFileProcessor576 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:10.971372
[2018-04-19 21:15:10,975] {jobs.py:351} DagFileProcessor576 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:15:12,041] {jobs.py:343} DagFileProcessor577 INFO - Started process (PID=4213) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:12,046] {jobs.py:534} DagFileProcessor577 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:12,047] {jobs.py:1521} DagFileProcessor577 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:12,048] {models.py:167} DagFileProcessor577 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:12,159] {jobs.py:1535} DagFileProcessor577 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:12,178] {jobs.py:1169} DagFileProcessor577 INFO - Processing hello_world
[2018-04-19 21:15:12,187] {jobs.py:566} DagFileProcessor577 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:12,194] {models.py:322} DagFileProcessor577 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:12,195] {models.py:328} DagFileProcessor577 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:12.195233
[2018-04-19 21:15:12,199] {jobs.py:351} DagFileProcessor577 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:15:13,271] {jobs.py:343} DagFileProcessor578 INFO - Started process (PID=4214) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:13,276] {jobs.py:534} DagFileProcessor578 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:13,277] {jobs.py:1521} DagFileProcessor578 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:13,278] {models.py:167} DagFileProcessor578 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:13,383] {jobs.py:1535} DagFileProcessor578 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:13,404] {jobs.py:1169} DagFileProcessor578 INFO - Processing hello_world
[2018-04-19 21:15:13,415] {jobs.py:566} DagFileProcessor578 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:13,422] {models.py:322} DagFileProcessor578 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:13,423] {models.py:328} DagFileProcessor578 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:13.423208
[2018-04-19 21:15:13,428] {jobs.py:351} DagFileProcessor578 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:15:14,497] {jobs.py:343} DagFileProcessor579 INFO - Started process (PID=4218) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:14,502] {jobs.py:534} DagFileProcessor579 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:14,504] {jobs.py:1521} DagFileProcessor579 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:14,504] {models.py:167} DagFileProcessor579 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:14,618] {jobs.py:1535} DagFileProcessor579 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:14,639] {jobs.py:1169} DagFileProcessor579 INFO - Processing hello_world
[2018-04-19 21:15:14,648] {jobs.py:566} DagFileProcessor579 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:14,655] {models.py:322} DagFileProcessor579 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:14,656] {models.py:328} DagFileProcessor579 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:14.656084
[2018-04-19 21:15:14,660] {jobs.py:351} DagFileProcessor579 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.163 seconds
[2018-04-19 21:15:15,723] {jobs.py:343} DagFileProcessor580 INFO - Started process (PID=4219) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:15,729] {jobs.py:534} DagFileProcessor580 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:15,730] {jobs.py:1521} DagFileProcessor580 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:15,730] {models.py:167} DagFileProcessor580 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:15,852] {jobs.py:1535} DagFileProcessor580 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:15,874] {jobs.py:1169} DagFileProcessor580 INFO - Processing hello_world
[2018-04-19 21:15:15,887] {jobs.py:566} DagFileProcessor580 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:15,893] {models.py:322} DagFileProcessor580 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:15,893] {models.py:328} DagFileProcessor580 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:15.893604
[2018-04-19 21:15:15,897] {jobs.py:351} DagFileProcessor580 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.174 seconds
[2018-04-19 21:15:16,962] {jobs.py:343} DagFileProcessor581 INFO - Started process (PID=4220) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:16,967] {jobs.py:534} DagFileProcessor581 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:16,968] {jobs.py:1521} DagFileProcessor581 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:16,968] {models.py:167} DagFileProcessor581 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:17,077] {jobs.py:1535} DagFileProcessor581 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:17,100] {jobs.py:1169} DagFileProcessor581 INFO - Processing hello_world
[2018-04-19 21:15:17,109] {jobs.py:566} DagFileProcessor581 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:17,114] {models.py:322} DagFileProcessor581 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:17,114] {models.py:328} DagFileProcessor581 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:17.114634
[2018-04-19 21:15:17,118] {jobs.py:351} DagFileProcessor581 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:15:18,188] {jobs.py:343} DagFileProcessor582 INFO - Started process (PID=4221) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:18,193] {jobs.py:534} DagFileProcessor582 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:18,194] {jobs.py:1521} DagFileProcessor582 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:18,195] {models.py:167} DagFileProcessor582 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:18,298] {jobs.py:1535} DagFileProcessor582 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:18,320] {jobs.py:1169} DagFileProcessor582 INFO - Processing hello_world
[2018-04-19 21:15:18,328] {jobs.py:566} DagFileProcessor582 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:18,333] {models.py:322} DagFileProcessor582 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:18,334] {models.py:328} DagFileProcessor582 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:18.333840
[2018-04-19 21:15:18,337] {jobs.py:351} DagFileProcessor582 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:15:19,418] {jobs.py:343} DagFileProcessor583 INFO - Started process (PID=4222) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:19,425] {jobs.py:534} DagFileProcessor583 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:19,426] {jobs.py:1521} DagFileProcessor583 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:19,427] {models.py:167} DagFileProcessor583 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:19,546] {jobs.py:1535} DagFileProcessor583 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:19,569] {jobs.py:1169} DagFileProcessor583 INFO - Processing hello_world
[2018-04-19 21:15:19,579] {jobs.py:566} DagFileProcessor583 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:19,585] {models.py:322} DagFileProcessor583 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:19,586] {models.py:328} DagFileProcessor583 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:19.586176
[2018-04-19 21:15:19,590] {jobs.py:351} DagFileProcessor583 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.172 seconds
[2018-04-19 21:15:20,648] {jobs.py:343} DagFileProcessor584 INFO - Started process (PID=4223) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:20,653] {jobs.py:534} DagFileProcessor584 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:20,654] {jobs.py:1521} DagFileProcessor584 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:20,654] {models.py:167} DagFileProcessor584 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:20,757] {jobs.py:1535} DagFileProcessor584 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:20,777] {jobs.py:1169} DagFileProcessor584 INFO - Processing hello_world
[2018-04-19 21:15:20,786] {jobs.py:566} DagFileProcessor584 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:20,791] {models.py:322} DagFileProcessor584 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:20,792] {models.py:328} DagFileProcessor584 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:20.791981
[2018-04-19 21:15:20,795] {jobs.py:351} DagFileProcessor584 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:15:21,884] {jobs.py:343} DagFileProcessor585 INFO - Started process (PID=4231) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:21,889] {jobs.py:534} DagFileProcessor585 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:21,890] {jobs.py:1521} DagFileProcessor585 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:21,891] {models.py:167} DagFileProcessor585 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:22,017] {jobs.py:1535} DagFileProcessor585 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:22,036] {jobs.py:1169} DagFileProcessor585 INFO - Processing hello_world
[2018-04-19 21:15:22,045] {jobs.py:566} DagFileProcessor585 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:22,050] {models.py:322} DagFileProcessor585 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:22,051] {models.py:328} DagFileProcessor585 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:22.051230
[2018-04-19 21:15:22,054] {jobs.py:351} DagFileProcessor585 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.170 seconds
[2018-04-19 21:15:23,111] {jobs.py:343} DagFileProcessor586 INFO - Started process (PID=4232) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:23,116] {jobs.py:534} DagFileProcessor586 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:23,117] {jobs.py:1521} DagFileProcessor586 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:23,118] {models.py:167} DagFileProcessor586 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:23,223] {jobs.py:1535} DagFileProcessor586 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:23,245] {jobs.py:1169} DagFileProcessor586 INFO - Processing hello_world
[2018-04-19 21:15:23,253] {jobs.py:566} DagFileProcessor586 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:23,258] {models.py:322} DagFileProcessor586 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:23,259] {models.py:328} DagFileProcessor586 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:23.259110
[2018-04-19 21:15:23,262] {jobs.py:351} DagFileProcessor586 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:15:24,346] {jobs.py:343} DagFileProcessor587 INFO - Started process (PID=4234) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:24,351] {jobs.py:534} DagFileProcessor587 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:24,352] {jobs.py:1521} DagFileProcessor587 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:24,352] {models.py:167} DagFileProcessor587 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:24,456] {jobs.py:1535} DagFileProcessor587 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:24,474] {jobs.py:1169} DagFileProcessor587 INFO - Processing hello_world
[2018-04-19 21:15:24,483] {jobs.py:566} DagFileProcessor587 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:24,488] {models.py:322} DagFileProcessor587 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:24,489] {models.py:328} DagFileProcessor587 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:24.488866
[2018-04-19 21:15:24,492] {jobs.py:351} DagFileProcessor587 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:15:25,570] {jobs.py:343} DagFileProcessor588 INFO - Started process (PID=4235) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:25,575] {jobs.py:534} DagFileProcessor588 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:25,576] {jobs.py:1521} DagFileProcessor588 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:25,577] {models.py:167} DagFileProcessor588 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:25,679] {jobs.py:1535} DagFileProcessor588 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:25,697] {jobs.py:1169} DagFileProcessor588 INFO - Processing hello_world
[2018-04-19 21:15:25,706] {jobs.py:566} DagFileProcessor588 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:25,711] {models.py:322} DagFileProcessor588 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:25,711] {models.py:328} DagFileProcessor588 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:25.711506
[2018-04-19 21:15:25,714] {jobs.py:351} DagFileProcessor588 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:15:26,805] {jobs.py:343} DagFileProcessor589 INFO - Started process (PID=4236) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:26,810] {jobs.py:534} DagFileProcessor589 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:26,811] {jobs.py:1521} DagFileProcessor589 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:26,812] {models.py:167} DagFileProcessor589 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:26,915] {jobs.py:1535} DagFileProcessor589 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:26,933] {jobs.py:1169} DagFileProcessor589 INFO - Processing hello_world
[2018-04-19 21:15:26,942] {jobs.py:566} DagFileProcessor589 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:26,947] {models.py:322} DagFileProcessor589 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:26,948] {models.py:328} DagFileProcessor589 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:26.948064
[2018-04-19 21:15:26,951] {jobs.py:351} DagFileProcessor589 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:15:28,034] {jobs.py:343} DagFileProcessor590 INFO - Started process (PID=4237) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:28,039] {jobs.py:534} DagFileProcessor590 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:28,040] {jobs.py:1521} DagFileProcessor590 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:28,040] {models.py:167} DagFileProcessor590 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:28,144] {jobs.py:1535} DagFileProcessor590 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:28,165] {jobs.py:1169} DagFileProcessor590 INFO - Processing hello_world
[2018-04-19 21:15:28,172] {jobs.py:566} DagFileProcessor590 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:28,178] {models.py:322} DagFileProcessor590 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:28,178] {models.py:328} DagFileProcessor590 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:28.178696
[2018-04-19 21:15:28,182] {jobs.py:351} DagFileProcessor590 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:15:29,270] {jobs.py:343} DagFileProcessor591 INFO - Started process (PID=4238) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:29,275] {jobs.py:534} DagFileProcessor591 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:29,276] {jobs.py:1521} DagFileProcessor591 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:29,277] {models.py:167} DagFileProcessor591 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:29,380] {jobs.py:1535} DagFileProcessor591 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:29,399] {jobs.py:1169} DagFileProcessor591 INFO - Processing hello_world
[2018-04-19 21:15:29,408] {jobs.py:566} DagFileProcessor591 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:29,413] {models.py:322} DagFileProcessor591 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:29,413] {models.py:328} DagFileProcessor591 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:29.413587
[2018-04-19 21:15:29,417] {jobs.py:351} DagFileProcessor591 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:15:30,501] {jobs.py:343} DagFileProcessor592 INFO - Started process (PID=4239) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:30,508] {jobs.py:534} DagFileProcessor592 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:30,510] {jobs.py:1521} DagFileProcessor592 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:30,510] {models.py:167} DagFileProcessor592 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:30,638] {jobs.py:1535} DagFileProcessor592 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:30,663] {jobs.py:1169} DagFileProcessor592 INFO - Processing hello_world
[2018-04-19 21:15:30,673] {jobs.py:566} DagFileProcessor592 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:30,680] {models.py:322} DagFileProcessor592 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:30,680] {models.py:328} DagFileProcessor592 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:30.680623
[2018-04-19 21:15:30,684] {jobs.py:351} DagFileProcessor592 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.183 seconds
[2018-04-19 21:15:31,733] {jobs.py:343} DagFileProcessor593 INFO - Started process (PID=4240) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:31,738] {jobs.py:534} DagFileProcessor593 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:31,740] {jobs.py:1521} DagFileProcessor593 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:31,740] {models.py:167} DagFileProcessor593 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:31,847] {jobs.py:1535} DagFileProcessor593 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:31,867] {jobs.py:1169} DagFileProcessor593 INFO - Processing hello_world
[2018-04-19 21:15:31,876] {jobs.py:566} DagFileProcessor593 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:31,882] {models.py:322} DagFileProcessor593 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:31,883] {models.py:328} DagFileProcessor593 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:31.882971
[2018-04-19 21:15:31,886] {jobs.py:351} DagFileProcessor593 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:15:32,959] {jobs.py:343} DagFileProcessor594 INFO - Started process (PID=4241) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:32,964] {jobs.py:534} DagFileProcessor594 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:32,965] {jobs.py:1521} DagFileProcessor594 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:32,966] {models.py:167} DagFileProcessor594 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:33,088] {jobs.py:1535} DagFileProcessor594 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:33,108] {jobs.py:1169} DagFileProcessor594 INFO - Processing hello_world
[2018-04-19 21:15:33,117] {jobs.py:566} DagFileProcessor594 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:33,123] {models.py:322} DagFileProcessor594 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:33,123] {models.py:328} DagFileProcessor594 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:33.123771
[2018-04-19 21:15:33,127] {jobs.py:351} DagFileProcessor594 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.168 seconds
[2018-04-19 21:15:34,187] {jobs.py:343} DagFileProcessor595 INFO - Started process (PID=4243) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:34,192] {jobs.py:534} DagFileProcessor595 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:34,193] {jobs.py:1521} DagFileProcessor595 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:34,194] {models.py:167} DagFileProcessor595 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:34,304] {jobs.py:1535} DagFileProcessor595 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:34,324] {jobs.py:1169} DagFileProcessor595 INFO - Processing hello_world
[2018-04-19 21:15:34,333] {jobs.py:566} DagFileProcessor595 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:34,339] {models.py:322} DagFileProcessor595 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:34,340] {models.py:328} DagFileProcessor595 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:34.339938
[2018-04-19 21:15:34,343] {jobs.py:351} DagFileProcessor595 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:15:35,431] {jobs.py:343} DagFileProcessor596 INFO - Started process (PID=4244) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:35,441] {jobs.py:534} DagFileProcessor596 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:35,444] {jobs.py:1521} DagFileProcessor596 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:35,444] {models.py:167} DagFileProcessor596 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:35,563] {jobs.py:1535} DagFileProcessor596 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:35,584] {jobs.py:1169} DagFileProcessor596 INFO - Processing hello_world
[2018-04-19 21:15:35,595] {jobs.py:566} DagFileProcessor596 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:35,600] {models.py:322} DagFileProcessor596 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:35,601] {models.py:328} DagFileProcessor596 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:35.601244
[2018-04-19 21:15:35,605] {jobs.py:351} DagFileProcessor596 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.174 seconds
[2018-04-19 21:15:36,652] {jobs.py:343} DagFileProcessor597 INFO - Started process (PID=4245) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:36,658] {jobs.py:534} DagFileProcessor597 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:36,659] {jobs.py:1521} DagFileProcessor597 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:36,660] {models.py:167} DagFileProcessor597 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:36,787] {jobs.py:1535} DagFileProcessor597 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:36,811] {jobs.py:1169} DagFileProcessor597 INFO - Processing hello_world
[2018-04-19 21:15:36,822] {jobs.py:566} DagFileProcessor597 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:36,828] {models.py:322} DagFileProcessor597 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:36,829] {models.py:328} DagFileProcessor597 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:36.829053
[2018-04-19 21:15:36,832] {jobs.py:351} DagFileProcessor597 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.181 seconds
[2018-04-19 21:15:37,883] {jobs.py:343} DagFileProcessor598 INFO - Started process (PID=4246) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:37,888] {jobs.py:534} DagFileProcessor598 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:37,889] {jobs.py:1521} DagFileProcessor598 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:37,890] {models.py:167} DagFileProcessor598 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:37,998] {jobs.py:1535} DagFileProcessor598 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:38,019] {jobs.py:1169} DagFileProcessor598 INFO - Processing hello_world
[2018-04-19 21:15:38,028] {jobs.py:566} DagFileProcessor598 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:38,034] {models.py:322} DagFileProcessor598 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:38,034] {models.py:328} DagFileProcessor598 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:38.034734
[2018-04-19 21:15:38,038] {jobs.py:351} DagFileProcessor598 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.155 seconds
[2018-04-19 21:15:39,107] {jobs.py:343} DagFileProcessor599 INFO - Started process (PID=4247) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:39,112] {jobs.py:534} DagFileProcessor599 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:39,113] {jobs.py:1521} DagFileProcessor599 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:39,114] {models.py:167} DagFileProcessor599 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:39,220] {jobs.py:1535} DagFileProcessor599 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:39,241] {jobs.py:1169} DagFileProcessor599 INFO - Processing hello_world
[2018-04-19 21:15:39,249] {jobs.py:566} DagFileProcessor599 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:39,254] {models.py:322} DagFileProcessor599 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:39,255] {models.py:328} DagFileProcessor599 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:39.255167
[2018-04-19 21:15:39,258] {jobs.py:351} DagFileProcessor599 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:15:40,333] {jobs.py:343} DagFileProcessor600 INFO - Started process (PID=4248) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:40,338] {jobs.py:534} DagFileProcessor600 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:40,339] {jobs.py:1521} DagFileProcessor600 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:40,340] {models.py:167} DagFileProcessor600 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:40,441] {jobs.py:1535} DagFileProcessor600 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:40,459] {jobs.py:1169} DagFileProcessor600 INFO - Processing hello_world
[2018-04-19 21:15:40,468] {jobs.py:566} DagFileProcessor600 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:40,473] {models.py:322} DagFileProcessor600 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:40,474] {models.py:328} DagFileProcessor600 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:40.473883
[2018-04-19 21:15:40,477] {jobs.py:351} DagFileProcessor600 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:15:41,567] {jobs.py:343} DagFileProcessor601 INFO - Started process (PID=4249) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:41,572] {jobs.py:534} DagFileProcessor601 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:41,574] {jobs.py:1521} DagFileProcessor601 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:41,574] {models.py:167} DagFileProcessor601 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:41,707] {jobs.py:1535} DagFileProcessor601 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:41,732] {jobs.py:1169} DagFileProcessor601 INFO - Processing hello_world
[2018-04-19 21:15:41,744] {jobs.py:566} DagFileProcessor601 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:41,752] {models.py:322} DagFileProcessor601 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:41,753] {models.py:328} DagFileProcessor601 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:41.753084
[2018-04-19 21:15:41,757] {jobs.py:351} DagFileProcessor601 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.190 seconds
[2018-04-19 21:15:42,788] {jobs.py:343} DagFileProcessor602 INFO - Started process (PID=4250) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:42,795] {jobs.py:534} DagFileProcessor602 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:42,796] {jobs.py:1521} DagFileProcessor602 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:42,797] {models.py:167} DagFileProcessor602 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:42,917] {jobs.py:1535} DagFileProcessor602 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:42,938] {jobs.py:1169} DagFileProcessor602 INFO - Processing hello_world
[2018-04-19 21:15:42,948] {jobs.py:566} DagFileProcessor602 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:42,955] {models.py:322} DagFileProcessor602 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:42,956] {models.py:328} DagFileProcessor602 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:42.956130
[2018-04-19 21:15:42,960] {jobs.py:351} DagFileProcessor602 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.171 seconds
[2018-04-19 21:15:44,014] {jobs.py:343} DagFileProcessor603 INFO - Started process (PID=4252) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:44,019] {jobs.py:534} DagFileProcessor603 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:44,021] {jobs.py:1521} DagFileProcessor603 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:44,021] {models.py:167} DagFileProcessor603 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:44,128] {jobs.py:1535} DagFileProcessor603 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:44,149] {jobs.py:1169} DagFileProcessor603 INFO - Processing hello_world
[2018-04-19 21:15:44,158] {jobs.py:566} DagFileProcessor603 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:44,163] {models.py:322} DagFileProcessor603 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:44,164] {models.py:328} DagFileProcessor603 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:44.164139
[2018-04-19 21:15:44,167] {jobs.py:351} DagFileProcessor603 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:15:45,244] {jobs.py:343} DagFileProcessor604 INFO - Started process (PID=4253) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:45,249] {jobs.py:534} DagFileProcessor604 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:45,251] {jobs.py:1521} DagFileProcessor604 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:45,251] {models.py:167} DagFileProcessor604 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:45,357] {jobs.py:1535} DagFileProcessor604 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:45,376] {jobs.py:1169} DagFileProcessor604 INFO - Processing hello_world
[2018-04-19 21:15:45,385] {jobs.py:566} DagFileProcessor604 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:45,391] {models.py:322} DagFileProcessor604 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:45,391] {models.py:328} DagFileProcessor604 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:45.391596
[2018-04-19 21:15:45,395] {jobs.py:351} DagFileProcessor604 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:15:46,477] {jobs.py:343} DagFileProcessor605 INFO - Started process (PID=4254) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:46,482] {jobs.py:534} DagFileProcessor605 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:46,484] {jobs.py:1521} DagFileProcessor605 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:46,484] {models.py:167} DagFileProcessor605 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:46,590] {jobs.py:1535} DagFileProcessor605 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:46,609] {jobs.py:1169} DagFileProcessor605 INFO - Processing hello_world
[2018-04-19 21:15:46,618] {jobs.py:566} DagFileProcessor605 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:46,624] {models.py:322} DagFileProcessor605 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:46,625] {models.py:328} DagFileProcessor605 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:46.624912
[2018-04-19 21:15:46,629] {jobs.py:351} DagFileProcessor605 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:15:47,713] {jobs.py:343} DagFileProcessor606 INFO - Started process (PID=4255) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:47,718] {jobs.py:534} DagFileProcessor606 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:47,720] {jobs.py:1521} DagFileProcessor606 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:47,720] {models.py:167} DagFileProcessor606 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:47,833] {jobs.py:1535} DagFileProcessor606 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:47,853] {jobs.py:1169} DagFileProcessor606 INFO - Processing hello_world
[2018-04-19 21:15:47,863] {jobs.py:566} DagFileProcessor606 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:47,869] {models.py:322} DagFileProcessor606 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:47,869] {models.py:328} DagFileProcessor606 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:47.869617
[2018-04-19 21:15:47,873] {jobs.py:351} DagFileProcessor606 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.160 seconds
[2018-04-19 21:15:48,941] {jobs.py:343} DagFileProcessor607 INFO - Started process (PID=4256) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:48,946] {jobs.py:534} DagFileProcessor607 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:48,947] {jobs.py:1521} DagFileProcessor607 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:48,948] {models.py:167} DagFileProcessor607 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:49,060] {jobs.py:1535} DagFileProcessor607 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:49,081] {jobs.py:1169} DagFileProcessor607 INFO - Processing hello_world
[2018-04-19 21:15:49,090] {jobs.py:566} DagFileProcessor607 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:49,096] {models.py:322} DagFileProcessor607 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:49,097] {models.py:328} DagFileProcessor607 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:49.096851
[2018-04-19 21:15:49,100] {jobs.py:351} DagFileProcessor607 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:15:50,170] {jobs.py:343} DagFileProcessor608 INFO - Started process (PID=4257) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:50,176] {jobs.py:534} DagFileProcessor608 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:50,177] {jobs.py:1521} DagFileProcessor608 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:50,177] {models.py:167} DagFileProcessor608 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:50,284] {jobs.py:1535} DagFileProcessor608 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:50,303] {jobs.py:1169} DagFileProcessor608 INFO - Processing hello_world
[2018-04-19 21:15:50,312] {jobs.py:566} DagFileProcessor608 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:50,317] {models.py:322} DagFileProcessor608 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:50,318] {models.py:328} DagFileProcessor608 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:50.318028
[2018-04-19 21:15:50,323] {jobs.py:351} DagFileProcessor608 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:15:51,399] {jobs.py:343} DagFileProcessor609 INFO - Started process (PID=4258) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:51,404] {jobs.py:534} DagFileProcessor609 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:51,405] {jobs.py:1521} DagFileProcessor609 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:51,405] {models.py:167} DagFileProcessor609 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:51,511] {jobs.py:1535} DagFileProcessor609 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:51,530] {jobs.py:1169} DagFileProcessor609 INFO - Processing hello_world
[2018-04-19 21:15:51,538] {jobs.py:566} DagFileProcessor609 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:51,544] {models.py:322} DagFileProcessor609 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:51,544] {models.py:328} DagFileProcessor609 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:51.544464
[2018-04-19 21:15:51,548] {jobs.py:351} DagFileProcessor609 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:15:52,625] {jobs.py:343} DagFileProcessor610 INFO - Started process (PID=4259) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:52,631] {jobs.py:534} DagFileProcessor610 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:52,632] {jobs.py:1521} DagFileProcessor610 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:52,632] {models.py:167} DagFileProcessor610 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:52,734] {jobs.py:1535} DagFileProcessor610 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:52,752] {jobs.py:1169} DagFileProcessor610 INFO - Processing hello_world
[2018-04-19 21:15:52,762] {jobs.py:566} DagFileProcessor610 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:52,767] {models.py:322} DagFileProcessor610 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:52,767] {models.py:328} DagFileProcessor610 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:52.767709
[2018-04-19 21:15:52,771] {jobs.py:351} DagFileProcessor610 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:15:53,863] {jobs.py:343} DagFileProcessor611 INFO - Started process (PID=4261) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:53,868] {jobs.py:534} DagFileProcessor611 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:53,869] {jobs.py:1521} DagFileProcessor611 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:53,870] {models.py:167} DagFileProcessor611 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:53,975] {jobs.py:1535} DagFileProcessor611 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:53,997] {jobs.py:1169} DagFileProcessor611 INFO - Processing hello_world
[2018-04-19 21:15:54,005] {jobs.py:566} DagFileProcessor611 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:54,010] {models.py:322} DagFileProcessor611 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:54,011] {models.py:328} DagFileProcessor611 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:54.011103
[2018-04-19 21:15:54,014] {jobs.py:351} DagFileProcessor611 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:15:55,092] {jobs.py:343} DagFileProcessor612 INFO - Started process (PID=4262) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:55,097] {jobs.py:534} DagFileProcessor612 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:55,098] {jobs.py:1521} DagFileProcessor612 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:55,098] {models.py:167} DagFileProcessor612 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:55,204] {jobs.py:1535} DagFileProcessor612 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:55,224] {jobs.py:1169} DagFileProcessor612 INFO - Processing hello_world
[2018-04-19 21:15:55,233] {jobs.py:566} DagFileProcessor612 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:55,240] {models.py:322} DagFileProcessor612 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:55,241] {models.py:328} DagFileProcessor612 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:55.241368
[2018-04-19 21:15:55,244] {jobs.py:351} DagFileProcessor612 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:15:56,312] {jobs.py:343} DagFileProcessor613 INFO - Started process (PID=4263) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:56,320] {jobs.py:534} DagFileProcessor613 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:56,321] {jobs.py:1521} DagFileProcessor613 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:56,321] {models.py:167} DagFileProcessor613 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:56,438] {jobs.py:1535} DagFileProcessor613 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:56,459] {jobs.py:1169} DagFileProcessor613 INFO - Processing hello_world
[2018-04-19 21:15:56,469] {jobs.py:566} DagFileProcessor613 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:56,474] {models.py:322} DagFileProcessor613 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:56,475] {models.py:328} DagFileProcessor613 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:56.475123
[2018-04-19 21:15:56,478] {jobs.py:351} DagFileProcessor613 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.166 seconds
[2018-04-19 21:15:57,542] {jobs.py:343} DagFileProcessor614 INFO - Started process (PID=4264) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:57,547] {jobs.py:534} DagFileProcessor614 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:57,549] {jobs.py:1521} DagFileProcessor614 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:57,549] {models.py:167} DagFileProcessor614 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:57,668] {jobs.py:1535} DagFileProcessor614 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:57,690] {jobs.py:1169} DagFileProcessor614 INFO - Processing hello_world
[2018-04-19 21:15:57,700] {jobs.py:566} DagFileProcessor614 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:57,706] {models.py:322} DagFileProcessor614 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:57,707] {models.py:328} DagFileProcessor614 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:57.707226
[2018-04-19 21:15:57,711] {jobs.py:351} DagFileProcessor614 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.169 seconds
[2018-04-19 21:15:58,761] {jobs.py:343} DagFileProcessor615 INFO - Started process (PID=4265) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:58,766] {jobs.py:534} DagFileProcessor615 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:15:58,767] {jobs.py:1521} DagFileProcessor615 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:15:58,768] {models.py:167} DagFileProcessor615 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:58,874] {jobs.py:1535} DagFileProcessor615 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:15:58,894] {jobs.py:1169} DagFileProcessor615 INFO - Processing hello_world
[2018-04-19 21:15:58,903] {jobs.py:566} DagFileProcessor615 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:15:58,908] {models.py:322} DagFileProcessor615 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:15:58,909] {models.py:328} DagFileProcessor615 INFO - Failing jobs without heartbeat after 2018-04-19 21:10:58.908955
[2018-04-19 21:15:58,912] {jobs.py:351} DagFileProcessor615 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:16:00,001] {jobs.py:343} DagFileProcessor616 INFO - Started process (PID=4266) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:00,006] {jobs.py:534} DagFileProcessor616 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:00,007] {jobs.py:1521} DagFileProcessor616 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:00,008] {models.py:167} DagFileProcessor616 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:00,112] {jobs.py:1535} DagFileProcessor616 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:00,132] {jobs.py:1169} DagFileProcessor616 INFO - Processing hello_world
[2018-04-19 21:16:00,140] {jobs.py:566} DagFileProcessor616 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:00,146] {models.py:322} DagFileProcessor616 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:00,146] {models.py:328} DagFileProcessor616 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:00.146470
[2018-04-19 21:16:00,150] {jobs.py:351} DagFileProcessor616 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:16:01,226] {jobs.py:343} DagFileProcessor617 INFO - Started process (PID=4267) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:01,231] {jobs.py:534} DagFileProcessor617 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:01,232] {jobs.py:1521} DagFileProcessor617 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:01,232] {models.py:167} DagFileProcessor617 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:01,337] {jobs.py:1535} DagFileProcessor617 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:01,358] {jobs.py:1169} DagFileProcessor617 INFO - Processing hello_world
[2018-04-19 21:16:01,366] {jobs.py:566} DagFileProcessor617 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:01,371] {models.py:322} DagFileProcessor617 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:01,372] {models.py:328} DagFileProcessor617 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:01.371924
[2018-04-19 21:16:01,375] {jobs.py:351} DagFileProcessor617 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:16:02,457] {jobs.py:343} DagFileProcessor618 INFO - Started process (PID=4268) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:02,462] {jobs.py:534} DagFileProcessor618 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:02,463] {jobs.py:1521} DagFileProcessor618 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:02,463] {models.py:167} DagFileProcessor618 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:02,565] {jobs.py:1535} DagFileProcessor618 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:02,583] {jobs.py:1169} DagFileProcessor618 INFO - Processing hello_world
[2018-04-19 21:16:02,595] {jobs.py:566} DagFileProcessor618 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:02,601] {models.py:322} DagFileProcessor618 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:02,602] {models.py:328} DagFileProcessor618 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:02.602067
[2018-04-19 21:16:02,605] {jobs.py:351} DagFileProcessor618 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:16:03,688] {jobs.py:343} DagFileProcessor619 INFO - Started process (PID=4270) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:03,693] {jobs.py:534} DagFileProcessor619 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:03,694] {jobs.py:1521} DagFileProcessor619 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:03,694] {models.py:167} DagFileProcessor619 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:03,800] {jobs.py:1535} DagFileProcessor619 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:03,819] {jobs.py:1169} DagFileProcessor619 INFO - Processing hello_world
[2018-04-19 21:16:03,828] {jobs.py:566} DagFileProcessor619 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:03,833] {models.py:322} DagFileProcessor619 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:03,834] {models.py:328} DagFileProcessor619 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:03.834201
[2018-04-19 21:16:03,837] {jobs.py:351} DagFileProcessor619 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:16:04,912] {jobs.py:343} DagFileProcessor620 INFO - Started process (PID=4271) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:04,917] {jobs.py:534} DagFileProcessor620 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:04,918] {jobs.py:1521} DagFileProcessor620 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:04,919] {models.py:167} DagFileProcessor620 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:05,030] {jobs.py:1535} DagFileProcessor620 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:05,052] {jobs.py:1169} DagFileProcessor620 INFO - Processing hello_world
[2018-04-19 21:16:05,061] {jobs.py:566} DagFileProcessor620 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:05,068] {models.py:322} DagFileProcessor620 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:05,068] {models.py:328} DagFileProcessor620 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:05.068689
[2018-04-19 21:16:05,072] {jobs.py:351} DagFileProcessor620 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.160 seconds
[2018-04-19 21:16:06,146] {jobs.py:343} DagFileProcessor621 INFO - Started process (PID=4272) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:06,151] {jobs.py:534} DagFileProcessor621 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:06,152] {jobs.py:1521} DagFileProcessor621 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:06,153] {models.py:167} DagFileProcessor621 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:06,264] {jobs.py:1535} DagFileProcessor621 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:06,287] {jobs.py:1169} DagFileProcessor621 INFO - Processing hello_world
[2018-04-19 21:16:06,296] {jobs.py:566} DagFileProcessor621 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:06,302] {models.py:322} DagFileProcessor621 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:06,303] {models.py:328} DagFileProcessor621 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:06.302792
[2018-04-19 21:16:06,307] {jobs.py:351} DagFileProcessor621 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.161 seconds
[2018-04-19 21:16:07,373] {jobs.py:343} DagFileProcessor622 INFO - Started process (PID=4273) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:07,378] {jobs.py:534} DagFileProcessor622 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:07,379] {jobs.py:1521} DagFileProcessor622 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:07,380] {models.py:167} DagFileProcessor622 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:07,485] {jobs.py:1535} DagFileProcessor622 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:07,503] {jobs.py:1169} DagFileProcessor622 INFO - Processing hello_world
[2018-04-19 21:16:07,511] {jobs.py:566} DagFileProcessor622 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:07,516] {models.py:322} DagFileProcessor622 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:07,517] {models.py:328} DagFileProcessor622 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:07.517310
[2018-04-19 21:16:07,521] {jobs.py:351} DagFileProcessor622 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:16:08,600] {jobs.py:343} DagFileProcessor623 INFO - Started process (PID=4274) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:08,605] {jobs.py:534} DagFileProcessor623 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:08,606] {jobs.py:1521} DagFileProcessor623 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:08,607] {models.py:167} DagFileProcessor623 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:08,712] {jobs.py:1535} DagFileProcessor623 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:08,731] {jobs.py:1169} DagFileProcessor623 INFO - Processing hello_world
[2018-04-19 21:16:08,739] {jobs.py:566} DagFileProcessor623 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:08,744] {models.py:322} DagFileProcessor623 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:08,745] {models.py:328} DagFileProcessor623 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:08.745312
[2018-04-19 21:16:08,748] {jobs.py:351} DagFileProcessor623 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:16:09,835] {jobs.py:343} DagFileProcessor624 INFO - Started process (PID=4275) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:09,840] {jobs.py:534} DagFileProcessor624 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:09,841] {jobs.py:1521} DagFileProcessor624 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:09,842] {models.py:167} DagFileProcessor624 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:09,947] {jobs.py:1535} DagFileProcessor624 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:09,968] {jobs.py:1169} DagFileProcessor624 INFO - Processing hello_world
[2018-04-19 21:16:09,976] {jobs.py:566} DagFileProcessor624 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:09,981] {models.py:322} DagFileProcessor624 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:09,982] {models.py:328} DagFileProcessor624 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:09.982070
[2018-04-19 21:16:09,985] {jobs.py:351} DagFileProcessor624 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:16:11,066] {jobs.py:343} DagFileProcessor625 INFO - Started process (PID=4276) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:11,072] {jobs.py:534} DagFileProcessor625 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:11,073] {jobs.py:1521} DagFileProcessor625 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:11,073] {models.py:167} DagFileProcessor625 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:11,180] {jobs.py:1535} DagFileProcessor625 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:11,200] {jobs.py:1169} DagFileProcessor625 INFO - Processing hello_world
[2018-04-19 21:16:11,208] {jobs.py:566} DagFileProcessor625 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:11,214] {models.py:322} DagFileProcessor625 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:11,214] {models.py:328} DagFileProcessor625 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:11.214310
[2018-04-19 21:16:11,217] {jobs.py:351} DagFileProcessor625 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:16:12,300] {jobs.py:343} DagFileProcessor626 INFO - Started process (PID=4277) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:12,305] {jobs.py:534} DagFileProcessor626 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:12,306] {jobs.py:1521} DagFileProcessor626 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:12,306] {models.py:167} DagFileProcessor626 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:12,420] {jobs.py:1535} DagFileProcessor626 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:12,441] {jobs.py:1169} DagFileProcessor626 INFO - Processing hello_world
[2018-04-19 21:16:12,449] {jobs.py:566} DagFileProcessor626 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:12,455] {models.py:322} DagFileProcessor626 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:12,456] {models.py:328} DagFileProcessor626 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:12.456165
[2018-04-19 21:16:12,459] {jobs.py:351} DagFileProcessor626 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.160 seconds
[2018-04-19 21:16:13,527] {jobs.py:343} DagFileProcessor627 INFO - Started process (PID=4278) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:13,532] {jobs.py:534} DagFileProcessor627 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:13,533] {jobs.py:1521} DagFileProcessor627 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:13,534] {models.py:167} DagFileProcessor627 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:13,643] {jobs.py:1535} DagFileProcessor627 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:13,663] {jobs.py:1169} DagFileProcessor627 INFO - Processing hello_world
[2018-04-19 21:16:13,671] {jobs.py:566} DagFileProcessor627 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:13,677] {models.py:322} DagFileProcessor627 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:13,678] {models.py:328} DagFileProcessor627 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:13.677974
[2018-04-19 21:16:13,681] {jobs.py:351} DagFileProcessor627 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:16:14,760] {jobs.py:343} DagFileProcessor628 INFO - Started process (PID=4280) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:14,765] {jobs.py:534} DagFileProcessor628 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:14,766] {jobs.py:1521} DagFileProcessor628 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:14,766] {models.py:167} DagFileProcessor628 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:14,868] {jobs.py:1535} DagFileProcessor628 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:14,886] {jobs.py:1169} DagFileProcessor628 INFO - Processing hello_world
[2018-04-19 21:16:14,895] {jobs.py:566} DagFileProcessor628 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:14,900] {models.py:322} DagFileProcessor628 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:14,900] {models.py:328} DagFileProcessor628 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:14.900524
[2018-04-19 21:16:14,903] {jobs.py:351} DagFileProcessor628 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:16:15,989] {jobs.py:343} DagFileProcessor629 INFO - Started process (PID=4281) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:15,994] {jobs.py:534} DagFileProcessor629 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:15,995] {jobs.py:1521} DagFileProcessor629 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:15,995] {models.py:167} DagFileProcessor629 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:16,096] {jobs.py:1535} DagFileProcessor629 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:16,114] {jobs.py:1169} DagFileProcessor629 INFO - Processing hello_world
[2018-04-19 21:16:16,123] {jobs.py:566} DagFileProcessor629 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:16,128] {models.py:322} DagFileProcessor629 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:16,129] {models.py:328} DagFileProcessor629 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:16.129182
[2018-04-19 21:16:16,132] {jobs.py:351} DagFileProcessor629 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:16:17,220] {jobs.py:343} DagFileProcessor630 INFO - Started process (PID=4282) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:17,226] {jobs.py:534} DagFileProcessor630 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:17,227] {jobs.py:1521} DagFileProcessor630 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:17,227] {models.py:167} DagFileProcessor630 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:17,329] {jobs.py:1535} DagFileProcessor630 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:17,347] {jobs.py:1169} DagFileProcessor630 INFO - Processing hello_world
[2018-04-19 21:16:17,355] {jobs.py:566} DagFileProcessor630 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:17,360] {models.py:322} DagFileProcessor630 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:17,360] {models.py:328} DagFileProcessor630 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:17.360713
[2018-04-19 21:16:17,364] {jobs.py:351} DagFileProcessor630 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:16:18,460] {jobs.py:343} DagFileProcessor631 INFO - Started process (PID=4283) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:18,465] {jobs.py:534} DagFileProcessor631 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:18,466] {jobs.py:1521} DagFileProcessor631 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:18,467] {models.py:167} DagFileProcessor631 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:18,567] {jobs.py:1535} DagFileProcessor631 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:18,585] {jobs.py:1169} DagFileProcessor631 INFO - Processing hello_world
[2018-04-19 21:16:18,593] {jobs.py:566} DagFileProcessor631 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:18,598] {models.py:322} DagFileProcessor631 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:18,599] {models.py:328} DagFileProcessor631 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:18.599100
[2018-04-19 21:16:18,602] {jobs.py:351} DagFileProcessor631 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.142 seconds
[2018-04-19 21:16:19,689] {jobs.py:343} DagFileProcessor632 INFO - Started process (PID=4284) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:19,694] {jobs.py:534} DagFileProcessor632 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:19,695] {jobs.py:1521} DagFileProcessor632 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:19,695] {models.py:167} DagFileProcessor632 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:19,797] {jobs.py:1535} DagFileProcessor632 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:19,817] {jobs.py:1169} DagFileProcessor632 INFO - Processing hello_world
[2018-04-19 21:16:19,824] {jobs.py:566} DagFileProcessor632 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:19,830] {models.py:322} DagFileProcessor632 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:19,830] {models.py:328} DagFileProcessor632 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:19.830450
[2018-04-19 21:16:19,833] {jobs.py:351} DagFileProcessor632 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:16:20,920] {jobs.py:343} DagFileProcessor633 INFO - Started process (PID=4285) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:20,925] {jobs.py:534} DagFileProcessor633 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:20,926] {jobs.py:1521} DagFileProcessor633 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:20,926] {models.py:167} DagFileProcessor633 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:21,027] {jobs.py:1535} DagFileProcessor633 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:21,046] {jobs.py:1169} DagFileProcessor633 INFO - Processing hello_world
[2018-04-19 21:16:21,055] {jobs.py:566} DagFileProcessor633 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:21,060] {models.py:322} DagFileProcessor633 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:21,060] {models.py:328} DagFileProcessor633 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:21.060671
[2018-04-19 21:16:21,064] {jobs.py:351} DagFileProcessor633 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:16:22,150] {jobs.py:343} DagFileProcessor634 INFO - Started process (PID=4286) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:22,156] {jobs.py:534} DagFileProcessor634 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:22,157] {jobs.py:1521} DagFileProcessor634 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:22,157] {models.py:167} DagFileProcessor634 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:22,260] {jobs.py:1535} DagFileProcessor634 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:22,277] {jobs.py:1169} DagFileProcessor634 INFO - Processing hello_world
[2018-04-19 21:16:22,286] {jobs.py:566} DagFileProcessor634 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:22,292] {models.py:322} DagFileProcessor634 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:22,292] {models.py:328} DagFileProcessor634 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:22.292530
[2018-04-19 21:16:22,296] {jobs.py:351} DagFileProcessor634 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:16:23,385] {jobs.py:343} DagFileProcessor635 INFO - Started process (PID=4287) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:23,390] {jobs.py:534} DagFileProcessor635 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:23,391] {jobs.py:1521} DagFileProcessor635 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:23,391] {models.py:167} DagFileProcessor635 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:23,490] {jobs.py:1535} DagFileProcessor635 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:23,512] {jobs.py:1169} DagFileProcessor635 INFO - Processing hello_world
[2018-04-19 21:16:23,520] {jobs.py:566} DagFileProcessor635 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:23,526] {models.py:322} DagFileProcessor635 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:23,526] {models.py:328} DagFileProcessor635 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:23.526519
[2018-04-19 21:16:23,529] {jobs.py:351} DagFileProcessor635 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:16:24,619] {jobs.py:343} DagFileProcessor636 INFO - Started process (PID=4289) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:24,624] {jobs.py:534} DagFileProcessor636 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:24,625] {jobs.py:1521} DagFileProcessor636 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:24,625] {models.py:167} DagFileProcessor636 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:24,734] {jobs.py:1535} DagFileProcessor636 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:24,754] {jobs.py:1169} DagFileProcessor636 INFO - Processing hello_world
[2018-04-19 21:16:24,763] {jobs.py:566} DagFileProcessor636 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:24,769] {models.py:322} DagFileProcessor636 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:24,769] {models.py:328} DagFileProcessor636 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:24.769479
[2018-04-19 21:16:24,773] {jobs.py:351} DagFileProcessor636 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:16:25,845] {jobs.py:343} DagFileProcessor637 INFO - Started process (PID=4290) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:25,850] {jobs.py:534} DagFileProcessor637 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:25,851] {jobs.py:1521} DagFileProcessor637 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:25,851] {models.py:167} DagFileProcessor637 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:25,951] {jobs.py:1535} DagFileProcessor637 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:25,971] {jobs.py:1169} DagFileProcessor637 INFO - Processing hello_world
[2018-04-19 21:16:25,979] {jobs.py:566} DagFileProcessor637 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:25,985] {models.py:322} DagFileProcessor637 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:25,985] {models.py:328} DagFileProcessor637 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:25.985560
[2018-04-19 21:16:25,989] {jobs.py:351} DagFileProcessor637 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:16:27,073] {jobs.py:343} DagFileProcessor638 INFO - Started process (PID=4291) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:27,082] {jobs.py:534} DagFileProcessor638 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:27,083] {jobs.py:1521} DagFileProcessor638 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:27,084] {models.py:167} DagFileProcessor638 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:27,184] {jobs.py:1535} DagFileProcessor638 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:27,202] {jobs.py:1169} DagFileProcessor638 INFO - Processing hello_world
[2018-04-19 21:16:27,211] {jobs.py:566} DagFileProcessor638 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:27,217] {models.py:322} DagFileProcessor638 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:27,217] {models.py:328} DagFileProcessor638 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:27.217628
[2018-04-19 21:16:27,222] {jobs.py:351} DagFileProcessor638 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:16:28,303] {jobs.py:343} DagFileProcessor639 INFO - Started process (PID=4292) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:28,308] {jobs.py:534} DagFileProcessor639 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:28,309] {jobs.py:1521} DagFileProcessor639 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:28,309] {models.py:167} DagFileProcessor639 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:28,416] {jobs.py:1535} DagFileProcessor639 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:28,435] {jobs.py:1169} DagFileProcessor639 INFO - Processing hello_world
[2018-04-19 21:16:28,443] {jobs.py:566} DagFileProcessor639 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:28,448] {models.py:322} DagFileProcessor639 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:28,449] {models.py:328} DagFileProcessor639 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:28.449269
[2018-04-19 21:16:28,453] {jobs.py:351} DagFileProcessor639 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:16:29,535] {jobs.py:343} DagFileProcessor640 INFO - Started process (PID=4293) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:29,540] {jobs.py:534} DagFileProcessor640 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:29,541] {jobs.py:1521} DagFileProcessor640 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:29,542] {models.py:167} DagFileProcessor640 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:29,641] {jobs.py:1535} DagFileProcessor640 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:29,660] {jobs.py:1169} DagFileProcessor640 INFO - Processing hello_world
[2018-04-19 21:16:29,669] {jobs.py:566} DagFileProcessor640 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:29,675] {models.py:322} DagFileProcessor640 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:29,676] {models.py:328} DagFileProcessor640 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:29.675953
[2018-04-19 21:16:29,679] {jobs.py:351} DagFileProcessor640 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:16:30,769] {jobs.py:343} DagFileProcessor641 INFO - Started process (PID=4294) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:30,774] {jobs.py:534} DagFileProcessor641 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:30,775] {jobs.py:1521} DagFileProcessor641 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:30,775] {models.py:167} DagFileProcessor641 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:30,877] {jobs.py:1535} DagFileProcessor641 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:30,897] {jobs.py:1169} DagFileProcessor641 INFO - Processing hello_world
[2018-04-19 21:16:30,905] {jobs.py:566} DagFileProcessor641 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:30,910] {models.py:322} DagFileProcessor641 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:30,910] {models.py:328} DagFileProcessor641 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:30.910738
[2018-04-19 21:16:30,914] {jobs.py:351} DagFileProcessor641 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:16:31,986] {jobs.py:343} DagFileProcessor642 INFO - Started process (PID=4295) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:31,991] {jobs.py:534} DagFileProcessor642 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:31,992] {jobs.py:1521} DagFileProcessor642 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:31,992] {models.py:167} DagFileProcessor642 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:32,131] {jobs.py:1535} DagFileProcessor642 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:32,158] {jobs.py:1169} DagFileProcessor642 INFO - Processing hello_world
[2018-04-19 21:16:32,168] {jobs.py:566} DagFileProcessor642 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:32,175] {models.py:322} DagFileProcessor642 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:32,175] {models.py:328} DagFileProcessor642 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:32.175513
[2018-04-19 21:16:32,179] {jobs.py:351} DagFileProcessor642 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.193 seconds
[2018-04-19 21:16:33,215] {jobs.py:343} DagFileProcessor643 INFO - Started process (PID=4296) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:33,220] {jobs.py:534} DagFileProcessor643 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:33,221] {jobs.py:1521} DagFileProcessor643 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:33,221] {models.py:167} DagFileProcessor643 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:33,326] {jobs.py:1535} DagFileProcessor643 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:33,349] {jobs.py:1169} DagFileProcessor643 INFO - Processing hello_world
[2018-04-19 21:16:33,358] {jobs.py:566} DagFileProcessor643 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:33,364] {models.py:322} DagFileProcessor643 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:33,365] {models.py:328} DagFileProcessor643 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:33.365234
[2018-04-19 21:16:33,368] {jobs.py:351} DagFileProcessor643 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:16:34,441] {jobs.py:343} DagFileProcessor644 INFO - Started process (PID=4298) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:34,446] {jobs.py:534} DagFileProcessor644 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:34,447] {jobs.py:1521} DagFileProcessor644 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:34,448] {models.py:167} DagFileProcessor644 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:34,553] {jobs.py:1535} DagFileProcessor644 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:34,572] {jobs.py:1169} DagFileProcessor644 INFO - Processing hello_world
[2018-04-19 21:16:34,581] {jobs.py:566} DagFileProcessor644 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:34,586] {models.py:322} DagFileProcessor644 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:34,587] {models.py:328} DagFileProcessor644 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:34.587417
[2018-04-19 21:16:34,591] {jobs.py:351} DagFileProcessor644 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:16:35,668] {jobs.py:343} DagFileProcessor645 INFO - Started process (PID=4299) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:35,673] {jobs.py:534} DagFileProcessor645 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:35,674] {jobs.py:1521} DagFileProcessor645 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:35,675] {models.py:167} DagFileProcessor645 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:35,783] {jobs.py:1535} DagFileProcessor645 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:35,807] {jobs.py:1169} DagFileProcessor645 INFO - Processing hello_world
[2018-04-19 21:16:35,817] {jobs.py:566} DagFileProcessor645 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:35,823] {models.py:322} DagFileProcessor645 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:35,824] {models.py:328} DagFileProcessor645 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:35.824051
[2018-04-19 21:16:35,827] {jobs.py:351} DagFileProcessor645 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:16:36,905] {jobs.py:343} DagFileProcessor646 INFO - Started process (PID=4300) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:36,910] {jobs.py:534} DagFileProcessor646 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:36,911] {jobs.py:1521} DagFileProcessor646 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:36,911] {models.py:167} DagFileProcessor646 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:37,019] {jobs.py:1535} DagFileProcessor646 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:37,041] {jobs.py:1169} DagFileProcessor646 INFO - Processing hello_world
[2018-04-19 21:16:37,050] {jobs.py:566} DagFileProcessor646 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:37,055] {models.py:322} DagFileProcessor646 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:37,056] {models.py:328} DagFileProcessor646 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:37.056303
[2018-04-19 21:16:37,060] {jobs.py:351} DagFileProcessor646 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:16:38,133] {jobs.py:343} DagFileProcessor647 INFO - Started process (PID=4301) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:38,138] {jobs.py:534} DagFileProcessor647 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:38,139] {jobs.py:1521} DagFileProcessor647 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:38,139] {models.py:167} DagFileProcessor647 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:38,243] {jobs.py:1535} DagFileProcessor647 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:38,263] {jobs.py:1169} DagFileProcessor647 INFO - Processing hello_world
[2018-04-19 21:16:38,273] {jobs.py:566} DagFileProcessor647 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:38,279] {models.py:322} DagFileProcessor647 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:38,280] {models.py:328} DagFileProcessor647 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:38.280386
[2018-04-19 21:16:38,284] {jobs.py:351} DagFileProcessor647 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:16:39,358] {jobs.py:343} DagFileProcessor648 INFO - Started process (PID=4302) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:39,363] {jobs.py:534} DagFileProcessor648 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:39,364] {jobs.py:1521} DagFileProcessor648 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:39,365] {models.py:167} DagFileProcessor648 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:39,465] {jobs.py:1535} DagFileProcessor648 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:39,487] {jobs.py:1169} DagFileProcessor648 INFO - Processing hello_world
[2018-04-19 21:16:39,497] {jobs.py:566} DagFileProcessor648 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:39,502] {models.py:322} DagFileProcessor648 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:39,503] {models.py:328} DagFileProcessor648 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:39.502920
[2018-04-19 21:16:39,506] {jobs.py:351} DagFileProcessor648 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:16:40,585] {jobs.py:343} DagFileProcessor649 INFO - Started process (PID=4303) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:40,590] {jobs.py:534} DagFileProcessor649 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:40,591] {jobs.py:1521} DagFileProcessor649 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:40,591] {models.py:167} DagFileProcessor649 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:40,700] {jobs.py:1535} DagFileProcessor649 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:40,720] {jobs.py:1169} DagFileProcessor649 INFO - Processing hello_world
[2018-04-19 21:16:40,728] {jobs.py:566} DagFileProcessor649 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:40,734] {models.py:322} DagFileProcessor649 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:40,734] {models.py:328} DagFileProcessor649 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:40.734451
[2018-04-19 21:16:40,738] {jobs.py:351} DagFileProcessor649 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:16:41,811] {jobs.py:343} DagFileProcessor650 INFO - Started process (PID=4304) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:41,815] {jobs.py:534} DagFileProcessor650 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:41,817] {jobs.py:1521} DagFileProcessor650 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:41,817] {models.py:167} DagFileProcessor650 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:41,926] {jobs.py:1535} DagFileProcessor650 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:41,947] {jobs.py:1169} DagFileProcessor650 INFO - Processing hello_world
[2018-04-19 21:16:41,955] {jobs.py:566} DagFileProcessor650 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:41,961] {models.py:322} DagFileProcessor650 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:41,961] {models.py:328} DagFileProcessor650 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:41.961813
[2018-04-19 21:16:41,965] {jobs.py:351} DagFileProcessor650 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:16:43,046] {jobs.py:343} DagFileProcessor651 INFO - Started process (PID=4312) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:43,051] {jobs.py:534} DagFileProcessor651 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:43,052] {jobs.py:1521} DagFileProcessor651 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:43,052] {models.py:167} DagFileProcessor651 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:43,166] {jobs.py:1535} DagFileProcessor651 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:43,188] {jobs.py:1169} DagFileProcessor651 INFO - Processing hello_world
[2018-04-19 21:16:43,200] {jobs.py:566} DagFileProcessor651 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:43,206] {models.py:322} DagFileProcessor651 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:43,206] {models.py:328} DagFileProcessor651 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:43.206392
[2018-04-19 21:16:43,210] {jobs.py:351} DagFileProcessor651 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.164 seconds
[2018-04-19 21:16:44,271] {jobs.py:343} DagFileProcessor652 INFO - Started process (PID=4314) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:44,276] {jobs.py:534} DagFileProcessor652 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:44,277] {jobs.py:1521} DagFileProcessor652 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:44,278] {models.py:167} DagFileProcessor652 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:44,386] {jobs.py:1535} DagFileProcessor652 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:44,409] {jobs.py:1169} DagFileProcessor652 INFO - Processing hello_world
[2018-04-19 21:16:44,418] {jobs.py:566} DagFileProcessor652 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:44,425] {models.py:322} DagFileProcessor652 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:44,425] {models.py:328} DagFileProcessor652 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:44.425444
[2018-04-19 21:16:44,429] {jobs.py:351} DagFileProcessor652 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.158 seconds
[2018-04-19 21:16:45,496] {jobs.py:343} DagFileProcessor653 INFO - Started process (PID=4315) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:45,501] {jobs.py:534} DagFileProcessor653 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:45,502] {jobs.py:1521} DagFileProcessor653 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:45,502] {models.py:167} DagFileProcessor653 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:45,603] {jobs.py:1535} DagFileProcessor653 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:45,624] {jobs.py:1169} DagFileProcessor653 INFO - Processing hello_world
[2018-04-19 21:16:45,632] {jobs.py:566} DagFileProcessor653 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:45,637] {models.py:322} DagFileProcessor653 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:45,638] {models.py:328} DagFileProcessor653 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:45.638268
[2018-04-19 21:16:45,641] {jobs.py:351} DagFileProcessor653 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:16:46,728] {jobs.py:343} DagFileProcessor654 INFO - Started process (PID=4316) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:46,733] {jobs.py:534} DagFileProcessor654 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:46,734] {jobs.py:1521} DagFileProcessor654 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:46,734] {models.py:167} DagFileProcessor654 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:46,837] {jobs.py:1535} DagFileProcessor654 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:46,858] {jobs.py:1169} DagFileProcessor654 INFO - Processing hello_world
[2018-04-19 21:16:46,868] {jobs.py:566} DagFileProcessor654 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:46,873] {models.py:322} DagFileProcessor654 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:46,873] {models.py:328} DagFileProcessor654 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:46.873792
[2018-04-19 21:16:46,877] {jobs.py:351} DagFileProcessor654 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:16:47,959] {jobs.py:343} DagFileProcessor655 INFO - Started process (PID=4317) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:47,964] {jobs.py:534} DagFileProcessor655 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:47,965] {jobs.py:1521} DagFileProcessor655 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:47,966] {models.py:167} DagFileProcessor655 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:48,066] {jobs.py:1535} DagFileProcessor655 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:48,086] {jobs.py:1169} DagFileProcessor655 INFO - Processing hello_world
[2018-04-19 21:16:48,095] {jobs.py:566} DagFileProcessor655 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:48,100] {models.py:322} DagFileProcessor655 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:48,101] {models.py:328} DagFileProcessor655 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:48.100976
[2018-04-19 21:16:48,104] {jobs.py:351} DagFileProcessor655 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:16:49,197] {jobs.py:343} DagFileProcessor656 INFO - Started process (PID=4318) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:49,202] {jobs.py:534} DagFileProcessor656 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:49,203] {jobs.py:1521} DagFileProcessor656 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:49,203] {models.py:167} DagFileProcessor656 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:49,310] {jobs.py:1535} DagFileProcessor656 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:49,331] {jobs.py:1169} DagFileProcessor656 INFO - Processing hello_world
[2018-04-19 21:16:49,339] {jobs.py:566} DagFileProcessor656 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:49,344] {models.py:322} DagFileProcessor656 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:49,344] {models.py:328} DagFileProcessor656 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:49.344808
[2018-04-19 21:16:49,348] {jobs.py:351} DagFileProcessor656 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:16:50,421] {jobs.py:343} DagFileProcessor657 INFO - Started process (PID=4319) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:50,425] {jobs.py:534} DagFileProcessor657 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:50,427] {jobs.py:1521} DagFileProcessor657 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:50,427] {models.py:167} DagFileProcessor657 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:50,529] {jobs.py:1535} DagFileProcessor657 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:50,549] {jobs.py:1169} DagFileProcessor657 INFO - Processing hello_world
[2018-04-19 21:16:50,560] {jobs.py:566} DagFileProcessor657 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:50,565] {models.py:322} DagFileProcessor657 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:50,566] {models.py:328} DagFileProcessor657 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:50.566235
[2018-04-19 21:16:50,570] {jobs.py:351} DagFileProcessor657 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:16:51,651] {jobs.py:343} DagFileProcessor658 INFO - Started process (PID=4320) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:51,656] {jobs.py:534} DagFileProcessor658 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:51,657] {jobs.py:1521} DagFileProcessor658 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:51,658] {models.py:167} DagFileProcessor658 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:51,761] {jobs.py:1535} DagFileProcessor658 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:51,783] {jobs.py:1169} DagFileProcessor658 INFO - Processing hello_world
[2018-04-19 21:16:51,792] {jobs.py:566} DagFileProcessor658 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:51,798] {models.py:322} DagFileProcessor658 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:51,799] {models.py:328} DagFileProcessor658 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:51.798855
[2018-04-19 21:16:51,802] {jobs.py:351} DagFileProcessor658 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:16:52,879] {jobs.py:343} DagFileProcessor659 INFO - Started process (PID=4321) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:52,885] {jobs.py:534} DagFileProcessor659 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:52,886] {jobs.py:1521} DagFileProcessor659 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:52,886] {models.py:167} DagFileProcessor659 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:53,000] {jobs.py:1535} DagFileProcessor659 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:53,020] {jobs.py:1169} DagFileProcessor659 INFO - Processing hello_world
[2018-04-19 21:16:53,030] {jobs.py:566} DagFileProcessor659 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:53,036] {models.py:322} DagFileProcessor659 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:53,037] {models.py:328} DagFileProcessor659 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:53.037050
[2018-04-19 21:16:53,041] {jobs.py:351} DagFileProcessor659 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.162 seconds
[2018-04-19 21:16:54,107] {jobs.py:343} DagFileProcessor660 INFO - Started process (PID=4323) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:54,112] {jobs.py:534} DagFileProcessor660 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:54,113] {jobs.py:1521} DagFileProcessor660 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:54,113] {models.py:167} DagFileProcessor660 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:54,225] {jobs.py:1535} DagFileProcessor660 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:54,246] {jobs.py:1169} DagFileProcessor660 INFO - Processing hello_world
[2018-04-19 21:16:54,255] {jobs.py:566} DagFileProcessor660 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:54,262] {models.py:322} DagFileProcessor660 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:54,262] {models.py:328} DagFileProcessor660 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:54.262471
[2018-04-19 21:16:54,266] {jobs.py:351} DagFileProcessor660 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:16:55,345] {jobs.py:343} DagFileProcessor661 INFO - Started process (PID=4324) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:55,350] {jobs.py:534} DagFileProcessor661 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:55,351] {jobs.py:1521} DagFileProcessor661 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:55,351] {models.py:167} DagFileProcessor661 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:55,458] {jobs.py:1535} DagFileProcessor661 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:55,478] {jobs.py:1169} DagFileProcessor661 INFO - Processing hello_world
[2018-04-19 21:16:55,486] {jobs.py:566} DagFileProcessor661 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:55,493] {models.py:322} DagFileProcessor661 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:55,493] {models.py:328} DagFileProcessor661 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:55.493442
[2018-04-19 21:16:55,497] {jobs.py:351} DagFileProcessor661 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:16:56,575] {jobs.py:343} DagFileProcessor662 INFO - Started process (PID=4325) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:56,579] {jobs.py:534} DagFileProcessor662 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:56,581] {jobs.py:1521} DagFileProcessor662 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:56,581] {models.py:167} DagFileProcessor662 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:56,694] {jobs.py:1535} DagFileProcessor662 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:56,711] {jobs.py:1169} DagFileProcessor662 INFO - Processing hello_world
[2018-04-19 21:16:56,720] {jobs.py:566} DagFileProcessor662 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:56,726] {models.py:322} DagFileProcessor662 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:56,726] {models.py:328} DagFileProcessor662 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:56.726659
[2018-04-19 21:16:56,730] {jobs.py:351} DagFileProcessor662 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:16:57,795] {jobs.py:343} DagFileProcessor663 INFO - Started process (PID=4326) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:57,799] {jobs.py:534} DagFileProcessor663 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:57,801] {jobs.py:1521} DagFileProcessor663 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:57,801] {models.py:167} DagFileProcessor663 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:57,918] {jobs.py:1535} DagFileProcessor663 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:57,941] {jobs.py:1169} DagFileProcessor663 INFO - Processing hello_world
[2018-04-19 21:16:57,950] {jobs.py:566} DagFileProcessor663 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:57,956] {models.py:322} DagFileProcessor663 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:57,957] {models.py:328} DagFileProcessor663 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:57.957174
[2018-04-19 21:16:57,960] {jobs.py:351} DagFileProcessor663 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.166 seconds
[2018-04-19 21:16:59,018] {jobs.py:343} DagFileProcessor664 INFO - Started process (PID=4327) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:59,024] {jobs.py:534} DagFileProcessor664 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:16:59,025] {jobs.py:1521} DagFileProcessor664 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:16:59,025] {models.py:167} DagFileProcessor664 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:59,136] {jobs.py:1535} DagFileProcessor664 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:16:59,159] {jobs.py:1169} DagFileProcessor664 INFO - Processing hello_world
[2018-04-19 21:16:59,169] {jobs.py:566} DagFileProcessor664 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:16:59,176] {models.py:322} DagFileProcessor664 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:16:59,176] {models.py:328} DagFileProcessor664 INFO - Failing jobs without heartbeat after 2018-04-19 21:11:59.176595
[2018-04-19 21:16:59,180] {jobs.py:351} DagFileProcessor664 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.162 seconds
[2018-04-19 21:17:00,249] {jobs.py:343} DagFileProcessor665 INFO - Started process (PID=4328) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:00,255] {jobs.py:534} DagFileProcessor665 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:00,256] {jobs.py:1521} DagFileProcessor665 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:00,256] {models.py:167} DagFileProcessor665 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:00,371] {jobs.py:1535} DagFileProcessor665 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:00,394] {jobs.py:1169} DagFileProcessor665 INFO - Processing hello_world
[2018-04-19 21:17:00,403] {jobs.py:566} DagFileProcessor665 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:00,410] {models.py:322} DagFileProcessor665 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:00,410] {models.py:328} DagFileProcessor665 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:00.410428
[2018-04-19 21:17:00,414] {jobs.py:351} DagFileProcessor665 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.165 seconds
[2018-04-19 21:17:01,484] {jobs.py:343} DagFileProcessor666 INFO - Started process (PID=4329) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:01,489] {jobs.py:534} DagFileProcessor666 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:01,490] {jobs.py:1521} DagFileProcessor666 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:01,491] {models.py:167} DagFileProcessor666 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:01,596] {jobs.py:1535} DagFileProcessor666 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:01,616] {jobs.py:1169} DagFileProcessor666 INFO - Processing hello_world
[2018-04-19 21:17:01,625] {jobs.py:566} DagFileProcessor666 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:01,630] {models.py:322} DagFileProcessor666 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:01,630] {models.py:328} DagFileProcessor666 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:01.630602
[2018-04-19 21:17:01,634] {jobs.py:351} DagFileProcessor666 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:17:02,713] {jobs.py:343} DagFileProcessor667 INFO - Started process (PID=4330) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:02,718] {jobs.py:534} DagFileProcessor667 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:02,719] {jobs.py:1521} DagFileProcessor667 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:02,719] {models.py:167} DagFileProcessor667 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:02,824] {jobs.py:1535} DagFileProcessor667 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:02,843] {jobs.py:1169} DagFileProcessor667 INFO - Processing hello_world
[2018-04-19 21:17:02,851] {jobs.py:566} DagFileProcessor667 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:02,857] {models.py:322} DagFileProcessor667 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:02,857] {models.py:328} DagFileProcessor667 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:02.857394
[2018-04-19 21:17:02,861] {jobs.py:351} DagFileProcessor667 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:17:03,940] {jobs.py:343} DagFileProcessor668 INFO - Started process (PID=4332) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:03,945] {jobs.py:534} DagFileProcessor668 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:03,946] {jobs.py:1521} DagFileProcessor668 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:03,946] {models.py:167} DagFileProcessor668 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:04,065] {jobs.py:1535} DagFileProcessor668 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:04,091] {jobs.py:1169} DagFileProcessor668 INFO - Processing hello_world
[2018-04-19 21:17:04,100] {jobs.py:566} DagFileProcessor668 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:04,106] {models.py:322} DagFileProcessor668 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:04,107] {models.py:328} DagFileProcessor668 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:04.107256
[2018-04-19 21:17:04,111] {jobs.py:351} DagFileProcessor668 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.172 seconds
[2018-04-19 21:17:05,166] {jobs.py:343} DagFileProcessor669 INFO - Started process (PID=4333) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:05,172] {jobs.py:534} DagFileProcessor669 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:05,173] {jobs.py:1521} DagFileProcessor669 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:05,173] {models.py:167} DagFileProcessor669 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:05,284] {jobs.py:1535} DagFileProcessor669 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:05,304] {jobs.py:1169} DagFileProcessor669 INFO - Processing hello_world
[2018-04-19 21:17:05,313] {jobs.py:566} DagFileProcessor669 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:05,319] {models.py:322} DagFileProcessor669 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:05,320] {models.py:328} DagFileProcessor669 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:05.320334
[2018-04-19 21:17:05,324] {jobs.py:351} DagFileProcessor669 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.158 seconds
[2018-04-19 21:17:06,402] {jobs.py:343} DagFileProcessor670 INFO - Started process (PID=4334) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:06,408] {jobs.py:534} DagFileProcessor670 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:06,409] {jobs.py:1521} DagFileProcessor670 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:06,409] {models.py:167} DagFileProcessor670 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:06,521] {jobs.py:1535} DagFileProcessor670 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:06,539] {jobs.py:1169} DagFileProcessor670 INFO - Processing hello_world
[2018-04-19 21:17:06,549] {jobs.py:566} DagFileProcessor670 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:06,557] {models.py:322} DagFileProcessor670 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:06,558] {models.py:328} DagFileProcessor670 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:06.558111
[2018-04-19 21:17:06,562] {jobs.py:351} DagFileProcessor670 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.160 seconds
[2018-04-19 21:17:07,639] {jobs.py:343} DagFileProcessor671 INFO - Started process (PID=4335) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:07,648] {jobs.py:534} DagFileProcessor671 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:07,649] {jobs.py:1521} DagFileProcessor671 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:07,649] {models.py:167} DagFileProcessor671 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:07,768] {jobs.py:1535} DagFileProcessor671 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:07,792] {jobs.py:1169} DagFileProcessor671 INFO - Processing hello_world
[2018-04-19 21:17:07,800] {jobs.py:566} DagFileProcessor671 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:07,808] {models.py:322} DagFileProcessor671 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:07,808] {models.py:328} DagFileProcessor671 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:07.808608
[2018-04-19 21:17:07,812] {jobs.py:351} DagFileProcessor671 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.173 seconds
[2018-04-19 21:17:08,864] {jobs.py:343} DagFileProcessor672 INFO - Started process (PID=4336) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:08,869] {jobs.py:534} DagFileProcessor672 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:08,870] {jobs.py:1521} DagFileProcessor672 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:08,871] {models.py:167} DagFileProcessor672 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:08,981] {jobs.py:1535} DagFileProcessor672 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:09,001] {jobs.py:1169} DagFileProcessor672 INFO - Processing hello_world
[2018-04-19 21:17:09,009] {jobs.py:566} DagFileProcessor672 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:09,014] {models.py:322} DagFileProcessor672 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:09,015] {models.py:328} DagFileProcessor672 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:09.015136
[2018-04-19 21:17:09,018] {jobs.py:351} DagFileProcessor672 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:17:10,088] {jobs.py:343} DagFileProcessor673 INFO - Started process (PID=4337) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:10,093] {jobs.py:534} DagFileProcessor673 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:10,094] {jobs.py:1521} DagFileProcessor673 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:10,095] {models.py:167} DagFileProcessor673 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:10,200] {jobs.py:1535} DagFileProcessor673 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:10,221] {jobs.py:1169} DagFileProcessor673 INFO - Processing hello_world
[2018-04-19 21:17:10,230] {jobs.py:566} DagFileProcessor673 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:10,235] {models.py:322} DagFileProcessor673 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:10,236] {models.py:328} DagFileProcessor673 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:10.236170
[2018-04-19 21:17:10,240] {jobs.py:351} DagFileProcessor673 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:17:11,316] {jobs.py:343} DagFileProcessor674 INFO - Started process (PID=4338) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:11,321] {jobs.py:534} DagFileProcessor674 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:11,322] {jobs.py:1521} DagFileProcessor674 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:11,323] {models.py:167} DagFileProcessor674 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:11,429] {jobs.py:1535} DagFileProcessor674 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:11,450] {jobs.py:1169} DagFileProcessor674 INFO - Processing hello_world
[2018-04-19 21:17:11,459] {jobs.py:566} DagFileProcessor674 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:11,465] {models.py:322} DagFileProcessor674 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:11,465] {models.py:328} DagFileProcessor674 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:11.465414
[2018-04-19 21:17:11,469] {jobs.py:351} DagFileProcessor674 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:17:12,546] {jobs.py:343} DagFileProcessor675 INFO - Started process (PID=4339) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:12,551] {jobs.py:534} DagFileProcessor675 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:12,552] {jobs.py:1521} DagFileProcessor675 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:12,552] {models.py:167} DagFileProcessor675 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:12,670] {jobs.py:1535} DagFileProcessor675 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:12,690] {jobs.py:1169} DagFileProcessor675 INFO - Processing hello_world
[2018-04-19 21:17:12,698] {jobs.py:566} DagFileProcessor675 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:12,705] {models.py:322} DagFileProcessor675 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:12,705] {models.py:328} DagFileProcessor675 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:12.705541
[2018-04-19 21:17:12,709] {jobs.py:351} DagFileProcessor675 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.163 seconds
[2018-04-19 21:17:13,782] {jobs.py:343} DagFileProcessor676 INFO - Started process (PID=4341) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:13,787] {jobs.py:534} DagFileProcessor676 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:13,789] {jobs.py:1521} DagFileProcessor676 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:13,789] {models.py:167} DagFileProcessor676 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:13,892] {jobs.py:1535} DagFileProcessor676 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:13,911] {jobs.py:1169} DagFileProcessor676 INFO - Processing hello_world
[2018-04-19 21:17:13,920] {jobs.py:566} DagFileProcessor676 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:13,925] {models.py:322} DagFileProcessor676 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:13,926] {models.py:328} DagFileProcessor676 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:13.925938
[2018-04-19 21:17:13,929] {jobs.py:351} DagFileProcessor676 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:17:15,014] {jobs.py:343} DagFileProcessor677 INFO - Started process (PID=4342) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:15,019] {jobs.py:534} DagFileProcessor677 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:15,020] {jobs.py:1521} DagFileProcessor677 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:15,021] {models.py:167} DagFileProcessor677 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:15,128] {jobs.py:1535} DagFileProcessor677 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:15,150] {jobs.py:1169} DagFileProcessor677 INFO - Processing hello_world
[2018-04-19 21:17:15,159] {jobs.py:566} DagFileProcessor677 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:15,164] {models.py:322} DagFileProcessor677 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:15,165] {models.py:328} DagFileProcessor677 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:15.165093
[2018-04-19 21:17:15,168] {jobs.py:351} DagFileProcessor677 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:17:16,238] {jobs.py:343} DagFileProcessor678 INFO - Started process (PID=4343) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:16,243] {jobs.py:534} DagFileProcessor678 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:16,244] {jobs.py:1521} DagFileProcessor678 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:16,244] {models.py:167} DagFileProcessor678 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:16,350] {jobs.py:1535} DagFileProcessor678 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:16,371] {jobs.py:1169} DagFileProcessor678 INFO - Processing hello_world
[2018-04-19 21:17:16,379] {jobs.py:566} DagFileProcessor678 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:16,386] {models.py:322} DagFileProcessor678 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:16,386] {models.py:328} DagFileProcessor678 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:16.386421
[2018-04-19 21:17:16,390] {jobs.py:351} DagFileProcessor678 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:17:17,465] {jobs.py:343} DagFileProcessor679 INFO - Started process (PID=4344) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:17,470] {jobs.py:534} DagFileProcessor679 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:17,472] {jobs.py:1521} DagFileProcessor679 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:17,472] {models.py:167} DagFileProcessor679 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:17,577] {jobs.py:1535} DagFileProcessor679 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:17,595] {jobs.py:1169} DagFileProcessor679 INFO - Processing hello_world
[2018-04-19 21:17:17,604] {jobs.py:566} DagFileProcessor679 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:17,609] {models.py:322} DagFileProcessor679 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:17,610] {models.py:328} DagFileProcessor679 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:17.610135
[2018-04-19 21:17:17,613] {jobs.py:351} DagFileProcessor679 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:17:18,691] {jobs.py:343} DagFileProcessor680 INFO - Started process (PID=4345) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:18,696] {jobs.py:534} DagFileProcessor680 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:18,697] {jobs.py:1521} DagFileProcessor680 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:18,697] {models.py:167} DagFileProcessor680 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:18,806] {jobs.py:1535} DagFileProcessor680 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:18,830] {jobs.py:1169} DagFileProcessor680 INFO - Processing hello_world
[2018-04-19 21:17:18,839] {jobs.py:566} DagFileProcessor680 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:18,845] {models.py:322} DagFileProcessor680 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:18,845] {models.py:328} DagFileProcessor680 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:18.845645
[2018-04-19 21:17:18,849] {jobs.py:351} DagFileProcessor680 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.158 seconds
[2018-04-19 21:17:19,926] {jobs.py:343} DagFileProcessor681 INFO - Started process (PID=4346) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:19,931] {jobs.py:534} DagFileProcessor681 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:19,932] {jobs.py:1521} DagFileProcessor681 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:19,932] {models.py:167} DagFileProcessor681 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:20,038] {jobs.py:1535} DagFileProcessor681 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:20,057] {jobs.py:1169} DagFileProcessor681 INFO - Processing hello_world
[2018-04-19 21:17:20,066] {jobs.py:566} DagFileProcessor681 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:20,071] {models.py:322} DagFileProcessor681 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:20,072] {models.py:328} DagFileProcessor681 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:20.071895
[2018-04-19 21:17:20,076] {jobs.py:351} DagFileProcessor681 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:17:21,164] {jobs.py:343} DagFileProcessor682 INFO - Started process (PID=4348) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:21,169] {jobs.py:534} DagFileProcessor682 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:21,170] {jobs.py:1521} DagFileProcessor682 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:21,171] {models.py:167} DagFileProcessor682 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:21,283] {jobs.py:1535} DagFileProcessor682 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:21,303] {jobs.py:1169} DagFileProcessor682 INFO - Processing hello_world
[2018-04-19 21:17:21,311] {jobs.py:566} DagFileProcessor682 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:21,316] {models.py:322} DagFileProcessor682 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:21,317] {models.py:328} DagFileProcessor682 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:21.317099
[2018-04-19 21:17:21,320] {jobs.py:351} DagFileProcessor682 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:17:22,383] {jobs.py:343} DagFileProcessor683 INFO - Started process (PID=4349) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:22,388] {jobs.py:534} DagFileProcessor683 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:22,389] {jobs.py:1521} DagFileProcessor683 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:22,389] {models.py:167} DagFileProcessor683 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:22,490] {jobs.py:1535} DagFileProcessor683 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:22,508] {jobs.py:1169} DagFileProcessor683 INFO - Processing hello_world
[2018-04-19 21:17:22,516] {jobs.py:566} DagFileProcessor683 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:22,522] {models.py:322} DagFileProcessor683 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:22,522] {models.py:328} DagFileProcessor683 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:22.522309
[2018-04-19 21:17:22,525] {jobs.py:351} DagFileProcessor683 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.142 seconds
[2018-04-19 21:17:23,612] {jobs.py:343} DagFileProcessor684 INFO - Started process (PID=4350) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:23,618] {jobs.py:534} DagFileProcessor684 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:23,619] {jobs.py:1521} DagFileProcessor684 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:23,619] {models.py:167} DagFileProcessor684 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:23,726] {jobs.py:1535} DagFileProcessor684 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:23,745] {jobs.py:1169} DagFileProcessor684 INFO - Processing hello_world
[2018-04-19 21:17:23,753] {jobs.py:566} DagFileProcessor684 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:23,759] {models.py:322} DagFileProcessor684 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:23,759] {models.py:328} DagFileProcessor684 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:23.759428
[2018-04-19 21:17:23,763] {jobs.py:351} DagFileProcessor684 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:17:24,842] {jobs.py:343} DagFileProcessor685 INFO - Started process (PID=4352) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:24,847] {jobs.py:534} DagFileProcessor685 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:24,848] {jobs.py:1521} DagFileProcessor685 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:24,849] {models.py:167} DagFileProcessor685 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:24,954] {jobs.py:1535} DagFileProcessor685 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:24,972] {jobs.py:1169} DagFileProcessor685 INFO - Processing hello_world
[2018-04-19 21:17:24,981] {jobs.py:566} DagFileProcessor685 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:24,986] {models.py:322} DagFileProcessor685 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:24,987] {models.py:328} DagFileProcessor685 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:24.987171
[2018-04-19 21:17:24,991] {jobs.py:351} DagFileProcessor685 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:17:26,072] {jobs.py:343} DagFileProcessor686 INFO - Started process (PID=4353) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:26,077] {jobs.py:534} DagFileProcessor686 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:26,078] {jobs.py:1521} DagFileProcessor686 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:26,079] {models.py:167} DagFileProcessor686 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:26,186] {jobs.py:1535} DagFileProcessor686 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:26,205] {jobs.py:1169} DagFileProcessor686 INFO - Processing hello_world
[2018-04-19 21:17:26,218] {jobs.py:566} DagFileProcessor686 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:26,224] {models.py:322} DagFileProcessor686 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:26,225] {models.py:328} DagFileProcessor686 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:26.225190
[2018-04-19 21:17:26,229] {jobs.py:351} DagFileProcessor686 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:17:27,304] {jobs.py:343} DagFileProcessor687 INFO - Started process (PID=4354) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:27,309] {jobs.py:534} DagFileProcessor687 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:27,310] {jobs.py:1521} DagFileProcessor687 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:27,310] {models.py:167} DagFileProcessor687 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:27,413] {jobs.py:1535} DagFileProcessor687 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:27,431] {jobs.py:1169} DagFileProcessor687 INFO - Processing hello_world
[2018-04-19 21:17:27,440] {jobs.py:566} DagFileProcessor687 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:27,446] {models.py:322} DagFileProcessor687 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:27,447] {models.py:328} DagFileProcessor687 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:27.446826
[2018-04-19 21:17:27,450] {jobs.py:351} DagFileProcessor687 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:17:28,538] {jobs.py:343} DagFileProcessor688 INFO - Started process (PID=4355) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:28,543] {jobs.py:534} DagFileProcessor688 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:28,544] {jobs.py:1521} DagFileProcessor688 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:28,544] {models.py:167} DagFileProcessor688 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:28,646] {jobs.py:1535} DagFileProcessor688 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:28,664] {jobs.py:1169} DagFileProcessor688 INFO - Processing hello_world
[2018-04-19 21:17:28,673] {jobs.py:566} DagFileProcessor688 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:28,680] {models.py:322} DagFileProcessor688 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:28,680] {models.py:328} DagFileProcessor688 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:28.680619
[2018-04-19 21:17:28,684] {jobs.py:351} DagFileProcessor688 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:17:29,767] {jobs.py:343} DagFileProcessor689 INFO - Started process (PID=4356) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:29,772] {jobs.py:534} DagFileProcessor689 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:29,773] {jobs.py:1521} DagFileProcessor689 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:29,774] {models.py:167} DagFileProcessor689 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:29,876] {jobs.py:1535} DagFileProcessor689 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:29,893] {jobs.py:1169} DagFileProcessor689 INFO - Processing hello_world
[2018-04-19 21:17:29,902] {jobs.py:566} DagFileProcessor689 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:29,908] {models.py:322} DagFileProcessor689 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:29,908] {models.py:328} DagFileProcessor689 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:29.908682
[2018-04-19 21:17:29,912] {jobs.py:351} DagFileProcessor689 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:17:30,999] {jobs.py:343} DagFileProcessor690 INFO - Started process (PID=4357) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:31,004] {jobs.py:534} DagFileProcessor690 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:31,005] {jobs.py:1521} DagFileProcessor690 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:31,005] {models.py:167} DagFileProcessor690 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:31,109] {jobs.py:1535} DagFileProcessor690 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:31,129] {jobs.py:1169} DagFileProcessor690 INFO - Processing hello_world
[2018-04-19 21:17:31,136] {jobs.py:566} DagFileProcessor690 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:31,142] {models.py:322} DagFileProcessor690 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:31,142] {models.py:328} DagFileProcessor690 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:31.142496
[2018-04-19 21:17:31,146] {jobs.py:351} DagFileProcessor690 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:17:32,230] {jobs.py:343} DagFileProcessor691 INFO - Started process (PID=4358) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:32,235] {jobs.py:534} DagFileProcessor691 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:32,236] {jobs.py:1521} DagFileProcessor691 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:32,237] {models.py:167} DagFileProcessor691 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:32,339] {jobs.py:1535} DagFileProcessor691 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:32,358] {jobs.py:1169} DagFileProcessor691 INFO - Processing hello_world
[2018-04-19 21:17:32,366] {jobs.py:566} DagFileProcessor691 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:32,371] {models.py:322} DagFileProcessor691 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:32,372] {models.py:328} DagFileProcessor691 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:32.372315
[2018-04-19 21:17:32,376] {jobs.py:351} DagFileProcessor691 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:17:33,462] {jobs.py:343} DagFileProcessor692 INFO - Started process (PID=4359) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:33,467] {jobs.py:534} DagFileProcessor692 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:33,468] {jobs.py:1521} DagFileProcessor692 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:33,469] {models.py:167} DagFileProcessor692 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:33,570] {jobs.py:1535} DagFileProcessor692 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:33,588] {jobs.py:1169} DagFileProcessor692 INFO - Processing hello_world
[2018-04-19 21:17:33,596] {jobs.py:566} DagFileProcessor692 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:33,602] {models.py:322} DagFileProcessor692 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:33,602] {models.py:328} DagFileProcessor692 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:33.602673
[2018-04-19 21:17:33,606] {jobs.py:351} DagFileProcessor692 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:17:34,685] {jobs.py:343} DagFileProcessor693 INFO - Started process (PID=4361) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:34,690] {jobs.py:534} DagFileProcessor693 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:34,691] {jobs.py:1521} DagFileProcessor693 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:34,692] {models.py:167} DagFileProcessor693 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:34,795] {jobs.py:1535} DagFileProcessor693 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:34,813] {jobs.py:1169} DagFileProcessor693 INFO - Processing hello_world
[2018-04-19 21:17:34,821] {jobs.py:566} DagFileProcessor693 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:34,826] {models.py:322} DagFileProcessor693 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:34,827] {models.py:328} DagFileProcessor693 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:34.827191
[2018-04-19 21:17:34,830] {jobs.py:351} DagFileProcessor693 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:17:35,914] {jobs.py:343} DagFileProcessor694 INFO - Started process (PID=4362) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:35,919] {jobs.py:534} DagFileProcessor694 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:35,920] {jobs.py:1521} DagFileProcessor694 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:35,920] {models.py:167} DagFileProcessor694 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:36,022] {jobs.py:1535} DagFileProcessor694 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:36,041] {jobs.py:1169} DagFileProcessor694 INFO - Processing hello_world
[2018-04-19 21:17:36,049] {jobs.py:566} DagFileProcessor694 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:36,054] {models.py:322} DagFileProcessor694 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:36,055] {models.py:328} DagFileProcessor694 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:36.055268
[2018-04-19 21:17:36,058] {jobs.py:351} DagFileProcessor694 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:17:37,137] {jobs.py:343} DagFileProcessor695 INFO - Started process (PID=4363) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:37,142] {jobs.py:534} DagFileProcessor695 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:37,143] {jobs.py:1521} DagFileProcessor695 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:37,144] {models.py:167} DagFileProcessor695 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:37,246] {jobs.py:1535} DagFileProcessor695 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:37,264] {jobs.py:1169} DagFileProcessor695 INFO - Processing hello_world
[2018-04-19 21:17:37,273] {jobs.py:566} DagFileProcessor695 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:37,278] {models.py:322} DagFileProcessor695 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:37,279] {models.py:328} DagFileProcessor695 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:37.279039
[2018-04-19 21:17:37,282] {jobs.py:351} DagFileProcessor695 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:17:38,377] {jobs.py:343} DagFileProcessor696 INFO - Started process (PID=4364) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:38,381] {jobs.py:534} DagFileProcessor696 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:38,383] {jobs.py:1521} DagFileProcessor696 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:38,383] {models.py:167} DagFileProcessor696 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:38,485] {jobs.py:1535} DagFileProcessor696 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:38,502] {jobs.py:1169} DagFileProcessor696 INFO - Processing hello_world
[2018-04-19 21:17:38,511] {jobs.py:566} DagFileProcessor696 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:38,516] {models.py:322} DagFileProcessor696 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:38,517] {models.py:328} DagFileProcessor696 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:38.516954
[2018-04-19 21:17:38,520] {jobs.py:351} DagFileProcessor696 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:17:39,611] {jobs.py:343} DagFileProcessor697 INFO - Started process (PID=4365) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:39,616] {jobs.py:534} DagFileProcessor697 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:39,617] {jobs.py:1521} DagFileProcessor697 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:39,617] {models.py:167} DagFileProcessor697 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:39,719] {jobs.py:1535} DagFileProcessor697 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:39,736] {jobs.py:1169} DagFileProcessor697 INFO - Processing hello_world
[2018-04-19 21:17:39,745] {jobs.py:566} DagFileProcessor697 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:39,751] {models.py:322} DagFileProcessor697 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:39,752] {models.py:328} DagFileProcessor697 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:39.752089
[2018-04-19 21:17:39,755] {jobs.py:351} DagFileProcessor697 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:17:40,837] {jobs.py:343} DagFileProcessor698 INFO - Started process (PID=4366) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:40,841] {jobs.py:534} DagFileProcessor698 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:40,842] {jobs.py:1521} DagFileProcessor698 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:40,843] {models.py:167} DagFileProcessor698 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:40,944] {jobs.py:1535} DagFileProcessor698 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:40,964] {jobs.py:1169} DagFileProcessor698 INFO - Processing hello_world
[2018-04-19 21:17:40,972] {jobs.py:566} DagFileProcessor698 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:40,977] {models.py:322} DagFileProcessor698 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:40,977] {models.py:328} DagFileProcessor698 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:40.977704
[2018-04-19 21:17:40,982] {jobs.py:351} DagFileProcessor698 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:17:42,063] {jobs.py:343} DagFileProcessor699 INFO - Started process (PID=4367) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:42,068] {jobs.py:534} DagFileProcessor699 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:42,069] {jobs.py:1521} DagFileProcessor699 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:42,069] {models.py:167} DagFileProcessor699 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:42,171] {jobs.py:1535} DagFileProcessor699 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:42,189] {jobs.py:1169} DagFileProcessor699 INFO - Processing hello_world
[2018-04-19 21:17:42,197] {jobs.py:566} DagFileProcessor699 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:42,202] {models.py:322} DagFileProcessor699 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:42,203] {models.py:328} DagFileProcessor699 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:42.203229
[2018-04-19 21:17:42,207] {jobs.py:351} DagFileProcessor699 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:17:43,286] {jobs.py:343} DagFileProcessor700 INFO - Started process (PID=4375) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:43,290] {jobs.py:534} DagFileProcessor700 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:43,291] {jobs.py:1521} DagFileProcessor700 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:43,292] {models.py:167} DagFileProcessor700 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:43,394] {jobs.py:1535} DagFileProcessor700 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:43,413] {jobs.py:1169} DagFileProcessor700 INFO - Processing hello_world
[2018-04-19 21:17:43,423] {jobs.py:566} DagFileProcessor700 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:43,428] {models.py:322} DagFileProcessor700 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:43,429] {models.py:328} DagFileProcessor700 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:43.429153
[2018-04-19 21:17:43,432] {jobs.py:351} DagFileProcessor700 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:17:44,519] {jobs.py:343} DagFileProcessor701 INFO - Started process (PID=4377) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:44,524] {jobs.py:534} DagFileProcessor701 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:44,525] {jobs.py:1521} DagFileProcessor701 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:44,525] {models.py:167} DagFileProcessor701 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:44,631] {jobs.py:1535} DagFileProcessor701 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:44,651] {jobs.py:1169} DagFileProcessor701 INFO - Processing hello_world
[2018-04-19 21:17:44,659] {jobs.py:566} DagFileProcessor701 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:44,664] {models.py:322} DagFileProcessor701 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:44,665] {models.py:328} DagFileProcessor701 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:44.665177
[2018-04-19 21:17:44,668] {jobs.py:351} DagFileProcessor701 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:17:45,748] {jobs.py:343} DagFileProcessor702 INFO - Started process (PID=4378) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:45,753] {jobs.py:534} DagFileProcessor702 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:45,754] {jobs.py:1521} DagFileProcessor702 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:45,755] {models.py:167} DagFileProcessor702 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:45,859] {jobs.py:1535} DagFileProcessor702 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:45,878] {jobs.py:1169} DagFileProcessor702 INFO - Processing hello_world
[2018-04-19 21:17:45,886] {jobs.py:566} DagFileProcessor702 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:45,892] {models.py:322} DagFileProcessor702 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:45,892] {models.py:328} DagFileProcessor702 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:45.892329
[2018-04-19 21:17:45,895] {jobs.py:351} DagFileProcessor702 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:17:46,977] {jobs.py:343} DagFileProcessor703 INFO - Started process (PID=4379) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:46,982] {jobs.py:534} DagFileProcessor703 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:46,983] {jobs.py:1521} DagFileProcessor703 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:46,984] {models.py:167} DagFileProcessor703 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:47,089] {jobs.py:1535} DagFileProcessor703 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:47,108] {jobs.py:1169} DagFileProcessor703 INFO - Processing hello_world
[2018-04-19 21:17:47,117] {jobs.py:566} DagFileProcessor703 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:47,123] {models.py:322} DagFileProcessor703 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:47,123] {models.py:328} DagFileProcessor703 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:47.123380
[2018-04-19 21:17:47,127] {jobs.py:351} DagFileProcessor703 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:17:48,206] {jobs.py:343} DagFileProcessor704 INFO - Started process (PID=4380) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:48,211] {jobs.py:534} DagFileProcessor704 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:48,212] {jobs.py:1521} DagFileProcessor704 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:48,213] {models.py:167} DagFileProcessor704 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:48,320] {jobs.py:1535} DagFileProcessor704 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:48,342] {jobs.py:1169} DagFileProcessor704 INFO - Processing hello_world
[2018-04-19 21:17:48,351] {jobs.py:566} DagFileProcessor704 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:48,356] {models.py:322} DagFileProcessor704 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:48,356] {models.py:328} DagFileProcessor704 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:48.356774
[2018-04-19 21:17:48,360] {jobs.py:351} DagFileProcessor704 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:17:49,435] {jobs.py:343} DagFileProcessor705 INFO - Started process (PID=4381) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:49,440] {jobs.py:534} DagFileProcessor705 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:49,441] {jobs.py:1521} DagFileProcessor705 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:49,441] {models.py:167} DagFileProcessor705 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:49,547] {jobs.py:1535} DagFileProcessor705 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:49,565] {jobs.py:1169} DagFileProcessor705 INFO - Processing hello_world
[2018-04-19 21:17:49,574] {jobs.py:566} DagFileProcessor705 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:49,579] {models.py:322} DagFileProcessor705 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:49,580] {models.py:328} DagFileProcessor705 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:49.580148
[2018-04-19 21:17:49,583] {jobs.py:351} DagFileProcessor705 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:17:50,675] {jobs.py:343} DagFileProcessor706 INFO - Started process (PID=4382) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:50,680] {jobs.py:534} DagFileProcessor706 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:50,681] {jobs.py:1521} DagFileProcessor706 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:50,681] {models.py:167} DagFileProcessor706 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:50,786] {jobs.py:1535} DagFileProcessor706 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:50,805] {jobs.py:1169} DagFileProcessor706 INFO - Processing hello_world
[2018-04-19 21:17:50,814] {jobs.py:566} DagFileProcessor706 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:50,819] {models.py:322} DagFileProcessor706 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:50,819] {models.py:328} DagFileProcessor706 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:50.819753
[2018-04-19 21:17:50,823] {jobs.py:351} DagFileProcessor706 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:17:51,898] {jobs.py:343} DagFileProcessor707 INFO - Started process (PID=4383) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:51,903] {jobs.py:534} DagFileProcessor707 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:51,904] {jobs.py:1521} DagFileProcessor707 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:51,905] {models.py:167} DagFileProcessor707 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:52,008] {jobs.py:1535} DagFileProcessor707 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:52,026] {jobs.py:1169} DagFileProcessor707 INFO - Processing hello_world
[2018-04-19 21:17:52,034] {jobs.py:566} DagFileProcessor707 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:52,040] {models.py:322} DagFileProcessor707 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:52,040] {models.py:328} DagFileProcessor707 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:52.040434
[2018-04-19 21:17:52,044] {jobs.py:351} DagFileProcessor707 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:17:53,131] {jobs.py:343} DagFileProcessor708 INFO - Started process (PID=4384) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:53,136] {jobs.py:534} DagFileProcessor708 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:53,137] {jobs.py:1521} DagFileProcessor708 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:53,137] {models.py:167} DagFileProcessor708 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:53,242] {jobs.py:1535} DagFileProcessor708 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:53,262] {jobs.py:1169} DagFileProcessor708 INFO - Processing hello_world
[2018-04-19 21:17:53,270] {jobs.py:566} DagFileProcessor708 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:53,279] {models.py:322} DagFileProcessor708 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:53,279] {models.py:328} DagFileProcessor708 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:53.279629
[2018-04-19 21:17:53,284] {jobs.py:351} DagFileProcessor708 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:17:54,366] {jobs.py:343} DagFileProcessor709 INFO - Started process (PID=4386) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:54,371] {jobs.py:534} DagFileProcessor709 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:54,372] {jobs.py:1521} DagFileProcessor709 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:54,372] {models.py:167} DagFileProcessor709 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:54,474] {jobs.py:1535} DagFileProcessor709 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:54,494] {jobs.py:1169} DagFileProcessor709 INFO - Processing hello_world
[2018-04-19 21:17:54,502] {jobs.py:566} DagFileProcessor709 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:54,507] {models.py:322} DagFileProcessor709 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:54,508] {models.py:328} DagFileProcessor709 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:54.508266
[2018-04-19 21:17:54,511] {jobs.py:351} DagFileProcessor709 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:17:55,595] {jobs.py:343} DagFileProcessor710 INFO - Started process (PID=4387) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:55,599] {jobs.py:534} DagFileProcessor710 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:55,600] {jobs.py:1521} DagFileProcessor710 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:55,601] {models.py:167} DagFileProcessor710 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:55,700] {jobs.py:1535} DagFileProcessor710 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:55,718] {jobs.py:1169} DagFileProcessor710 INFO - Processing hello_world
[2018-04-19 21:17:55,727] {jobs.py:566} DagFileProcessor710 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:55,732] {models.py:322} DagFileProcessor710 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:55,733] {models.py:328} DagFileProcessor710 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:55.732901
[2018-04-19 21:17:55,736] {jobs.py:351} DagFileProcessor710 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.141 seconds
[2018-04-19 21:17:56,827] {jobs.py:343} DagFileProcessor711 INFO - Started process (PID=4388) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:56,835] {jobs.py:534} DagFileProcessor711 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:56,836] {jobs.py:1521} DagFileProcessor711 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:56,837] {models.py:167} DagFileProcessor711 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:56,940] {jobs.py:1535} DagFileProcessor711 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:56,959] {jobs.py:1169} DagFileProcessor711 INFO - Processing hello_world
[2018-04-19 21:17:56,968] {jobs.py:566} DagFileProcessor711 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:56,974] {models.py:322} DagFileProcessor711 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:56,974] {models.py:328} DagFileProcessor711 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:56.974431
[2018-04-19 21:17:56,978] {jobs.py:351} DagFileProcessor711 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:17:58,057] {jobs.py:343} DagFileProcessor712 INFO - Started process (PID=4389) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:58,062] {jobs.py:534} DagFileProcessor712 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:58,063] {jobs.py:1521} DagFileProcessor712 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:58,063] {models.py:167} DagFileProcessor712 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:58,165] {jobs.py:1535} DagFileProcessor712 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:58,186] {jobs.py:1169} DagFileProcessor712 INFO - Processing hello_world
[2018-04-19 21:17:58,195] {jobs.py:566} DagFileProcessor712 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:58,200] {models.py:322} DagFileProcessor712 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:58,200] {models.py:328} DagFileProcessor712 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:58.200623
[2018-04-19 21:17:58,204] {jobs.py:351} DagFileProcessor712 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:17:59,290] {jobs.py:343} DagFileProcessor713 INFO - Started process (PID=4390) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:59,294] {jobs.py:534} DagFileProcessor713 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:17:59,295] {jobs.py:1521} DagFileProcessor713 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:17:59,296] {models.py:167} DagFileProcessor713 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:59,395] {jobs.py:1535} DagFileProcessor713 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:17:59,416] {jobs.py:1169} DagFileProcessor713 INFO - Processing hello_world
[2018-04-19 21:17:59,424] {jobs.py:566} DagFileProcessor713 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:17:59,429] {models.py:322} DagFileProcessor713 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:17:59,430] {models.py:328} DagFileProcessor713 INFO - Failing jobs without heartbeat after 2018-04-19 21:12:59.429845
[2018-04-19 21:17:59,433] {jobs.py:351} DagFileProcessor713 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:18:00,525] {jobs.py:343} DagFileProcessor714 INFO - Started process (PID=4391) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:00,530] {jobs.py:534} DagFileProcessor714 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:00,531] {jobs.py:1521} DagFileProcessor714 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:00,532] {models.py:167} DagFileProcessor714 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:00,633] {jobs.py:1535} DagFileProcessor714 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:00,653] {jobs.py:1169} DagFileProcessor714 INFO - Processing hello_world
[2018-04-19 21:18:00,662] {jobs.py:566} DagFileProcessor714 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:00,667] {models.py:322} DagFileProcessor714 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:00,667] {models.py:328} DagFileProcessor714 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:00.667652
[2018-04-19 21:18:00,671] {jobs.py:351} DagFileProcessor714 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:18:01,756] {jobs.py:343} DagFileProcessor715 INFO - Started process (PID=4392) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:01,761] {jobs.py:534} DagFileProcessor715 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:01,762] {jobs.py:1521} DagFileProcessor715 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:01,763] {models.py:167} DagFileProcessor715 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:01,866] {jobs.py:1535} DagFileProcessor715 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:01,888] {jobs.py:1169} DagFileProcessor715 INFO - Processing hello_world
[2018-04-19 21:18:01,896] {jobs.py:566} DagFileProcessor715 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:01,901] {models.py:322} DagFileProcessor715 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:01,902] {models.py:328} DagFileProcessor715 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:01.901920
[2018-04-19 21:18:01,905] {jobs.py:351} DagFileProcessor715 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:18:02,988] {jobs.py:343} DagFileProcessor716 INFO - Started process (PID=4393) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:02,993] {jobs.py:534} DagFileProcessor716 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:02,994] {jobs.py:1521} DagFileProcessor716 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:02,994] {models.py:167} DagFileProcessor716 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:03,091] {jobs.py:1535} DagFileProcessor716 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:03,108] {jobs.py:1169} DagFileProcessor716 INFO - Processing hello_world
[2018-04-19 21:18:03,117] {jobs.py:566} DagFileProcessor716 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:03,122] {models.py:322} DagFileProcessor716 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:03,123] {models.py:328} DagFileProcessor716 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:03.123032
[2018-04-19 21:18:03,126] {jobs.py:351} DagFileProcessor716 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.138 seconds
[2018-04-19 21:18:04,220] {jobs.py:343} DagFileProcessor717 INFO - Started process (PID=4395) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:04,225] {jobs.py:534} DagFileProcessor717 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:04,226] {jobs.py:1521} DagFileProcessor717 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:04,227] {models.py:167} DagFileProcessor717 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:04,333] {jobs.py:1535} DagFileProcessor717 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:04,352] {jobs.py:1169} DagFileProcessor717 INFO - Processing hello_world
[2018-04-19 21:18:04,361] {jobs.py:566} DagFileProcessor717 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:04,366] {models.py:322} DagFileProcessor717 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:04,367] {models.py:328} DagFileProcessor717 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:04.367055
[2018-04-19 21:18:04,370] {jobs.py:351} DagFileProcessor717 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:18:05,455] {jobs.py:343} DagFileProcessor718 INFO - Started process (PID=4396) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:05,460] {jobs.py:534} DagFileProcessor718 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:05,461] {jobs.py:1521} DagFileProcessor718 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:05,462] {models.py:167} DagFileProcessor718 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:05,561] {jobs.py:1535} DagFileProcessor718 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:05,580] {jobs.py:1169} DagFileProcessor718 INFO - Processing hello_world
[2018-04-19 21:18:05,588] {jobs.py:566} DagFileProcessor718 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:05,595] {models.py:322} DagFileProcessor718 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:05,596] {models.py:328} DagFileProcessor718 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:05.596350
[2018-04-19 21:18:05,601] {jobs.py:351} DagFileProcessor718 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:18:06,685] {jobs.py:343} DagFileProcessor719 INFO - Started process (PID=4397) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:06,690] {jobs.py:534} DagFileProcessor719 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:06,691] {jobs.py:1521} DagFileProcessor719 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:06,691] {models.py:167} DagFileProcessor719 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:06,789] {jobs.py:1535} DagFileProcessor719 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:06,807] {jobs.py:1169} DagFileProcessor719 INFO - Processing hello_world
[2018-04-19 21:18:06,816] {jobs.py:566} DagFileProcessor719 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:06,824] {models.py:322} DagFileProcessor719 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:06,825] {models.py:328} DagFileProcessor719 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:06.825092
[2018-04-19 21:18:06,829] {jobs.py:351} DagFileProcessor719 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.145 seconds
[2018-04-19 21:18:07,916] {jobs.py:343} DagFileProcessor720 INFO - Started process (PID=4398) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:07,921] {jobs.py:534} DagFileProcessor720 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:07,922] {jobs.py:1521} DagFileProcessor720 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:07,923] {models.py:167} DagFileProcessor720 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:08,023] {jobs.py:1535} DagFileProcessor720 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:08,043] {jobs.py:1169} DagFileProcessor720 INFO - Processing hello_world
[2018-04-19 21:18:08,051] {jobs.py:566} DagFileProcessor720 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:08,056] {models.py:322} DagFileProcessor720 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:08,057] {models.py:328} DagFileProcessor720 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:08.057254
[2018-04-19 21:18:08,060] {jobs.py:351} DagFileProcessor720 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:18:09,152] {jobs.py:343} DagFileProcessor721 INFO - Started process (PID=4399) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:09,158] {jobs.py:534} DagFileProcessor721 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:09,159] {jobs.py:1521} DagFileProcessor721 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:09,159] {models.py:167} DagFileProcessor721 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:09,268] {jobs.py:1535} DagFileProcessor721 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:09,288] {jobs.py:1169} DagFileProcessor721 INFO - Processing hello_world
[2018-04-19 21:18:09,298] {jobs.py:566} DagFileProcessor721 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:09,303] {models.py:322} DagFileProcessor721 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:09,304] {models.py:328} DagFileProcessor721 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:09.304091
[2018-04-19 21:18:09,308] {jobs.py:351} DagFileProcessor721 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:18:10,384] {jobs.py:343} DagFileProcessor722 INFO - Started process (PID=4400) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:10,389] {jobs.py:534} DagFileProcessor722 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:10,391] {jobs.py:1521} DagFileProcessor722 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:10,391] {models.py:167} DagFileProcessor722 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:10,502] {jobs.py:1535} DagFileProcessor722 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:10,523] {jobs.py:1169} DagFileProcessor722 INFO - Processing hello_world
[2018-04-19 21:18:10,531] {jobs.py:566} DagFileProcessor722 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:10,537] {models.py:322} DagFileProcessor722 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:10,537] {models.py:328} DagFileProcessor722 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:10.537643
[2018-04-19 21:18:10,541] {jobs.py:351} DagFileProcessor722 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:18:11,617] {jobs.py:343} DagFileProcessor723 INFO - Started process (PID=4401) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:11,623] {jobs.py:534} DagFileProcessor723 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:11,624] {jobs.py:1521} DagFileProcessor723 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:11,625] {models.py:167} DagFileProcessor723 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:11,728] {jobs.py:1535} DagFileProcessor723 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:11,749] {jobs.py:1169} DagFileProcessor723 INFO - Processing hello_world
[2018-04-19 21:18:11,758] {jobs.py:566} DagFileProcessor723 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:11,764] {models.py:322} DagFileProcessor723 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:11,764] {models.py:328} DagFileProcessor723 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:11.764333
[2018-04-19 21:18:11,767] {jobs.py:351} DagFileProcessor723 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:18:12,847] {jobs.py:343} DagFileProcessor724 INFO - Started process (PID=4402) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:12,852] {jobs.py:534} DagFileProcessor724 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:12,854] {jobs.py:1521} DagFileProcessor724 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:12,854] {models.py:167} DagFileProcessor724 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:12,966] {jobs.py:1535} DagFileProcessor724 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:12,985] {jobs.py:1169} DagFileProcessor724 INFO - Processing hello_world
[2018-04-19 21:18:12,995] {jobs.py:566} DagFileProcessor724 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:13,001] {models.py:322} DagFileProcessor724 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:13,002] {models.py:328} DagFileProcessor724 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:13.001859
[2018-04-19 21:18:13,006] {jobs.py:351} DagFileProcessor724 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:18:14,070] {jobs.py:343} DagFileProcessor725 INFO - Started process (PID=4404) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:14,075] {jobs.py:534} DagFileProcessor725 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:14,076] {jobs.py:1521} DagFileProcessor725 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:14,077] {models.py:167} DagFileProcessor725 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:14,189] {jobs.py:1535} DagFileProcessor725 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:14,209] {jobs.py:1169} DagFileProcessor725 INFO - Processing hello_world
[2018-04-19 21:18:14,217] {jobs.py:566} DagFileProcessor725 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:14,223] {models.py:322} DagFileProcessor725 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:14,224] {models.py:328} DagFileProcessor725 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:14.224174
[2018-04-19 21:18:14,227] {jobs.py:351} DagFileProcessor725 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.158 seconds
[2018-04-19 21:18:15,307] {jobs.py:343} DagFileProcessor726 INFO - Started process (PID=4405) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:15,311] {jobs.py:534} DagFileProcessor726 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:15,312] {jobs.py:1521} DagFileProcessor726 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:15,313] {models.py:167} DagFileProcessor726 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:15,425] {jobs.py:1535} DagFileProcessor726 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:15,443] {jobs.py:1169} DagFileProcessor726 INFO - Processing hello_world
[2018-04-19 21:18:15,452] {jobs.py:566} DagFileProcessor726 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:15,460] {models.py:322} DagFileProcessor726 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:15,460] {models.py:328} DagFileProcessor726 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:15.460502
[2018-04-19 21:18:15,464] {jobs.py:351} DagFileProcessor726 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:18:16,538] {jobs.py:343} DagFileProcessor727 INFO - Started process (PID=4406) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:16,544] {jobs.py:534} DagFileProcessor727 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:16,545] {jobs.py:1521} DagFileProcessor727 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:16,546] {models.py:167} DagFileProcessor727 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:16,652] {jobs.py:1535} DagFileProcessor727 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:16,674] {jobs.py:1169} DagFileProcessor727 INFO - Processing hello_world
[2018-04-19 21:18:16,685] {jobs.py:566} DagFileProcessor727 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:16,692] {models.py:322} DagFileProcessor727 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:16,693] {models.py:328} DagFileProcessor727 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:16.693124
[2018-04-19 21:18:16,696] {jobs.py:351} DagFileProcessor727 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:18:17,773] {jobs.py:343} DagFileProcessor728 INFO - Started process (PID=4407) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:17,778] {jobs.py:534} DagFileProcessor728 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:17,779] {jobs.py:1521} DagFileProcessor728 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:17,779] {models.py:167} DagFileProcessor728 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:17,878] {jobs.py:1535} DagFileProcessor728 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:17,896] {jobs.py:1169} DagFileProcessor728 INFO - Processing hello_world
[2018-04-19 21:18:17,905] {jobs.py:566} DagFileProcessor728 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:17,910] {models.py:322} DagFileProcessor728 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:17,911] {models.py:328} DagFileProcessor728 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:17.911041
[2018-04-19 21:18:17,915] {jobs.py:351} DagFileProcessor728 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.142 seconds
[2018-04-19 21:18:19,000] {jobs.py:343} DagFileProcessor729 INFO - Started process (PID=4408) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:19,005] {jobs.py:534} DagFileProcessor729 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:19,006] {jobs.py:1521} DagFileProcessor729 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:19,007] {models.py:167} DagFileProcessor729 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:19,105] {jobs.py:1535} DagFileProcessor729 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:19,125] {jobs.py:1169} DagFileProcessor729 INFO - Processing hello_world
[2018-04-19 21:18:19,133] {jobs.py:566} DagFileProcessor729 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:19,138] {models.py:322} DagFileProcessor729 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:19,139] {models.py:328} DagFileProcessor729 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:19.139041
[2018-04-19 21:18:19,143] {jobs.py:351} DagFileProcessor729 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:18:20,227] {jobs.py:343} DagFileProcessor730 INFO - Started process (PID=4409) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:20,232] {jobs.py:534} DagFileProcessor730 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:20,233] {jobs.py:1521} DagFileProcessor730 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:20,233] {models.py:167} DagFileProcessor730 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:20,341] {jobs.py:1535} DagFileProcessor730 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:20,361] {jobs.py:1169} DagFileProcessor730 INFO - Processing hello_world
[2018-04-19 21:18:20,370] {jobs.py:566} DagFileProcessor730 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:20,377] {models.py:322} DagFileProcessor730 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:20,378] {models.py:328} DagFileProcessor730 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:20.377828
[2018-04-19 21:18:20,381] {jobs.py:351} DagFileProcessor730 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:18:21,460] {jobs.py:343} DagFileProcessor731 INFO - Started process (PID=4410) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:21,465] {jobs.py:534} DagFileProcessor731 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:21,467] {jobs.py:1521} DagFileProcessor731 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:21,467] {models.py:167} DagFileProcessor731 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:21,567] {jobs.py:1535} DagFileProcessor731 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:21,589] {jobs.py:1169} DagFileProcessor731 INFO - Processing hello_world
[2018-04-19 21:18:21,597] {jobs.py:566} DagFileProcessor731 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:21,603] {models.py:322} DagFileProcessor731 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:21,603] {models.py:328} DagFileProcessor731 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:21.603407
[2018-04-19 21:18:21,607] {jobs.py:351} DagFileProcessor731 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:18:22,693] {jobs.py:343} DagFileProcessor732 INFO - Started process (PID=4411) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:22,697] {jobs.py:534} DagFileProcessor732 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:22,698] {jobs.py:1521} DagFileProcessor732 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:22,699] {models.py:167} DagFileProcessor732 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:22,802] {jobs.py:1535} DagFileProcessor732 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:22,823] {jobs.py:1169} DagFileProcessor732 INFO - Processing hello_world
[2018-04-19 21:18:22,832] {jobs.py:566} DagFileProcessor732 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:22,837] {models.py:322} DagFileProcessor732 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:22,838] {models.py:328} DagFileProcessor732 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:22.838201
[2018-04-19 21:18:22,841] {jobs.py:351} DagFileProcessor732 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:18:23,919] {jobs.py:343} DagFileProcessor733 INFO - Started process (PID=4413) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:23,924] {jobs.py:534} DagFileProcessor733 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:23,925] {jobs.py:1521} DagFileProcessor733 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:23,925] {models.py:167} DagFileProcessor733 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:24,032] {jobs.py:1535} DagFileProcessor733 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:24,053] {jobs.py:1169} DagFileProcessor733 INFO - Processing hello_world
[2018-04-19 21:18:24,062] {jobs.py:566} DagFileProcessor733 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:24,068] {models.py:322} DagFileProcessor733 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:24,068] {models.py:328} DagFileProcessor733 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:24.068764
[2018-04-19 21:18:24,072] {jobs.py:351} DagFileProcessor733 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:18:25,149] {jobs.py:343} DagFileProcessor734 INFO - Started process (PID=4414) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:25,154] {jobs.py:534} DagFileProcessor734 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:25,155] {jobs.py:1521} DagFileProcessor734 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:25,156] {models.py:167} DagFileProcessor734 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:25,258] {jobs.py:1535} DagFileProcessor734 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:25,276] {jobs.py:1169} DagFileProcessor734 INFO - Processing hello_world
[2018-04-19 21:18:25,285] {jobs.py:566} DagFileProcessor734 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:25,291] {models.py:322} DagFileProcessor734 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:25,292] {models.py:328} DagFileProcessor734 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:25.291908
[2018-04-19 21:18:25,295] {jobs.py:351} DagFileProcessor734 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:18:26,378] {jobs.py:343} DagFileProcessor735 INFO - Started process (PID=4415) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:26,388] {jobs.py:534} DagFileProcessor735 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:26,390] {jobs.py:1521} DagFileProcessor735 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:26,390] {models.py:167} DagFileProcessor735 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:26,496] {jobs.py:1535} DagFileProcessor735 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:26,516] {jobs.py:1169} DagFileProcessor735 INFO - Processing hello_world
[2018-04-19 21:18:26,524] {jobs.py:566} DagFileProcessor735 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:26,530] {models.py:322} DagFileProcessor735 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:26,530] {models.py:328} DagFileProcessor735 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:26.530754
[2018-04-19 21:18:26,534] {jobs.py:351} DagFileProcessor735 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:18:27,609] {jobs.py:343} DagFileProcessor736 INFO - Started process (PID=4416) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:27,614] {jobs.py:534} DagFileProcessor736 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:27,615] {jobs.py:1521} DagFileProcessor736 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:27,616] {models.py:167} DagFileProcessor736 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:27,722] {jobs.py:1535} DagFileProcessor736 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:27,744] {jobs.py:1169} DagFileProcessor736 INFO - Processing hello_world
[2018-04-19 21:18:27,753] {jobs.py:566} DagFileProcessor736 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:27,759] {models.py:322} DagFileProcessor736 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:27,759] {models.py:328} DagFileProcessor736 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:27.759533
[2018-04-19 21:18:27,763] {jobs.py:351} DagFileProcessor736 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:18:28,835] {jobs.py:343} DagFileProcessor737 INFO - Started process (PID=4417) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:28,839] {jobs.py:534} DagFileProcessor737 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:28,841] {jobs.py:1521} DagFileProcessor737 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:28,841] {models.py:167} DagFileProcessor737 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:28,947] {jobs.py:1535} DagFileProcessor737 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:28,967] {jobs.py:1169} DagFileProcessor737 INFO - Processing hello_world
[2018-04-19 21:18:28,975] {jobs.py:566} DagFileProcessor737 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:28,981] {models.py:322} DagFileProcessor737 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:28,982] {models.py:328} DagFileProcessor737 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:28.982190
[2018-04-19 21:18:28,985] {jobs.py:351} DagFileProcessor737 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:18:30,066] {jobs.py:343} DagFileProcessor738 INFO - Started process (PID=4418) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:30,071] {jobs.py:534} DagFileProcessor738 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:30,072] {jobs.py:1521} DagFileProcessor738 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:30,073] {models.py:167} DagFileProcessor738 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:30,169] {jobs.py:1535} DagFileProcessor738 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:30,191] {jobs.py:1169} DagFileProcessor738 INFO - Processing hello_world
[2018-04-19 21:18:30,200] {jobs.py:566} DagFileProcessor738 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:30,205] {models.py:322} DagFileProcessor738 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:30,206] {models.py:328} DagFileProcessor738 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:30.206199
[2018-04-19 21:18:30,210] {jobs.py:351} DagFileProcessor738 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:18:31,299] {jobs.py:343} DagFileProcessor739 INFO - Started process (PID=4419) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:31,304] {jobs.py:534} DagFileProcessor739 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:31,305] {jobs.py:1521} DagFileProcessor739 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:31,305] {models.py:167} DagFileProcessor739 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:31,408] {jobs.py:1535} DagFileProcessor739 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:31,424] {jobs.py:1169} DagFileProcessor739 INFO - Processing hello_world
[2018-04-19 21:18:31,433] {jobs.py:566} DagFileProcessor739 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:31,438] {models.py:322} DagFileProcessor739 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:31,438] {models.py:328} DagFileProcessor739 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:31.438482
[2018-04-19 21:18:31,442] {jobs.py:351} DagFileProcessor739 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:18:32,524] {jobs.py:343} DagFileProcessor740 INFO - Started process (PID=4420) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:32,529] {jobs.py:534} DagFileProcessor740 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:32,530] {jobs.py:1521} DagFileProcessor740 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:32,531] {models.py:167} DagFileProcessor740 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:32,637] {jobs.py:1535} DagFileProcessor740 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:32,656] {jobs.py:1169} DagFileProcessor740 INFO - Processing hello_world
[2018-04-19 21:18:32,665] {jobs.py:566} DagFileProcessor740 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:32,672] {models.py:322} DagFileProcessor740 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:32,672] {models.py:328} DagFileProcessor740 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:32.672465
[2018-04-19 21:18:32,676] {jobs.py:351} DagFileProcessor740 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:18:33,761] {jobs.py:343} DagFileProcessor741 INFO - Started process (PID=4421) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:33,766] {jobs.py:534} DagFileProcessor741 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:33,767] {jobs.py:1521} DagFileProcessor741 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:33,768] {models.py:167} DagFileProcessor741 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:33,868] {jobs.py:1535} DagFileProcessor741 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:33,890] {jobs.py:1169} DagFileProcessor741 INFO - Processing hello_world
[2018-04-19 21:18:33,899] {jobs.py:566} DagFileProcessor741 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:33,904] {models.py:322} DagFileProcessor741 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:33,905] {models.py:328} DagFileProcessor741 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:33.905093
[2018-04-19 21:18:33,909] {jobs.py:351} DagFileProcessor741 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:18:34,989] {jobs.py:343} DagFileProcessor742 INFO - Started process (PID=4423) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:34,994] {jobs.py:534} DagFileProcessor742 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:34,995] {jobs.py:1521} DagFileProcessor742 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:34,995] {models.py:167} DagFileProcessor742 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:35,100] {jobs.py:1535} DagFileProcessor742 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:35,120] {jobs.py:1169} DagFileProcessor742 INFO - Processing hello_world
[2018-04-19 21:18:35,129] {jobs.py:566} DagFileProcessor742 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:35,135] {models.py:322} DagFileProcessor742 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:35,135] {models.py:328} DagFileProcessor742 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:35.135526
[2018-04-19 21:18:35,139] {jobs.py:351} DagFileProcessor742 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:18:36,223] {jobs.py:343} DagFileProcessor743 INFO - Started process (PID=4424) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:36,228] {jobs.py:534} DagFileProcessor743 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:36,229] {jobs.py:1521} DagFileProcessor743 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:36,229] {models.py:167} DagFileProcessor743 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:36,329] {jobs.py:1535} DagFileProcessor743 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:36,350] {jobs.py:1169} DagFileProcessor743 INFO - Processing hello_world
[2018-04-19 21:18:36,358] {jobs.py:566} DagFileProcessor743 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:36,363] {models.py:322} DagFileProcessor743 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:36,363] {models.py:328} DagFileProcessor743 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:36.363601
[2018-04-19 21:18:36,366] {jobs.py:351} DagFileProcessor743 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:18:37,449] {jobs.py:343} DagFileProcessor744 INFO - Started process (PID=4425) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:37,454] {jobs.py:534} DagFileProcessor744 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:37,455] {jobs.py:1521} DagFileProcessor744 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:37,456] {models.py:167} DagFileProcessor744 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:37,561] {jobs.py:1535} DagFileProcessor744 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:37,581] {jobs.py:1169} DagFileProcessor744 INFO - Processing hello_world
[2018-04-19 21:18:37,590] {jobs.py:566} DagFileProcessor744 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:37,596] {models.py:322} DagFileProcessor744 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:37,597] {models.py:328} DagFileProcessor744 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:37.596893
[2018-04-19 21:18:37,600] {jobs.py:351} DagFileProcessor744 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:18:38,678] {jobs.py:343} DagFileProcessor745 INFO - Started process (PID=4426) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:38,683] {jobs.py:534} DagFileProcessor745 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:38,684] {jobs.py:1521} DagFileProcessor745 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:38,684] {models.py:167} DagFileProcessor745 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:38,784] {jobs.py:1535} DagFileProcessor745 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:38,805] {jobs.py:1169} DagFileProcessor745 INFO - Processing hello_world
[2018-04-19 21:18:38,814] {jobs.py:566} DagFileProcessor745 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:38,819] {models.py:322} DagFileProcessor745 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:38,820] {models.py:328} DagFileProcessor745 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:38.820138
[2018-04-19 21:18:38,823] {jobs.py:351} DagFileProcessor745 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:18:39,912] {jobs.py:343} DagFileProcessor746 INFO - Started process (PID=4427) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:39,917] {jobs.py:534} DagFileProcessor746 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:39,918] {jobs.py:1521} DagFileProcessor746 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:39,919] {models.py:167} DagFileProcessor746 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:40,019] {jobs.py:1535} DagFileProcessor746 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:40,037] {jobs.py:1169} DagFileProcessor746 INFO - Processing hello_world
[2018-04-19 21:18:40,046] {jobs.py:566} DagFileProcessor746 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:40,051] {models.py:322} DagFileProcessor746 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:40,052] {models.py:328} DagFileProcessor746 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:40.052052
[2018-04-19 21:18:40,055] {jobs.py:351} DagFileProcessor746 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.143 seconds
[2018-04-19 21:18:41,138] {jobs.py:343} DagFileProcessor747 INFO - Started process (PID=4428) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:41,143] {jobs.py:534} DagFileProcessor747 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:41,145] {jobs.py:1521} DagFileProcessor747 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:41,145] {models.py:167} DagFileProcessor747 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:41,249] {jobs.py:1535} DagFileProcessor747 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:41,269] {jobs.py:1169} DagFileProcessor747 INFO - Processing hello_world
[2018-04-19 21:18:41,278] {jobs.py:566} DagFileProcessor747 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:41,284] {models.py:322} DagFileProcessor747 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:41,284] {models.py:328} DagFileProcessor747 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:41.284485
[2018-04-19 21:18:41,288] {jobs.py:351} DagFileProcessor747 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:18:42,370] {jobs.py:343} DagFileProcessor748 INFO - Started process (PID=4429) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:42,374] {jobs.py:534} DagFileProcessor748 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:42,376] {jobs.py:1521} DagFileProcessor748 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:42,376] {models.py:167} DagFileProcessor748 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:42,487] {jobs.py:1535} DagFileProcessor748 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:42,507] {jobs.py:1169} DagFileProcessor748 INFO - Processing hello_world
[2018-04-19 21:18:42,520] {jobs.py:566} DagFileProcessor748 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:42,526] {models.py:322} DagFileProcessor748 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:42,527] {models.py:328} DagFileProcessor748 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:42.527005
[2018-04-19 21:18:42,530] {jobs.py:351} DagFileProcessor748 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.161 seconds
[2018-04-19 21:18:43,598] {jobs.py:343} DagFileProcessor749 INFO - Started process (PID=4430) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:43,602] {jobs.py:534} DagFileProcessor749 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:43,604] {jobs.py:1521} DagFileProcessor749 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:43,604] {models.py:167} DagFileProcessor749 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:43,710] {jobs.py:1535} DagFileProcessor749 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:43,732] {jobs.py:1169} DagFileProcessor749 INFO - Processing hello_world
[2018-04-19 21:18:43,741] {jobs.py:566} DagFileProcessor749 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:43,747] {models.py:322} DagFileProcessor749 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:43,748] {models.py:328} DagFileProcessor749 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:43.748018
[2018-04-19 21:18:43,752] {jobs.py:351} DagFileProcessor749 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:18:44,822] {jobs.py:343} DagFileProcessor750 INFO - Started process (PID=4432) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:44,827] {jobs.py:534} DagFileProcessor750 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:44,828] {jobs.py:1521} DagFileProcessor750 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:44,828] {models.py:167} DagFileProcessor750 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:44,933] {jobs.py:1535} DagFileProcessor750 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:44,952] {jobs.py:1169} DagFileProcessor750 INFO - Processing hello_world
[2018-04-19 21:18:44,961] {jobs.py:566} DagFileProcessor750 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:44,967] {models.py:322} DagFileProcessor750 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:44,968] {models.py:328} DagFileProcessor750 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:44.968128
[2018-04-19 21:18:44,971] {jobs.py:351} DagFileProcessor750 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:18:46,055] {jobs.py:343} DagFileProcessor751 INFO - Started process (PID=4440) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:46,061] {jobs.py:534} DagFileProcessor751 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:46,062] {jobs.py:1521} DagFileProcessor751 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:46,062] {models.py:167} DagFileProcessor751 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:46,171] {jobs.py:1535} DagFileProcessor751 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:46,190] {jobs.py:1169} DagFileProcessor751 INFO - Processing hello_world
[2018-04-19 21:18:46,199] {jobs.py:566} DagFileProcessor751 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:46,205] {models.py:322} DagFileProcessor751 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:46,205] {models.py:328} DagFileProcessor751 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:46.205612
[2018-04-19 21:18:46,209] {jobs.py:351} DagFileProcessor751 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:18:47,282] {jobs.py:343} DagFileProcessor752 INFO - Started process (PID=4441) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:47,287] {jobs.py:534} DagFileProcessor752 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:47,288] {jobs.py:1521} DagFileProcessor752 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:47,289] {models.py:167} DagFileProcessor752 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:47,392] {jobs.py:1535} DagFileProcessor752 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:47,413] {jobs.py:1169} DagFileProcessor752 INFO - Processing hello_world
[2018-04-19 21:18:47,423] {jobs.py:566} DagFileProcessor752 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:47,430] {models.py:322} DagFileProcessor752 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:47,431] {models.py:328} DagFileProcessor752 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:47.431308
[2018-04-19 21:18:47,434] {jobs.py:351} DagFileProcessor752 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.152 seconds
[2018-04-19 21:18:48,515] {jobs.py:343} DagFileProcessor753 INFO - Started process (PID=4442) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:48,520] {jobs.py:534} DagFileProcessor753 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:48,521] {jobs.py:1521} DagFileProcessor753 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:48,521] {models.py:167} DagFileProcessor753 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:48,622] {jobs.py:1535} DagFileProcessor753 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:48,641] {jobs.py:1169} DagFileProcessor753 INFO - Processing hello_world
[2018-04-19 21:18:48,650] {jobs.py:566} DagFileProcessor753 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:48,655] {models.py:322} DagFileProcessor753 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:48,656] {models.py:328} DagFileProcessor753 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:48.656111
[2018-04-19 21:18:48,659] {jobs.py:351} DagFileProcessor753 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:18:49,739] {jobs.py:343} DagFileProcessor754 INFO - Started process (PID=4443) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:49,745] {jobs.py:534} DagFileProcessor754 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:49,746] {jobs.py:1521} DagFileProcessor754 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:49,746] {models.py:167} DagFileProcessor754 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:49,853] {jobs.py:1535} DagFileProcessor754 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:49,874] {jobs.py:1169} DagFileProcessor754 INFO - Processing hello_world
[2018-04-19 21:18:49,885] {jobs.py:566} DagFileProcessor754 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:49,891] {models.py:322} DagFileProcessor754 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:49,891] {models.py:328} DagFileProcessor754 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:49.891451
[2018-04-19 21:18:49,895] {jobs.py:351} DagFileProcessor754 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:18:50,965] {jobs.py:343} DagFileProcessor755 INFO - Started process (PID=4444) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:50,970] {jobs.py:534} DagFileProcessor755 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:50,971] {jobs.py:1521} DagFileProcessor755 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:50,971] {models.py:167} DagFileProcessor755 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:51,080] {jobs.py:1535} DagFileProcessor755 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:51,100] {jobs.py:1169} DagFileProcessor755 INFO - Processing hello_world
[2018-04-19 21:18:51,109] {jobs.py:566} DagFileProcessor755 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:51,115] {models.py:322} DagFileProcessor755 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:51,115] {models.py:328} DagFileProcessor755 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:51.115677
[2018-04-19 21:18:51,119] {jobs.py:351} DagFileProcessor755 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:18:52,190] {jobs.py:343} DagFileProcessor756 INFO - Started process (PID=4445) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:52,196] {jobs.py:534} DagFileProcessor756 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:52,197] {jobs.py:1521} DagFileProcessor756 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:52,197] {models.py:167} DagFileProcessor756 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:52,300] {jobs.py:1535} DagFileProcessor756 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:52,318] {jobs.py:1169} DagFileProcessor756 INFO - Processing hello_world
[2018-04-19 21:18:52,327] {jobs.py:566} DagFileProcessor756 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:52,332] {models.py:322} DagFileProcessor756 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:52,333] {models.py:328} DagFileProcessor756 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:52.332961
[2018-04-19 21:18:52,336] {jobs.py:351} DagFileProcessor756 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:18:53,414] {jobs.py:343} DagFileProcessor757 INFO - Started process (PID=4446) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:53,419] {jobs.py:534} DagFileProcessor757 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:53,420] {jobs.py:1521} DagFileProcessor757 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:53,420] {models.py:167} DagFileProcessor757 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:53,523] {jobs.py:1535} DagFileProcessor757 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:53,543] {jobs.py:1169} DagFileProcessor757 INFO - Processing hello_world
[2018-04-19 21:18:53,552] {jobs.py:566} DagFileProcessor757 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:53,558] {models.py:322} DagFileProcessor757 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:53,558] {models.py:328} DagFileProcessor757 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:53.558381
[2018-04-19 21:18:53,562] {jobs.py:351} DagFileProcessor757 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:18:54,646] {jobs.py:343} DagFileProcessor758 INFO - Started process (PID=4448) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:54,651] {jobs.py:534} DagFileProcessor758 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:54,652] {jobs.py:1521} DagFileProcessor758 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:54,653] {models.py:167} DagFileProcessor758 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:54,757] {jobs.py:1535} DagFileProcessor758 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:54,775] {jobs.py:1169} DagFileProcessor758 INFO - Processing hello_world
[2018-04-19 21:18:54,783] {jobs.py:566} DagFileProcessor758 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:54,789] {models.py:322} DagFileProcessor758 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:54,789] {models.py:328} DagFileProcessor758 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:54.789663
[2018-04-19 21:18:54,794] {jobs.py:351} DagFileProcessor758 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:18:55,871] {jobs.py:343} DagFileProcessor759 INFO - Started process (PID=4449) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:55,876] {jobs.py:534} DagFileProcessor759 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:55,877] {jobs.py:1521} DagFileProcessor759 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:55,877] {models.py:167} DagFileProcessor759 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:55,985] {jobs.py:1535} DagFileProcessor759 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:56,001] {jobs.py:1169} DagFileProcessor759 INFO - Processing hello_world
[2018-04-19 21:18:56,010] {jobs.py:566} DagFileProcessor759 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:56,016] {models.py:322} DagFileProcessor759 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:56,016] {models.py:328} DagFileProcessor759 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:56.016609
[2018-04-19 21:18:56,021] {jobs.py:351} DagFileProcessor759 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:18:57,104] {jobs.py:343} DagFileProcessor760 INFO - Started process (PID=4450) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:57,112] {jobs.py:534} DagFileProcessor760 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:57,113] {jobs.py:1521} DagFileProcessor760 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:57,114] {models.py:167} DagFileProcessor760 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:57,221] {jobs.py:1535} DagFileProcessor760 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:57,241] {jobs.py:1169} DagFileProcessor760 INFO - Processing hello_world
[2018-04-19 21:18:57,251] {jobs.py:566} DagFileProcessor760 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:57,257] {models.py:322} DagFileProcessor760 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:57,258] {models.py:328} DagFileProcessor760 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:57.257911
[2018-04-19 21:18:57,261] {jobs.py:351} DagFileProcessor760 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:18:58,341] {jobs.py:343} DagFileProcessor761 INFO - Started process (PID=4451) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:58,346] {jobs.py:534} DagFileProcessor761 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:58,347] {jobs.py:1521} DagFileProcessor761 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:58,348] {models.py:167} DagFileProcessor761 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:58,457] {jobs.py:1535} DagFileProcessor761 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:58,476] {jobs.py:1169} DagFileProcessor761 INFO - Processing hello_world
[2018-04-19 21:18:58,486] {jobs.py:566} DagFileProcessor761 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:58,491] {models.py:322} DagFileProcessor761 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:58,491] {models.py:328} DagFileProcessor761 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:58.491651
[2018-04-19 21:18:58,495] {jobs.py:351} DagFileProcessor761 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.155 seconds
[2018-04-19 21:18:59,564] {jobs.py:343} DagFileProcessor762 INFO - Started process (PID=4452) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:59,569] {jobs.py:534} DagFileProcessor762 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:18:59,571] {jobs.py:1521} DagFileProcessor762 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:18:59,571] {models.py:167} DagFileProcessor762 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:59,671] {jobs.py:1535} DagFileProcessor762 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:18:59,691] {jobs.py:1169} DagFileProcessor762 INFO - Processing hello_world
[2018-04-19 21:18:59,699] {jobs.py:566} DagFileProcessor762 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:18:59,705] {models.py:322} DagFileProcessor762 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:18:59,705] {models.py:328} DagFileProcessor762 INFO - Failing jobs without heartbeat after 2018-04-19 21:13:59.705472
[2018-04-19 21:18:59,708] {jobs.py:351} DagFileProcessor762 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:19:00,793] {jobs.py:343} DagFileProcessor763 INFO - Started process (PID=4453) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:00,799] {jobs.py:534} DagFileProcessor763 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:00,800] {jobs.py:1521} DagFileProcessor763 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:00,800] {models.py:167} DagFileProcessor763 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:00,907] {jobs.py:1535} DagFileProcessor763 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:00,927] {jobs.py:1169} DagFileProcessor763 INFO - Processing hello_world
[2018-04-19 21:19:00,937] {jobs.py:566} DagFileProcessor763 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:00,942] {models.py:322} DagFileProcessor763 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:00,943] {models.py:328} DagFileProcessor763 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:00.942935
[2018-04-19 21:19:00,947] {jobs.py:351} DagFileProcessor763 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:19:02,022] {jobs.py:343} DagFileProcessor764 INFO - Started process (PID=4454) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:02,027] {jobs.py:534} DagFileProcessor764 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:02,028] {jobs.py:1521} DagFileProcessor764 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:02,028] {models.py:167} DagFileProcessor764 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:02,135] {jobs.py:1535} DagFileProcessor764 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:02,154] {jobs.py:1169} DagFileProcessor764 INFO - Processing hello_world
[2018-04-19 21:19:02,163] {jobs.py:566} DagFileProcessor764 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:02,169] {models.py:322} DagFileProcessor764 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:02,169] {models.py:328} DagFileProcessor764 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:02.169697
[2018-04-19 21:19:02,173] {jobs.py:351} DagFileProcessor764 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:19:03,244] {jobs.py:343} DagFileProcessor765 INFO - Started process (PID=4455) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:03,250] {jobs.py:534} DagFileProcessor765 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:03,251] {jobs.py:1521} DagFileProcessor765 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:03,251] {models.py:167} DagFileProcessor765 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:03,361] {jobs.py:1535} DagFileProcessor765 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:03,384] {jobs.py:1169} DagFileProcessor765 INFO - Processing hello_world
[2018-04-19 21:19:03,392] {jobs.py:566} DagFileProcessor765 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:03,399] {models.py:322} DagFileProcessor765 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:03,400] {models.py:328} DagFileProcessor765 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:03.400036
[2018-04-19 21:19:03,403] {jobs.py:351} DagFileProcessor765 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.159 seconds
[2018-04-19 21:19:04,486] {jobs.py:343} DagFileProcessor766 INFO - Started process (PID=4457) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:04,491] {jobs.py:534} DagFileProcessor766 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:04,492] {jobs.py:1521} DagFileProcessor766 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:04,492] {models.py:167} DagFileProcessor766 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:04,602] {jobs.py:1535} DagFileProcessor766 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:04,622] {jobs.py:1169} DagFileProcessor766 INFO - Processing hello_world
[2018-04-19 21:19:04,631] {jobs.py:566} DagFileProcessor766 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:04,637] {models.py:322} DagFileProcessor766 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:04,637] {models.py:328} DagFileProcessor766 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:04.637670
[2018-04-19 21:19:04,641] {jobs.py:351} DagFileProcessor766 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.155 seconds
[2018-04-19 21:19:05,709] {jobs.py:343} DagFileProcessor767 INFO - Started process (PID=4458) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:05,714] {jobs.py:534} DagFileProcessor767 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:05,715] {jobs.py:1521} DagFileProcessor767 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:05,716] {models.py:167} DagFileProcessor767 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:05,823] {jobs.py:1535} DagFileProcessor767 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:05,843] {jobs.py:1169} DagFileProcessor767 INFO - Processing hello_world
[2018-04-19 21:19:05,852] {jobs.py:566} DagFileProcessor767 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:05,858] {models.py:322} DagFileProcessor767 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:05,859] {models.py:328} DagFileProcessor767 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:05.859073
[2018-04-19 21:19:05,862] {jobs.py:351} DagFileProcessor767 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:19:06,938] {jobs.py:343} DagFileProcessor768 INFO - Started process (PID=4459) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:06,942] {jobs.py:534} DagFileProcessor768 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:06,943] {jobs.py:1521} DagFileProcessor768 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:06,944] {models.py:167} DagFileProcessor768 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:07,060] {jobs.py:1535} DagFileProcessor768 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:07,082] {jobs.py:1169} DagFileProcessor768 INFO - Processing hello_world
[2018-04-19 21:19:07,091] {jobs.py:566} DagFileProcessor768 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:07,098] {models.py:322} DagFileProcessor768 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:07,098] {models.py:328} DagFileProcessor768 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:07.098786
[2018-04-19 21:19:07,102] {jobs.py:351} DagFileProcessor768 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.165 seconds
[2018-04-19 21:19:08,161] {jobs.py:343} DagFileProcessor769 INFO - Started process (PID=4460) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:08,166] {jobs.py:534} DagFileProcessor769 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:08,167] {jobs.py:1521} DagFileProcessor769 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:08,168] {models.py:167} DagFileProcessor769 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:08,279] {jobs.py:1535} DagFileProcessor769 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:08,303] {jobs.py:1169} DagFileProcessor769 INFO - Processing hello_world
[2018-04-19 21:19:08,312] {jobs.py:566} DagFileProcessor769 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:08,318] {models.py:322} DagFileProcessor769 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:08,319] {models.py:328} DagFileProcessor769 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:08.319247
[2018-04-19 21:19:08,323] {jobs.py:351} DagFileProcessor769 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.162 seconds
[2018-04-19 21:19:09,394] {jobs.py:343} DagFileProcessor770 INFO - Started process (PID=4461) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:09,400] {jobs.py:534} DagFileProcessor770 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:09,401] {jobs.py:1521} DagFileProcessor770 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:09,402] {models.py:167} DagFileProcessor770 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:09,514] {jobs.py:1535} DagFileProcessor770 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:09,538] {jobs.py:1169} DagFileProcessor770 INFO - Processing hello_world
[2018-04-19 21:19:09,548] {jobs.py:566} DagFileProcessor770 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:09,554] {models.py:322} DagFileProcessor770 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:09,554] {models.py:328} DagFileProcessor770 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:09.554605
[2018-04-19 21:19:09,558] {jobs.py:351} DagFileProcessor770 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.163 seconds
[2018-04-19 21:19:10,626] {jobs.py:343} DagFileProcessor771 INFO - Started process (PID=4462) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:10,631] {jobs.py:534} DagFileProcessor771 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:10,633] {jobs.py:1521} DagFileProcessor771 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:10,633] {models.py:167} DagFileProcessor771 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:10,741] {jobs.py:1535} DagFileProcessor771 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:10,761] {jobs.py:1169} DagFileProcessor771 INFO - Processing hello_world
[2018-04-19 21:19:10,770] {jobs.py:566} DagFileProcessor771 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:10,776] {models.py:322} DagFileProcessor771 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:10,776] {models.py:328} DagFileProcessor771 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:10.776456
[2018-04-19 21:19:10,780] {jobs.py:351} DagFileProcessor771 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.154 seconds
[2018-04-19 21:19:11,850] {jobs.py:343} DagFileProcessor772 INFO - Started process (PID=4463) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:11,855] {jobs.py:534} DagFileProcessor772 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:11,856] {jobs.py:1521} DagFileProcessor772 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:11,856] {models.py:167} DagFileProcessor772 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:11,971] {jobs.py:1535} DagFileProcessor772 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:11,992] {jobs.py:1169} DagFileProcessor772 INFO - Processing hello_world
[2018-04-19 21:19:12,002] {jobs.py:566} DagFileProcessor772 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:12,008] {models.py:322} DagFileProcessor772 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:12,008] {models.py:328} DagFileProcessor772 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:12.008625
[2018-04-19 21:19:12,012] {jobs.py:351} DagFileProcessor772 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.162 seconds
[2018-04-19 21:19:13,078] {jobs.py:343} DagFileProcessor773 INFO - Started process (PID=4464) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:13,083] {jobs.py:534} DagFileProcessor773 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:13,085] {jobs.py:1521} DagFileProcessor773 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:13,085] {models.py:167} DagFileProcessor773 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:13,194] {jobs.py:1535} DagFileProcessor773 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:13,213] {jobs.py:1169} DagFileProcessor773 INFO - Processing hello_world
[2018-04-19 21:19:13,223] {jobs.py:566} DagFileProcessor773 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:13,229] {models.py:322} DagFileProcessor773 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:13,230] {models.py:328} DagFileProcessor773 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:13.230017
[2018-04-19 21:19:13,236] {jobs.py:351} DagFileProcessor773 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.158 seconds
[2018-04-19 21:19:14,307] {jobs.py:343} DagFileProcessor774 INFO - Started process (PID=4466) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:14,312] {jobs.py:534} DagFileProcessor774 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:14,313] {jobs.py:1521} DagFileProcessor774 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:14,314] {models.py:167} DagFileProcessor774 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:14,425] {jobs.py:1535} DagFileProcessor774 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:14,447] {jobs.py:1169} DagFileProcessor774 INFO - Processing hello_world
[2018-04-19 21:19:14,456] {jobs.py:566} DagFileProcessor774 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:14,462] {models.py:322} DagFileProcessor774 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:14,463] {models.py:328} DagFileProcessor774 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:14.463119
[2018-04-19 21:19:14,467] {jobs.py:351} DagFileProcessor774 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.160 seconds
[2018-04-19 21:19:15,529] {jobs.py:343} DagFileProcessor775 INFO - Started process (PID=4468) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:15,534] {jobs.py:534} DagFileProcessor775 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:15,535] {jobs.py:1521} DagFileProcessor775 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:15,536] {models.py:167} DagFileProcessor775 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:15,653] {jobs.py:1535} DagFileProcessor775 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:15,672] {jobs.py:1169} DagFileProcessor775 INFO - Processing hello_world
[2018-04-19 21:19:15,680] {jobs.py:566} DagFileProcessor775 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:15,686] {models.py:322} DagFileProcessor775 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:15,687] {models.py:328} DagFileProcessor775 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:15.687057
[2018-04-19 21:19:15,690] {jobs.py:351} DagFileProcessor775 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.161 seconds
[2018-04-19 21:19:16,770] {jobs.py:343} DagFileProcessor776 INFO - Started process (PID=4469) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:16,775] {jobs.py:534} DagFileProcessor776 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:16,776] {jobs.py:1521} DagFileProcessor776 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:16,776] {models.py:167} DagFileProcessor776 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:16,896] {jobs.py:1535} DagFileProcessor776 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:16,918] {jobs.py:1169} DagFileProcessor776 INFO - Processing hello_world
[2018-04-19 21:19:16,926] {jobs.py:566} DagFileProcessor776 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:16,933] {models.py:322} DagFileProcessor776 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:16,933] {models.py:328} DagFileProcessor776 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:16.933379
[2018-04-19 21:19:16,937] {jobs.py:351} DagFileProcessor776 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.167 seconds
[2018-04-19 21:19:17,996] {jobs.py:343} DagFileProcessor777 INFO - Started process (PID=4470) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:18,001] {jobs.py:534} DagFileProcessor777 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:18,002] {jobs.py:1521} DagFileProcessor777 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:18,003] {models.py:167} DagFileProcessor777 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:18,111] {jobs.py:1535} DagFileProcessor777 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:18,133] {jobs.py:1169} DagFileProcessor777 INFO - Processing hello_world
[2018-04-19 21:19:18,142] {jobs.py:566} DagFileProcessor777 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:18,147] {models.py:322} DagFileProcessor777 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:18,148] {models.py:328} DagFileProcessor777 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:18.148224
[2018-04-19 21:19:18,153] {jobs.py:351} DagFileProcessor777 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.157 seconds
[2018-04-19 21:19:19,223] {jobs.py:343} DagFileProcessor778 INFO - Started process (PID=4471) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:19,228] {jobs.py:534} DagFileProcessor778 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:19,229] {jobs.py:1521} DagFileProcessor778 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:19,230] {models.py:167} DagFileProcessor778 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:19,337] {jobs.py:1535} DagFileProcessor778 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:19,359] {jobs.py:1169} DagFileProcessor778 INFO - Processing hello_world
[2018-04-19 21:19:19,370] {jobs.py:566} DagFileProcessor778 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:19,377] {models.py:322} DagFileProcessor778 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:19,378] {models.py:328} DagFileProcessor778 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:19.378060
[2018-04-19 21:19:19,382] {jobs.py:351} DagFileProcessor778 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.158 seconds
[2018-04-19 21:19:20,455] {jobs.py:343} DagFileProcessor779 INFO - Started process (PID=4479) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:20,460] {jobs.py:534} DagFileProcessor779 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:20,461] {jobs.py:1521} DagFileProcessor779 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:20,461] {models.py:167} DagFileProcessor779 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:20,564] {jobs.py:1535} DagFileProcessor779 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:20,584] {jobs.py:1169} DagFileProcessor779 INFO - Processing hello_world
[2018-04-19 21:19:20,592] {jobs.py:566} DagFileProcessor779 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:20,600] {models.py:322} DagFileProcessor779 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:20,601] {models.py:328} DagFileProcessor779 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:20.600823
[2018-04-19 21:19:20,605] {jobs.py:351} DagFileProcessor779 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.150 seconds
[2018-04-19 21:19:21,679] {jobs.py:343} DagFileProcessor780 INFO - Started process (PID=4480) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:21,683] {jobs.py:534} DagFileProcessor780 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:21,684] {jobs.py:1521} DagFileProcessor780 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:21,685] {models.py:167} DagFileProcessor780 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:21,788] {jobs.py:1535} DagFileProcessor780 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:21,808] {jobs.py:1169} DagFileProcessor780 INFO - Processing hello_world
[2018-04-19 21:19:21,816] {jobs.py:566} DagFileProcessor780 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:21,821] {models.py:322} DagFileProcessor780 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:21,822] {models.py:328} DagFileProcessor780 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:21.821943
[2018-04-19 21:19:21,825] {jobs.py:351} DagFileProcessor780 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.146 seconds
[2018-04-19 21:19:22,913] {jobs.py:343} DagFileProcessor781 INFO - Started process (PID=4482) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:22,918] {jobs.py:534} DagFileProcessor781 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:22,919] {jobs.py:1521} DagFileProcessor781 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:22,919] {models.py:167} DagFileProcessor781 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:23,024] {jobs.py:1535} DagFileProcessor781 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:23,057] {jobs.py:1169} DagFileProcessor781 INFO - Processing hello_world
[2018-04-19 21:19:23,066] {jobs.py:566} DagFileProcessor781 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:23,071] {models.py:322} DagFileProcessor781 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:23,071] {models.py:328} DagFileProcessor781 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:23.071796
[2018-04-19 21:19:23,075] {jobs.py:351} DagFileProcessor781 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.162 seconds
[2018-04-19 21:19:24,146] {jobs.py:343} DagFileProcessor782 INFO - Started process (PID=4484) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:24,151] {jobs.py:534} DagFileProcessor782 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:24,152] {jobs.py:1521} DagFileProcessor782 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:24,152] {models.py:167} DagFileProcessor782 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:24,258] {jobs.py:1535} DagFileProcessor782 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:24,279] {jobs.py:1169} DagFileProcessor782 INFO - Processing hello_world
[2018-04-19 21:19:24,288] {jobs.py:566} DagFileProcessor782 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:24,293] {models.py:322} DagFileProcessor782 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:24,294] {models.py:328} DagFileProcessor782 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:24.294244
[2018-04-19 21:19:24,297] {jobs.py:351} DagFileProcessor782 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
[2018-04-19 21:19:25,373] {jobs.py:343} DagFileProcessor783 INFO - Started process (PID=4486) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:25,378] {jobs.py:534} DagFileProcessor783 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:25,379] {jobs.py:1521} DagFileProcessor783 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:25,379] {models.py:167} DagFileProcessor783 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:25,483] {jobs.py:1535} DagFileProcessor783 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:25,503] {jobs.py:1169} DagFileProcessor783 INFO - Processing hello_world
[2018-04-19 21:19:25,511] {jobs.py:566} DagFileProcessor783 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:25,516] {models.py:322} DagFileProcessor783 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:25,517] {models.py:328} DagFileProcessor783 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:25.517166
[2018-04-19 21:19:25,520] {jobs.py:351} DagFileProcessor783 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:19:26,601] {jobs.py:343} DagFileProcessor784 INFO - Started process (PID=4487) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:26,613] {jobs.py:534} DagFileProcessor784 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:26,614] {jobs.py:1521} DagFileProcessor784 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:26,615] {models.py:167} DagFileProcessor784 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:26,718] {jobs.py:1535} DagFileProcessor784 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:26,738] {jobs.py:1169} DagFileProcessor784 INFO - Processing hello_world
[2018-04-19 21:19:26,748] {jobs.py:566} DagFileProcessor784 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:26,753] {models.py:322} DagFileProcessor784 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:26,753] {models.py:328} DagFileProcessor784 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:26.753744
[2018-04-19 21:19:26,757] {jobs.py:351} DagFileProcessor784 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.156 seconds
[2018-04-19 21:19:27,830] {jobs.py:343} DagFileProcessor785 INFO - Started process (PID=4488) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:27,835] {jobs.py:534} DagFileProcessor785 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:27,836] {jobs.py:1521} DagFileProcessor785 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:27,836] {models.py:167} DagFileProcessor785 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:27,941] {jobs.py:1535} DagFileProcessor785 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:27,962] {jobs.py:1169} DagFileProcessor785 INFO - Processing hello_world
[2018-04-19 21:19:27,970] {jobs.py:566} DagFileProcessor785 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:27,975] {models.py:322} DagFileProcessor785 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:27,976] {models.py:328} DagFileProcessor785 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:27.975832
[2018-04-19 21:19:27,979] {jobs.py:351} DagFileProcessor785 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:19:29,069] {jobs.py:343} DagFileProcessor786 INFO - Started process (PID=4489) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:29,074] {jobs.py:534} DagFileProcessor786 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:29,075] {jobs.py:1521} DagFileProcessor786 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:29,075] {models.py:167} DagFileProcessor786 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:29,177] {jobs.py:1535} DagFileProcessor786 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:29,195] {jobs.py:1169} DagFileProcessor786 INFO - Processing hello_world
[2018-04-19 21:19:29,204] {jobs.py:566} DagFileProcessor786 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:29,209] {models.py:322} DagFileProcessor786 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:29,210] {models.py:328} DagFileProcessor786 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:29.210030
[2018-04-19 21:19:29,213] {jobs.py:351} DagFileProcessor786 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.144 seconds
[2018-04-19 21:19:30,295] {jobs.py:343} DagFileProcessor787 INFO - Started process (PID=4490) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:30,300] {jobs.py:534} DagFileProcessor787 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:30,302] {jobs.py:1521} DagFileProcessor787 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:30,302] {models.py:167} DagFileProcessor787 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:30,407] {jobs.py:1535} DagFileProcessor787 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:30,425] {jobs.py:1169} DagFileProcessor787 INFO - Processing hello_world
[2018-04-19 21:19:30,437] {jobs.py:566} DagFileProcessor787 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:30,444] {models.py:322} DagFileProcessor787 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:30,445] {models.py:328} DagFileProcessor787 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:30.445025
[2018-04-19 21:19:30,448] {jobs.py:351} DagFileProcessor787 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.153 seconds
[2018-04-19 21:19:31,526] {jobs.py:343} DagFileProcessor788 INFO - Started process (PID=4491) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:31,531] {jobs.py:534} DagFileProcessor788 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:31,532] {jobs.py:1521} DagFileProcessor788 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:31,532] {models.py:167} DagFileProcessor788 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:31,635] {jobs.py:1535} DagFileProcessor788 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:31,655] {jobs.py:1169} DagFileProcessor788 INFO - Processing hello_world
[2018-04-19 21:19:31,663] {jobs.py:566} DagFileProcessor788 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:31,668] {models.py:322} DagFileProcessor788 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:31,669] {models.py:328} DagFileProcessor788 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:31.669054
[2018-04-19 21:19:31,673] {jobs.py:351} DagFileProcessor788 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:19:32,760] {jobs.py:343} DagFileProcessor789 INFO - Started process (PID=4493) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:32,765] {jobs.py:534} DagFileProcessor789 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:32,766] {jobs.py:1521} DagFileProcessor789 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:32,766] {models.py:167} DagFileProcessor789 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:32,869] {jobs.py:1535} DagFileProcessor789 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:32,887] {jobs.py:1169} DagFileProcessor789 INFO - Processing hello_world
[2018-04-19 21:19:32,896] {jobs.py:566} DagFileProcessor789 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:32,903] {models.py:322} DagFileProcessor789 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:32,903] {models.py:328} DagFileProcessor789 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:32.903632
[2018-04-19 21:19:32,908] {jobs.py:351} DagFileProcessor789 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.147 seconds
[2018-04-19 21:19:33,990] {jobs.py:343} DagFileProcessor790 INFO - Started process (PID=4494) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:33,995] {jobs.py:534} DagFileProcessor790 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:33,996] {jobs.py:1521} DagFileProcessor790 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:33,996] {models.py:167} DagFileProcessor790 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:34,102] {jobs.py:1535} DagFileProcessor790 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:34,121] {jobs.py:1169} DagFileProcessor790 INFO - Processing hello_world
[2018-04-19 21:19:34,130] {jobs.py:566} DagFileProcessor790 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:34,135] {models.py:322} DagFileProcessor790 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:34,135] {models.py:328} DagFileProcessor790 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:34.135810
[2018-04-19 21:19:34,139] {jobs.py:351} DagFileProcessor790 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.149 seconds
[2018-04-19 21:19:35,228] {jobs.py:343} DagFileProcessor791 INFO - Started process (PID=4496) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:35,233] {jobs.py:534} DagFileProcessor791 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:35,234] {jobs.py:1521} DagFileProcessor791 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:35,234] {models.py:167} DagFileProcessor791 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:35,340] {jobs.py:1535} DagFileProcessor791 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:35,359] {jobs.py:1169} DagFileProcessor791 INFO - Processing hello_world
[2018-04-19 21:19:35,367] {jobs.py:566} DagFileProcessor791 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:35,372] {models.py:322} DagFileProcessor791 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:35,373] {models.py:328} DagFileProcessor791 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:35.372967
[2018-04-19 21:19:35,376] {jobs.py:351} DagFileProcessor791 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.148 seconds
[2018-04-19 21:19:36,462] {jobs.py:343} DagFileProcessor792 INFO - Started process (PID=4497) to work on /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:36,466] {jobs.py:534} DagFileProcessor792 ERROR - Cannot use more than 1 thread when using sqlite. Setting max_threads to 1
[2018-04-19 21:19:36,468] {jobs.py:1521} DagFileProcessor792 INFO - Processing file /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py for tasks to queue
[2018-04-19 21:19:36,468] {models.py:167} DagFileProcessor792 INFO - Filling up the DagBag from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:36,572] {jobs.py:1535} DagFileProcessor792 INFO - DAG(s) dict_keys(['hello_world']) retrieved from /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py
[2018-04-19 21:19:36,592] {jobs.py:1169} DagFileProcessor792 INFO - Processing hello_world
[2018-04-19 21:19:36,602] {jobs.py:566} DagFileProcessor792 INFO - Skipping SLA check for <DAG: hello_world> because no tasks in DAG have SLAs
[2018-04-19 21:19:36,608] {models.py:322} DagFileProcessor792 INFO - Finding 'running' jobs without a recent heartbeat
[2018-04-19 21:19:36,608] {models.py:328} DagFileProcessor792 INFO - Failing jobs without heartbeat after 2018-04-19 21:14:36.608499
[2018-04-19 21:19:36,612] {jobs.py:351} DagFileProcessor792 INFO - Processing /Users/Raj/Root/GitHub/Apache-Airflow-Sandbox/hello_world_workflow/airflow_home/dags/hello_world.py took 0.151 seconds
